backtest\
from .performance import PerformanceAnalyzer
from .professional_visualizer import ProfessionalVisualizer
from .advanced_metrics import AdvancedMetrics
"""
Advanced Performance Metrics - Tính toán metrics chuyên sâu
"""
import numpy as np
import pandas as pd
from scipy import stats
from typing import List, Dict, Tuple

class AdvancedMetrics:
    """Tính toán advanced metrics như Hedge Funds"""
    
    @staticmethod
    def calculate_sharpe_ratio(returns: pd.Series, risk_free_rate: float = 0.02) -> float:
        """Sharpe Ratio annualized"""
        if len(returns) < 2:
            return 0.0
        excess_returns = returns - risk_free_rate/252
        return np.sqrt(252) * excess_returns.mean() / excess_returns.std() if excess_returns.std() != 0 else 0
    
    @staticmethod
    def calculate_sortino_ratio(returns: pd.Series, risk_free_rate: float = 0.02) -> float:
        """Sortino Ratio (chỉ xét downside risk)"""
        if len(returns) < 2:
            return 0.0
        excess_returns = returns - risk_free_rate/252
        downside_returns = returns[returns < 0]
        downside_std = downside_returns.std() if len(downside_returns) > 0 else 0
        return np.sqrt(252) * excess_returns.mean() / downside_std if downside_std != 0 else 0
    
    @staticmethod
    def calculate_calmar_ratio(total_return: float, max_drawdown: float, years: float) -> float:
        """Calmar Ratio (Return/MaxDD)"""
        if max_drawdown == 0 or years == 0:
            return 0.0
        annual_return = total_return / years
        return annual_return / abs(max_drawdown)
    
    @staticmethod
    def calculate_omega_ratio(returns: pd.Series, threshold: float = 0.0) -> float:
        """Omega Ratio - đo lường probability weighted gains/losses"""
        if len(returns) == 0:
            return 0.0
        gains = returns[returns > threshold].sum()
        losses = abs(returns[returns < threshold].sum())
        return gains / losses if losses != 0 else 0
    
    @staticmethod
    def calculate_k_ratio(equity_curve: pd.Series) -> float:
        """K-Ratio (Kestner) - đo lường consistency của returns"""
        if len(equity_curve) < 2:
            return 0.0
        log_returns = np.log(equity_curve / equity_curve.shift(1)).dropna()
        x = np.arange(len(log_returns))
        slope, _, r_value, _, _ = stats.linregress(x, log_returns)
        return slope * r_value ** 2
    
    @staticmethod
    def calculate_ulcer_index(returns: pd.Series) -> float:
        """Ulcer Index - đo lường downside risk"""
        if len(returns) < 2:
            return 0.0
        cumulative = (1 + returns).cumprod()
        max_cumulative = cumulative.expanding().max()
        drawdown = (cumulative - max_cumulative) / max_cumulative
        squared_dd = drawdown ** 2
        return np.sqrt(squared_dd.mean())
    
    @staticmethod
    def calculate_tail_ratio(returns: pd.Series, percentile: float = 0.95) -> float:
        """Tail Ratio (95th percentile gain / 5th percentile loss)"""
        if len(returns) < 10:
            return 0.0
        positive_tail = returns.quantile(percentile)
        negative_tail = abs(returns.quantile(1 - percentile))
        return positive_tail / negative_tail if negative_tail != 0 else 0
    
    @staticmethod
    def calculate_gain_to_pain_ratio(total_gain: float, total_loss: float) -> float:
        """Gain to Pain Ratio (GPR)"""
        if total_loss == 0:
            return 0.0
        return abs(total_gain / total_loss)
    
    @staticmethod
    def calculate_risk_adjusted_return(returns: pd.Series) -> Dict:
        """Tổng hợp tất cả risk-adjusted metrics"""
        metrics = {
            'sharpe_ratio': AdvancedMetrics.calculate_sharpe_ratio(returns),
            'sortino_ratio': AdvancedMetrics.calculate_sortino_ratio(returns),
            'omega_ratio': AdvancedMetrics.calculate_omega_ratio(returns),
            'ulcer_index': AdvancedMetrics.calculate_ulcer_index(returns),
            'tail_ratio': AdvancedMetrics.calculate_tail_ratio(returns),
            'k_ratio': AdvancedMetrics.calculate_k_ratio(returns),
            'var_95': returns.quantile(0.05),
            'cvar_95': returns[returns <= returns.quantile(0.05)].mean(),
            'skewness': returns.skew(),
            'kurtosis': returns.kurtosis(),
            'positive_skew': returns.skew() > 0
        }
        return metrics
import pandas as pd
from core.data_manager import DataManager
from core.strategy_engine import StrategyEngine
from core.risk_manager import RiskManager
from backtest.performance import PerformanceAnalyzer
from backtest.visualizer import ResultVisualizer
from utils.logger import TradingLogger

class BacktestEngine:
    """Engine chạy backtest"""
    
    def __init__(self, data_file, initial_balance=1000):
        self.data_manager = DataManager("XAUUSD", data_file)
        self.strategy_engine = StrategyEngine(self.data_manager)
        self.risk_manager = RiskManager(initial_balance)
        self.performance_analyzer = PerformanceAnalyzer()
        self.visualizer = ResultVisualizer()
        self.logger = TradingLogger(__name__)
        
        self.trades = []
        self.equity_curve = []
    
    def run_backtest(self, show_results=True):
        """Chạy backtest hoàn chỉnh"""
        self.logger.info("Starting backtest...")
        
        data = self.data_manager.load_historical_data(self.data_manager.data_file)
        if data is None:
            self.logger.error("Failed to load data")
            return
        
        total_bars = len(data)
        self.logger.info(f"Processing {total_bars} bars")
        
        for i in range(50, total_bars):
            current_data = data.iloc[:i+1]
            current_bar = data.iloc[i]
            current_time = data.index[i]
            
            # Phân tích thị trường
            market_analysis = self.strategy_engine.analyze_market(current_data)
            
            # Kiểm tra exit conditions
            self._check_exit_conditions(current_bar, current_time)
            
            # Kiểm tra và tạo tín hiệu mới
            if self.risk_manager.can_open_position(current_time):
                signal = self.strategy_engine.generate_signals(current_data, market_analysis)
                if signal:
                    self._execute_trade(signal, current_bar, current_time)
            
            # Cập nhật equity curve
            self._update_equity_curve(current_bar)
        
        # Generate results
        results = self.performance_analyzer.analyze_performance(
            self.trades, self.equity_curve, self.risk_manager
        )
        
        if show_results:
            self.visualizer.display_results(results)
            self.visualizer.plot_results(self.trades, self.equity_curve)
        
        return results
    
    def _execute_trade(self, signal, current_bar, current_time):
        """Thực thi giao dịch"""
        # Implementation for trade execution
        pass
    
    def _check_exit_conditions(self, current_bar, current_time):
        """Kiểm tra điều kiện thoát lệnh"""
        # Implementation for exit conditions
        pass
    
    def _update_equity_curve(self, current_bar):
        """Cập nhật đường equity"""
        # Implementation for equity curve update
        pass
"""
Performance Analyzer - Phân tích hiệu suất backtest
"""
import numpy as np
import pandas as pd
from typing import List, Dict
from datetime import datetime
from .advanced_metrics import AdvancedMetrics
from .professional_visualizer import ProfessionalVisualizer

class PerformanceAnalyzer:
    def __init__(self):
        self.metrics_calculator = AdvancedMetrics()
        self.visualizer = ProfessionalVisualizer(style="dark")
    
    def analyze_performance(self, trades: List, equity_curve: List, 
                          timestamp: List, initial_balance: float = 10000) -> Dict:
        """Phân tích performance toàn diện"""
        
        if len(trades) == 0:
            return self._empty_results()
        
        # Convert to DataFrame
        df_trades = pd.DataFrame([t.__dict__ for t in trades])
        df_equity = pd.DataFrame({
            'timestamp': timestamp,
            'equity': equity_curve
        })
        
        # Calculate basic metrics
        total_return = (equity_curve[-1] - initial_balance) / initial_balance * 100
        total_pnl = equity_curve[-1] - initial_balance
        
        # Calculate returns
        df_equity['returns'] = df_equity['equity'].pct_change().fillna(0)
        
        # Trade statistics
        winning_trades = df_trades[df_trades['pnl'] > 0]
        losing_trades = df_trades[df_trades['pnl'] < 0]
        
        win_rate = len(winning_trades) / len(df_trades) * 100 if len(df_trades) > 0 else 0
        profit_factor = abs(winning_trades['pnl'].sum() / losing_trades['pnl'].sum()) if len(losing_trades) > 0 else 0
        
        # Risk metrics
        df_equity['peak'] = df_equity['equity'].cummax()
        df_equity['drawdown'] = (df_equity['equity'] - df_equity['peak']) / df_equity['peak'] * 100
        max_drawdown = df_equity['drawdown'].min()
        
        # Advanced metrics
        risk_metrics = self.metrics_calculator.calculate_risk_adjusted_return(df_equity['returns'])
        
        # Compile results
        results = {
            'summary': {
                'initial_balance': initial_balance,
                'final_balance': equity_curve[-1],
                'total_return_pct': total_return,
                'total_pnl': total_pnl,
                'total_trades': len(df_trades),
                'winning_trades': len(winning_trades),
                'losing_trades': len(losing_trades),
                'win_rate': win_rate,
                'profit_factor': profit_factor,
                'max_drawdown': max_drawdown,
                'avg_trade_pnl': df_trades['pnl'].mean(),
                'largest_win': df_trades['pnl'].max(),
                'largest_loss': df_trades['pnl'].min(),
                'avg_win': winning_trades['pnl'].mean() if len(winning_trades) > 0 else 0,
                'avg_loss': losing_trades['pnl'].mean() if len(losing_trades) > 0 else 0,
                'win_loss_ratio': abs(winning_trades['pnl'].mean() / losing_trades['pnl'].mean()) if len(losing_trades) > 0 else 0
            },
            'risk_metrics': risk_metrics,
            'trades': df_trades,
            'equity_curve': df_equity
        }
        
        # Add time-based metrics
        if len(timestamp) > 1:
            time_span = (timestamp[-1] - timestamp[0]).days / 365.25
            results['summary']['annual_return'] = total_return / time_span if time_span > 0 else 0
            results['summary']['calmar_ratio'] = self.metrics_calculator.calculate_calmar_ratio(
                total_return, max_drawdown, time_span
            )
        
        return results
    
    def generate_report(self, results: Dict, save_path: str = None):
        """Tạo báo cáo đầy đủ với visualizations"""
        
        # Create dashboard
        fig_dashboard = self.visualizer.create_dashboard(
            trades=results.get('trades', []),
            equity_curve=results['equity_curve']['equity'].tolist(),
            timestamp=results['equity_curve']['timestamp'].tolist(),
            initial_balance=results['summary']['initial_balance']
        )
        
        # Create summary table
        fig_table = self.visualizer.create_summary_table(results['summary'])
        
        # Display or save
        if save_path:
            fig_dashboard.write_html(f"{save_path}_dashboard.html")
            fig_table.write_html(f"{save_path}_summary.html")
        
        return fig_dashboard, fig_table
    
    def _empty_results(self):
        """Trả về kết quả empty"""
        return {
            'summary': {
                'initial_balance': 0,
                'final_balance': 0,
                'total_return_pct': 0,
                'total_trades': 0,
                'win_rate': 0,
                'profit_factor': 0,
                'max_drawdown': 0
            },
            'risk_metrics': {},
            'trades': pd.DataFrame(),
            'equity_curve': pd.DataFrame()
        }
"""
Professional Visualizer - Tạo biểu đồ Hedge Fund chuyên nghiệp
"""
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
from typing import List, Dict
import warnings
warnings.filterwarnings('ignore')

class ProfessionalVisualizer:
    """Tạo biểu đồ chuyên nghiệp theo phong cách Hedge Fund"""
    
    def __init__(self, style: str = "dark"):
        self.style = style
        self.colors = {
            'dark': {
                'background': '#0E1117',
                'paper': '#0E1117',
                'grid': '#2A2F3A',
                'text': '#FFFFFF',
                'profit': '#00D4AA',
                'loss': '#FF4D4D',
                'primary': '#636EFA',
                'secondary': '#EF553B'
            },
            'light': {
                'background': '#FFFFFF',
                'paper': '#FFFFFF',
                'grid': '#E5ECF6',
                'text': '#2A3F5F',
                'profit': '#00D4AA',
                'loss': '#FF4D4D',
                'primary': '#636EFA',
                'secondary': '#EF553B'
            }
        }
        self.colors = self.colors[self.style]
        
    def create_dashboard(self, trades: List, equity_curve: List, timestamp: List, 
                         initial_balance: float = 10000) -> go.Figure:
        """Tạo dashboard đầy đủ với multiple subplots"""
        
        # Tính toán metrics
        df_trades = pd.DataFrame([t.__dict__ for t in trades])
        df_equity = pd.DataFrame({
            'timestamp': timestamp,
            'equity': equity_curve
        })
        
        # Tạo figure với 4x2 grid
        fig = make_subplots(
            rows=4, cols=2,
            vertical_spacing=0.08,
            horizontal_spacing=0.1,
            subplot_titles=(
                'Equity Curve & Drawdown',
                'Monthly Returns Heatmap',
                'Trade Distribution',
                'Underwater Chart',
                'Profit Factor by Month',
                'Cumulative Returns',
                'Trade Duration Distribution',
                'Win Rate Statistics'
            ),
            specs=[
                [{"secondary_y": True}, {"type": "heatmap"}],
                [{"type": "histogram"}, {"type": "scatter"}],
                [{"type": "bar"}, {"type": "scatter"}],
                [{"type": "histogram"}, {"type": "pie"}]
            ]
        )
        
        # 1. Equity Curve & Drawdown (Row 1, Col 1)
        fig.add_trace(
            go.Scatter(
                x=df_equity['timestamp'],
                y=df_equity['equity'],
                mode='lines',
                name='Equity',
                line=dict(color=self.colors['primary'], width=2),
                fill='tozeroy',
                fillcolor='rgba(99, 110, 250, 0.1)'
            ),
            row=1, col=1
        )
        
        # Tính drawdown
        df_equity['peak'] = df_equity['equity'].cummax()
        df_equity['drawdown'] = (df_equity['equity'] - df_equity['peak']) / df_equity['peak'] * 100
        
        fig.add_trace(
            go.Scatter(
                x=df_equity['timestamp'],
                y=df_equity['drawdown'],
                mode='lines',
                name='Drawdown',
                line=dict(color=self.colors['loss'], width=1),
                fill='tozeroy',
                fillcolor='rgba(255, 77, 77, 0.2)'
            ),
            row=1, col=1, secondary_y=True
        )
        
        # 2. Monthly Returns Heatmap (Row 1, Col 2)
        if len(df_trades) > 0:
            df_trades['month'] = pd.to_datetime(df_trades['exit_time']).dt.strftime('%Y-%m')
            monthly_pnl = df_trades.groupby('month')['pnl'].sum().reset_index()
            
            # Tạo heatmap data
            monthly_pnl['year'] = monthly_pnl['month'].str[:4]
            monthly_pnl['month_num'] = monthly_pnl['month'].str[5:].astype(int)
            
            pivot_table = monthly_pnl.pivot(index='year', columns='month_num', values='pnl')
            
            fig.add_trace(
                go.Heatmap(
                    z=pivot_table.values,
                    x=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],
                    y=pivot_table.index,
                    colorscale=[[0, self.colors['loss']], 
                               [0.5, self.colors['text']], 
                               [1, self.colors['profit']]],
                    showscale=True,
                    colorbar=dict(title="PnL", titleside="right")
                ),
                row=1, col=2
            )
        
        # 3. Trade Distribution (Row 2, Col 1)
        if len(df_trades) > 0:
            fig.add_trace(
                go.Histogram(
                    x=df_trades['pnl'],
                    nbinsx=50,
                    name='PnL Distribution',
                    marker_color=self.colors['primary'],
                    opacity=0.7
                ),
                row=2, col=1
            )
        
        # 4. Underwater Chart (Row 2, Col 2)
        underwater = np.maximum.accumulate(df_equity['equity']) - df_equity['equity']
        fig.add_trace(
            go.Scatter(
                x=df_equity['timestamp'],
                y=underwater,
                mode='lines',
                fill='tozeroy',
                fillcolor='rgba(255, 77, 77, 0.3)',
                line=dict(color=self.colors['loss']),
                name='Underwater'
            ),
            row=2, col=2
        )
        
        # 5. Profit Factor by Month (Row 3, Col 1)
        if len(df_trades) > 0:
            df_trades['gross_profit'] = df_trades['pnl'].apply(lambda x: x if x > 0 else 0)
            df_trades['gross_loss'] = df_trades['pnl'].apply(lambda x: abs(x) if x < 0 else 0)
            
            monthly_stats = df_trades.groupby('month').agg({
                'gross_profit': 'sum',
                'gross_loss': 'sum'
            }).reset_index()
            
            monthly_stats['profit_factor'] = monthly_stats['gross_profit'] / monthly_stats['gross_loss']
            monthly_stats['profit_factor'] = monthly_stats['profit_factor'].replace([np.inf, -np.inf], 0).fillna(0)
            
            fig.add_trace(
                go.Bar(
                    x=monthly_stats['month'],
                    y=monthly_stats['profit_factor'],
                    name='Profit Factor',
                    marker_color=np.where(monthly_stats['profit_factor'] > 1, 
                                         self.colors['profit'], 
                                         self.colors['loss'])
                ),
                row=3, col=1
            )
        
        # 6. Cumulative Returns (Row 3, Col 2)
        returns = df_equity['equity'].pct_change().fillna(0)
        cumulative_returns = (1 + returns).cumprod() - 1
        
        fig.add_trace(
            go.Scatter(
                x=df_equity['timestamp'],
                y=cumulative_returns * 100,
                mode='lines',
                name='Cumulative Returns %',
                line=dict(color=self.colors['secondary'], width=2),
                fill='tozeroy',
                fillcolor='rgba(239, 85, 59, 0.1)'
            ),
            row=3, col=2
        )
        
        # 7. Trade Duration Distribution (Row 4, Col 1)
        if len(df_trades) > 0:
            df_trades['duration'] = (pd.to_datetime(df_trades['exit_time']) - 
                                    pd.to_datetime(df_trades['entry_time'])).dt.total_seconds() / 3600
            
            fig.add_trace(
                go.Histogram(
                    x=df_trades['duration'],
                    nbinsx=30,
                    name='Trade Duration (hours)',
                    marker_color=self.colors['secondary'],
                    opacity=0.7
                ),
                row=4, col=1
            )
        
        # 8. Win Rate Statistics (Row 4, Col 2)
        if len(df_trades) > 0:
            winning_trades = len(df_trades[df_trades['pnl'] > 0])
            losing_trades = len(df_trades[df_trades['pnl'] < 0])
            neutral_trades = len(df_trades[df_trades['pnl'] == 0])
            
            fig.add_trace(
                go.Pie(
                    labels=['Winning', 'Losing', 'Neutral'],
                    values=[winning_trades, losing_trades, neutral_trades],
                    marker=dict(colors=[self.colors['profit'], 
                                       self.colors['loss'], 
                                       self.colors['text']]),
                    hole=0.4,
                    showlegend=True
                ),
                row=4, col=2
            )
        
        # Update layout
        fig.update_layout(
            title=dict(
                text='<b>HEDGE FUND STYLE PERFORMANCE DASHBOARD</b>',
                x=0.5,
                font=dict(size=24, color=self.colors['text'])
            ),
            template='plotly_dark' if self.style == 'dark' else 'plotly_white',
            showlegend=True,
            height=1600,
            paper_bgcolor=self.colors['paper'],
            plot_bgcolor=self.colors['paper'],
            font=dict(color=self.colors['text'], size=10),
            hovermode='x unified'
        )
        
        # Update axes
        fig.update_xaxes(gridcolor=self.colors['grid'], linecolor=self.colors['grid'])
        fig.update_yaxes(gridcolor=self.colors['grid'], linecolor=self.colors['grid'])
        
        return fig
    
    def create_summary_table(self, metrics: Dict) -> go.Figure:
        """Tạo bảng summary metrics chuyên nghiệp"""
        
        # Define metrics
        metric_names = [
            'Total Return (%)',
            'Annual Return (%)',
            'Sharpe Ratio',
            'Sortino Ratio',
            'Max Drawdown (%)',
            'Calmar Ratio',
            'Profit Factor',
            'Win Rate (%)',
            'Average Win',
            'Average Loss',
            'Win/Loss Ratio',
            'Total Trades',
            'Profitability'
        ]
        
        metric_values = [
            f"{metrics.get('total_return', 0):.2f}",
            f"{metrics.get('annual_return', 0):.2f}",
            f"{metrics.get('sharpe_ratio', 0):.2f}",
            f"{metrics.get('sortino_ratio', 0):.2f}",
            f"{metrics.get('max_drawdown', 0):.2f}",
            f"{metrics.get('calmar_ratio', 0):.2f}",
            f"{metrics.get('profit_factor', 0):.2f}",
            f"{metrics.get('win_rate', 0):.2f}",
            f"{metrics.get('avg_win', 0):.2f}",
            f"{metrics.get('avg_loss', 0):.2f}",
            f"{metrics.get('win_loss_ratio', 0):.2f}",
            f"{metrics.get('total_trades', 0):.0f}",
            "✓" if metrics.get('total_return', 0) > 0 else "✗"
        ]
        
        # Tạo bảng
        fig = go.Figure(data=[go.Table(
            header=dict(
                values=['<b>METRIC</b>', '<b>VALUE</b>'],
                fill_color=self.colors['background'],
                align='center',
                font=dict(color=self.colors['text'], size=14, family="Arial Black"),
                height=40
            ),
            cells=dict(
                values=[metric_names, metric_values],
                fill_color=[self.colors['paper'], 
                           [self.colors['profit'] if 'Profit' in name or 
                            ('Win' in name and 'Loss' not in name) or 
                            v == '✓' else 
                            self.colors['loss'] if 'Loss' in name or 
                            'Drawdown' in name or 
                            v == '✗' else 
                            self.colors['paper'] for name, v in zip(metric_names, metric_values)]],
                align=['left', 'center'],
                font=dict(color=self.colors['text'], size=12),
                height=35
            )
        )])
        
        fig.update_layout(
            title=dict(
                text='<b>PERFORMANCE METRICS SUMMARY</b>',
                x=0.5,
                font=dict(size=18, color=self.colors['text'])
            ),
            height=600,
            paper_bgcolor=self.colors['paper'],
            margin=dict(t=60, b=20, l=20, r=20)
        )
        
        return fig

config\




from .settings import TradingSettings, IndicatorSettings
from .risk_management import RiskConfig
from .unified_config import get_config, UNIFIED_CONFIG

__all__ = [
    'TradingSettings', 
    'IndicatorSettings', 
    'RiskConfig',
    'get_config', 
    'UNIFIED_CONFIG'
]
class RiskConfig:
    """Cấu hình quản lý rủi ro"""
    
    # Account protection
    INITIAL_BALANCE = 1000.0
    MAX_DRAWDOWN_PERCENT = 10.0
    MAX_DAILY_LOSS_PERCENT = 5.0
    MAX_POSITIONS = 3
    MAX_DAILY_TRADES = 10
    
    # Position sizing
    BASE_RISK_PERCENT = 1.0
    MIN_LOT_SIZE = 0.01
    MAX_LOT_SIZE = 0.1
    
    # Volume tiers for compounding
    VOLUME_TIERS = [
        {'min': 10,    'max': 999,   'lot': 0.01, 'risk_percent': 1.0},
        {'min': 1000,  'max': 4999,  'lot': 0.02, 'risk_percent': 1.5},
        {'min': 5000,  'max': 9999,  'lot': 0.03, 'risk_percent': 2.0},
        {'min': 10000, 'max': 49999, 'lot': 0.05, 'risk_percent': 2.0},
    ]
    
    # Stop loss and take profit
    MIN_RR_RATIO = 1.0
    ENABLE_TRAILING_STOP = False
    TRAILING_STOP_ATR_MULTIPLIER = 1.0
class TradingSettings:
    """Cấu hình giao dịch chính"""
    
    # Symbol settings
    SYMBOL = "XAUUSD"
    SPREAD = 0.5
    COMMISSION = 0.0
    POINT_VALUE = 0.01
    PIP_VALUE_PER_LOT = 1.0
    
    # Trading hours
    TRADING_SESSIONS = {
        'asian': {'start': '00:00', 'end': '08:00'},
        'european': {'start': '08:00', 'end': '16:00'},
        'american': {'start': '16:00', 'end': '24:00'}
    }
    
    # Strategy settings
    ENABLE_SIDEWAYS_STRATEGY = True
    ENABLE_TREND_STRATEGY = True
    ENABLE_H1_FILTER = True
    
    # Signal settings
    MIN_CONFIDENCE = 0.65
    SIGNAL_COOLDOWN_MINUTES = 30

class IndicatorSettings:
    """Settings for indicators"""
    RANGE_PERIOD = 20
    VOLATILITY_THRESHOLD = 0.015
    RANGE_THRESHOLD = 0.03
    EMA_FAST_PERIOD = 12
    EMA_SLOW_PERIOD = 26
    RSI_PERIOD = 14
    ATR_PERIOD = 14
    MACD_FAST = 12
    MACD_SLOW = 26
    MACD_SIGNAL = 9
    BOLLINGER_PERIOD = 20
    BOLLINGER_STD = 2
    ICHIMOKU_TENKAN = 9
    ICHIMOKU_KIJUN = 26
    ICHIMOKU_SENKOU = 52
UNIFIED_CONFIG = {
    # Base settings
    'symbol': 'XAUUSD',
    'initial_balance': 1000,
    
    # Mode-specific settings
    'backtest': {
        'data_file': 'xauusd_data.csv',
        'enable_logging': True,
        'save_results': True,
    },
    
    'paper': {
        'broker': 'demo',
        'enable_live_logging': True,
        'update_interval': 1,
        'auto_stop_on_max_dd': True,
    },
    
    'live': {
        'broker': 'mt5',
        'login': 123456,
        'password': 'your_password',
        'server': 'BrokerServer',
        'enable_live_logging': True,
        'update_interval': 1,
        'auto_stop_on_max_dd': True,
    },
    
    # Strategy settings (shared)
    'strategies': {
        'sideways': {'enabled': True, 'min_confidence': 0.65},
        'trend': {'enabled': True, 'min_confidence': 0.68},
    },
    
    # Risk settings (shared)
    'risk': {
        'max_drawdown_percent': 10.0,
        'max_daily_trades': 10,
        'max_positions': 3,
        'risk_per_trade': 1.0,
    }
}

def get_config(mode, **overrides):
    """Lấy config cho mode cụ thể"""
    config = UNIFIED_CONFIG.copy()
    config['mode'] = mode
    
    # Merge mode-specific settings
    if mode in config:
        config.update(config[mode])
    
    # Apply overrides
    config.update(overrides)
    
    return config

core\

# core/__init__.py
from .data_manager import AdvancedDataManager
from .signal_generator import AdvancedSignalGenerator
from .risk_manager import AdvancedRiskManager
from .position_sizer import AdvancedPositionSizer
from .portfolio_manager import AdvancedPortfolioManager
from .performance_tracker import PerformanceTracker

__all__ = [
    'AdvancedDataManager',
    'AdvancedSignalGenerator',
    'AdvancedRiskManager',
    'AdvancedPositionSizer',
    'AdvancedPortfolioManager',
    'PerformanceTracker'
]
# core/data_manager.py
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import threading
import warnings
warnings.filterwarnings('ignore')

from indicators.advanced_indicators import AdvancedIndicators
from indicators.market_analyzer import EnhancedMarketAnalyzer

class AdvancedDataManager:
    """Quản lý data nâng cao với multi-timeframe và real-time processing"""
    
    def __init__(self, 
                 data_source=None, 
                 timeframes: List[str] = ['1m', '5m', '15m', '30m', '1h', '4h', '1d'],
                 cache_size: int = 10000):
        
        self.data_source = data_source
        self.timeframes = timeframes
        self.cache_size = cache_size
        
        # Data storage
        self.data_store = {tf: pd.DataFrame() for tf in timeframes}
        self.indicators_store = {tf: {} for tf in timeframes}
        self.market_analysis_store = {tf: {} for tf in timeframes}
        
        # Cache system
        self.cache = {}
        self.cache_lock = threading.RLock()
        
        # Multi-timeframe synchronization
        self.sync_manager = MultiTimeframeSync()
        
        # Performance optimization
        self.use_parallel = True
        self.batch_size = 1000
        
        # ML features storage
        self.feature_store = {}
        
        # Market analyzer
        self.market_analyzer = EnhancedMarketAnalyzer(use_ml=True)
        
        # Data quality metrics
        self.data_quality = {}
        
    def load_historical_data(self, 
                           symbol: str, 
                           start_date: datetime,
                           end_date: datetime = None) -> Dict[str, pd.DataFrame]:
        """Load historical data cho tất cả timeframes"""
        end_date = end_date or datetime.now()
        historical_data = {}
        
        for timeframe in self.timeframes:
            try:
                data = self._load_timeframe_data(symbol, timeframe, start_date, end_date)
                if data is not None and len(data) > 0:
                    historical_data[timeframe] = self._preprocess_data(data)
                    self.data_store[timeframe] = historical_data[timeframe]
                    
                    # Calculate indicators
                    self._calculate_timeframe_indicators(timeframe)
                    
                    # Analyze market
                    self._analyze_timeframe_market(timeframe)
                    
            except Exception as e:
                print(f"Error loading {timeframe} data for {symbol}: {e}")
        
        # Synchronize multi-timeframe data
        self._synchronize_multi_timeframe_data()
        
        return historical_data
    
    def _load_timeframe_data(self, symbol: str, timeframe: str, 
                           start_date: datetime, end_date: datetime) -> Optional[pd.DataFrame]:
        """Load data cho một timeframe cụ thể"""
        # Implementation depends on data source
        # This is a placeholder for actual data loading logic
        
        # Mock data for example
        dates = pd.date_range(start_date, end_date, freq=self._timeframe_to_freq(timeframe))
        n = len(dates)
        
        data = pd.DataFrame({
            'open': np.random.normal(100, 2, n).cumsum(),
            'high': np.random.normal(101, 3, n).cumsum(),
            'low': np.random.normal(99, 3, n).cumsum(),
            'close': np.random.normal(100, 2, n).cumsum(),
            'volume': np.random.normal(1000, 200, n)
        }, index=dates)
        
        return data
    
    def _timeframe_to_freq(self, timeframe: str) -> str:
        """Convert timeframe string to pandas frequency"""
        freq_map = {
            '1m': '1min',
            '5m': '5min',
            '15m': '15min',
            '30m': '30min',
            '1h': '1h',
            '4h': '4h',
            '1d': '1D',
            '1w': '1W',
            '1M': '1M'
        }
        return freq_map.get(timeframe, '1h')
    
    def _preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Preprocess data với quality checks"""
        if data.empty:
            return data
        
        # Clean data
        data = data.copy()
        
        # Remove duplicates
        data = data[~data.index.duplicated(keep='first')]
        
        # Handle missing values
        data = data.ffill().bfill()
        
        # Validate OHLC relationships
        data['high'] = data[['high', 'open', 'low', 'close']].max(axis=1)
        data['low'] = data[['high', 'open', 'low', 'close']].min(axis=1)
        
        # Add returns
        data['returns'] = data['close'].pct_change()
        
        # Add log returns
        data['log_returns'] = np.log(data['close'] / data['close'].shift(1))
        
        # Add basic indicators
        data['sma_20'] = data['close'].rolling(20).mean()
        data['sma_50'] = data['close'].rolling(50).mean()
        data['ema_12'] = data['close'].ewm(span=12, adjust=False).mean()
        data['ema_26'] = data['close'].ewm(span=26, adjust=False).mean()
        
        # Calculate ATR
        high_low = data['high'] - data['low']
        high_close = np.abs(data['high'] - data['close'].shift())
        low_close = np.abs(data['low'] - data['close'].shift())
        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        data['atr'] = tr.rolling(14).mean()
        
        # Volume indicators
        if 'volume' in data.columns:
            data['volume_sma'] = data['volume'].rolling(20).mean()
            data['volume_ratio'] = data['volume'] / data['volume_sma']
        
        return data
    
    def _calculate_timeframe_indicators(self, timeframe: str):
        """Tính toán advanced indicators cho timeframe"""
        if timeframe not in self.data_store or self.data_store[timeframe].empty:
            return
        
        data = self.data_store[timeframe]
        
        # Calculate advanced indicators
        with self.cache_lock:
            cache_key = f"indicators_{timeframe}_{data.index[-1].timestamp()}"
            if cache_key not in self.cache:
                indicators = AdvancedIndicators.calculate_all_indicators(data)
                self.indicators_store[timeframe] = indicators
                self.cache[cache_key] = indicators
            else:
                self.indicators_store[timeframe] = self.cache[cache_key]
    
    def _analyze_timeframe_market(self, timeframe: str):
        """Phân tích thị trường cho timeframe"""
        if timeframe not in self.data_store or self.data_store[timeframe].empty:
            return
        
        data = self.data_store[timeframe]
        indicators = self.indicators_store.get(timeframe, {})
        
        # Combine data và indicators cho analysis
        analysis_data = data.copy()
        for key, value in indicators.items():
            if isinstance(value, dict):
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, pd.Series):
                        analysis_data[f"{key}_{sub_key}"] = sub_value
            elif isinstance(value, pd.Series):
                analysis_data[key] = value
        
        # Analyze market
        market_analysis = self.market_analyzer.analyze_market(analysis_data, timeframe)
        self.market_analysis_store[timeframe] = market_analysis
    
    def _synchronize_multi_timeframe_data(self):
        """Đồng bộ hóa dữ liệu multi-timeframe"""
        # Ensure all timeframes have consistent timestamps
        # Align data across timeframes
        pass
    
    def update_real_time_data(self, symbol: str, timeframe: str, new_data: pd.DataFrame):
        """Cập nhật real-time data"""
        if timeframe not in self.data_store:
            self.data_store[timeframe] = pd.DataFrame()
        
        # Append new data
        self.data_store[timeframe] = pd.concat([
            self.data_store[timeframe], 
            new_data
        ]).drop_duplicates()
        
        # Keep only recent data
        if len(self.data_store[timeframe]) > self.cache_size:
            self.data_store[timeframe] = self.data_store[timeframe].iloc[-self.cache_size:]
        
        # Recalculate indicators
        self._calculate_timeframe_indicators(timeframe)
        
        # Reanalyze market
        self._analyze_timeframe_market(timeframe)
    
    def get_multi_timeframe_analysis(self) -> Dict:
        """Lấy multi-timeframe market analysis"""
        # Analyze each timeframe
        mtf_analysis = {}
        
        for timeframe in self.timeframes:
            if timeframe in self.market_analysis_store:
                mtf_analysis[timeframe] = self.market_analysis_store[timeframe]
        
        # Get overall synthesis
        overall = self.market_analyzer._synthesize_multi_timeframe_analysis(mtf_analysis)
        
        return {
            'timeframe_analysis': mtf_analysis,
            'overall_analysis': overall,
            'timestamp': datetime.now()
        }
    
    def get_data_for_strategy(self, 
                            strategy_name: str, 
                            lookback_periods: int = 100) -> Dict:
        """Lấy data optimized cho strategy cụ thể"""
        strategy_data = {}
        
        # Determine which timeframes strategy needs
        strategy_timeframes = self._get_strategy_timeframes(strategy_name)
        
        for timeframe in strategy_timeframes:
            if timeframe in self.data_store and not self.data_store[timeframe].empty:
                data = self.data_store[timeframe].tail(lookback_periods)
                indicators = self.indicators_store.get(timeframe, {})
                
                strategy_data[timeframe] = {
                    'data': data,
                    'indicators': indicators,
                    'market_analysis': self.market_analysis_store.get(timeframe, {})
                }
        
        return strategy_data
    
    def _get_strategy_timeframes(self, strategy_name: str) -> List[str]:
        """Xác định timeframes mà strategy cần"""
        # Map strategies to their required timeframes
        timeframe_mapping = {
            'TREND_ML_ENHANCED': ['4h', '1h', '30m', '15m', '5m', '1m'],
            'MEAN_REVERSION': ['1h', '30m', '15m'],
            'BREAKOUT': ['4h', '1h', '30m'],
            'SCALPING': ['5m', '1m']
        }
        
        return timeframe_mapping.get(strategy_name, ['1h'])
    
    def calculate_ml_features(self, 
                            target_timeframe: str = '1h',
                            feature_window: int = 50) -> pd.DataFrame:
        """Tính toán ML features cho model training"""
        if target_timeframe not in self.data_store:
            return pd.DataFrame()
        
        data = self.data_store[target_timeframe]
        if len(data) < feature_window:
            return pd.DataFrame()
        
        features = []
        
        # Price-based features
        close_prices = data['close'].values
        
        # Returns features
        returns = data['returns'].dropna().values
        
        # Technical indicator features
        for i in range(len(data) - feature_window, len(data)):
            window_data = data.iloc[i-feature_window:i]
            
            feature_vector = self._extract_feature_vector(window_data)
            features.append(feature_vector)
        
        feature_df = pd.DataFrame(features, index=data.index[-len(features):])
        
        # Store features
        self.feature_store[target_timeframe] = feature_df
        
        return feature_df
    
    def _extract_feature_vector(self, data_window: pd.DataFrame) -> np.ndarray:
        """Trích xuất feature vector từ data window"""
        features = []
        
        # Basic price features
        prices = data_window['close'].values
        features.extend([
            prices[-1] / prices[-2] - 1,  # 1-period return
            prices[-1] / prices[-6] - 1,  # 5-period return
            prices[-1] / prices[-21] - 1, # 20-period return
            np.std(prices[-20:]) / np.mean(prices[-20:]),  # Volatility
            (prices[-1] - np.min(prices[-20:])) / (np.max(prices[-20:]) - np.min(prices[-20:])),  # Position in range
        ])
        
        # Volume features
        if 'volume' in data_window.columns:
            volume = data_window['volume'].values
            features.extend([
                volume[-1] / np.mean(volume[-20:]),
                np.std(volume[-20:]) / np.mean(volume[-20:]),
            ])
        
        # Technical indicator features
        if 'sma_20' in data_window.columns:
            sma_20 = data_window['sma_20'].iloc[-1]
            sma_50 = data_window['sma_50'].iloc[-1]
            price = data_window['close'].iloc[-1]
            
            features.extend([
                price / sma_20 - 1,
                sma_20 / sma_50 - 1,
                1 if price > sma_20 else 0,
                1 if sma_20 > sma_50 else 0
            ])
        
        # Ensure fixed length
        expected_length = 20
        if len(features) < expected_length:
            features.extend([0.0] * (expected_length - len(features)))
        elif len(features) > expected_length:
            features = features[:expected_length]
        
        return np.array(features)
    
    def get_data_quality_report(self) -> Dict:
        """Báo cáo chất lượng data"""
        quality_report = {}
        
        for timeframe in self.timeframes:
            if timeframe in self.data_store and not self.data_store[timeframe].empty:
                data = self.data_store[timeframe]
                
                report = {
                    'rows': len(data),
                    'start_date': data.index[0],
                    'end_date': data.index[-1],
                    'missing_values': data.isnull().sum().sum(),
                    'duplicates': data.index.duplicated().sum(),
                    'ohlc_consistency': self._check_ohlc_consistency(data),
                    'volume_presence': 'volume' in data.columns,
                    'avg_spread': self._calculate_avg_spread(data),
                    'data_freshness': (datetime.now() - data.index[-1]).total_seconds() / 3600
                }
                
                quality_report[timeframe] = report
        
        return quality_report
    
    def _check_ohlc_consistency(self, data: pd.DataFrame) -> bool:
        """Kiểm tra tính consistency của OHLC data"""
        if data.empty:
            return False
        
        # Check OHLC relationships
        valid_high = (data['high'] >= data[['open', 'low', 'close']].max(axis=1)).all()
        valid_low = (data['low'] <= data[['open', 'high', 'close']].min(axis=1)).all()
        
        return valid_high and valid_low
    
    def _calculate_avg_spread(self, data: pd.DataFrame) -> float:
        """Tính average spread"""
        if 'ask' in data.columns and 'bid' in data.columns:
            spreads = (data['ask'] - data['bid']) / data['close']
            return float(spreads.mean())
        return 0.0
    
    def get_support_resistance_levels(self, 
                                    timeframe: str = '1h',
                                    lookback: int = 100) -> Dict:
        """Lấy support và resistance levels"""
        if timeframe not in self.data_store or self.data_store[timeframe].empty:
            return {}
        
        data = self.data_store[timeframe].tail(lookback)
        
        # Use market analyzer to get levels
        analysis = self.market_analyzer.analyze_market(data, timeframe)
        
        return analysis.get('support_resistance', {})
    
    def clear_cache(self):
        """Xóa cache"""
        with self.cache_lock:
            self.cache.clear()

class MultiTimeframeSync:
    """Đồng bộ hóa multi-timeframe data"""
    
    def __init__(self):
        self.sync_points = {}
        self.alignment_threshold = timedelta(minutes=5)
    
    def align_timeframes(self, data_dict: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """Align data across timeframes"""
        aligned_data = {}
        
        if not data_dict:
            return aligned_data
        
        # Find common timestamp range
        common_start = max(df.index[0] for df in data_dict.values())
        common_end = min(df.index[-1] for df in data_dict.values())
        
        for timeframe, data in data_dict.items():
            aligned = data[(data.index >= common_start) & (data.index <= common_end)]
            aligned_data[timeframe] = aligned
        
        return aligned_data
    
    def detect_anomalies(self, data_dict: Dict[str, pd.DataFrame]) -> Dict[str, List]:
        """Phát hiện anomalies trong multi-timeframe data"""
        anomalies = {}
        
        for timeframe, data in data_dict.items():
            timeframe_anomalies = []
            
            # Check for gaps
            time_diffs = data.index.to_series().diff()
            expected_freq = self._get_expected_frequency(timeframe)
            large_gaps = time_diffs[time_diffs > expected_freq * 2]
            
            if not large_gaps.empty:
                timeframe_anomalies.append({
                    'type': 'DATA_GAP',
                    'gaps': len(large_gaps),
                    'max_gap': large_gaps.max()
                })
            
            # Check for price jumps
            returns = data['close'].pct_change().abs()
            large_jumps = returns[returns > 0.05]  # 5% jumps
            
            if not large_jumps.empty:
                timeframe_anomalies.append({
                    'type': 'PRICE_JUMP',
                    'jumps': len(large_jumps),
                    'max_jump': large_jumps.max()
                })
            
            if timeframe_anomalies:
                anomalies[timeframe] = timeframe_anomalies
        
        return anomalies
    
    def _get_expected_frequency(self, timeframe: str) -> pd.Timedelta:
        """Lấy expected frequency cho timeframe"""
        freq_map = {
            '1m': pd.Timedelta(minutes=1),
            '5m': pd.Timedelta(minutes=5),
            '15m': pd.Timedelta(minutes=15),
            '30m': pd.Timedelta(minutes=30),
            '1h': pd.Timedelta(hours=1),
            '4h': pd.Timedelta(hours=4),
            '1d': pd.Timedelta(days=1)
        }
        return freq_map.get(timeframe, pd.Timedelta(hours=1))
# core/execution_engine.py
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import asyncio
import aiohttp
from dataclasses import dataclass
from enum import Enum
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OrderType(Enum):
    """Order types"""
    MARKET = "MARKET"
    LIMIT = "LIMIT"
    STOP = "STOP"
    STOP_LIMIT = "STOP_LIMIT"

class OrderStatus(Enum):
    """Order status"""
    PENDING = "PENDING"
    FILLED = "FILLED"
    PARTIALLY_FILLED = "PARTIALLY_FILLED"
    CANCELLED = "CANCELLED"
    REJECTED = "REJECTED"
    EXPIRED = "EXPIRED"

@dataclass
class Order:
    """Order data class"""
    order_id: str
    symbol: str
    order_type: OrderType
    side: str  # BUY or SELL
    quantity: float
    price: Optional[float] = None
    stop_price: Optional[float] = None
    limit_price: Optional[float] = None
    status: OrderStatus = OrderStatus.PENDING
    created_at: datetime = None
    updated_at: datetime = None
    filled_quantity: float = 0.0
    average_fill_price: float = 0.0
    fees: float = 0.0
    slippage: float = 0.0
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.updated_at is None:
            self.updated_at = self.created_at
    
    def update_status(self, 
                     status: OrderStatus,
                     filled_qty: float = 0.0,
                     fill_price: float = 0.0,
                     fees: float = 0.0,
                     slippage: float = 0.0):
        """Update order status"""
        self.status = status
        self.filled_quantity = filled_qty
        self.average_fill_price = fill_price
        self.fees = fees
        self.slippage = slippage
        self.updated_at = datetime.now()

class ExecutionEngine:
    """Advanced execution engine với slippage control và smart routing"""
    
    def __init__(self, 
                 api_client=None,
                 max_slippage_pct: float = 0.001,  # 0.1%
                 max_retries: int = 3,
                 use_smart_routing: bool = True):
        
        self.api_client = api_client
        self.max_slippage_pct = max_slippage_pct
        self.max_retries = max_retries
        self.use_smart_routing = use_smart_routing
        
        # Order management
        self.orders = {}  # order_id -> Order
        self.order_history = []
        
        # Execution metrics
        self.execution_metrics = {
            'total_orders': 0,
            'successful_orders': 0,
            'failed_orders': 0,
            'avg_slippage': 0.0,
            'avg_fill_time': 0.0,
            'total_fees': 0.0
        }
        
        # Market data cache
        self.market_data_cache = {}
        self.cache_ttl = timedelta(seconds=5)
        
        # Smart routing configuration
        self.routing_config = {
            'prefer_limit_orders': True,
            'use_iceberg_orders': False,
            'min_order_size': 0.01,
            'max_order_size': 100.0
        }
        
        # Risk controls
        self.risk_controls = {
            'max_position_size': 10000.0,
            'max_daily_orders': 50,
            'cooling_period_seconds': 1.0
        }
        
        # Performance tracking
        self.daily_order_count = 0
        self.last_order_time = None
        
        # Async session
        self.session = None
        
    async def initialize(self):
        """Initialize execution engine"""
        if self.api_client is None:
            # Initialize default API client
            self.api_client = await self._create_default_client()
        
        self.session = aiohttp.ClientSession()
        logger.info("Execution engine initialized")
    
    async def _create_default_client(self):
        """Create default API client"""
        # Placeholder for actual API client initialization
        return None
    
    async def execute_order(self, 
                          signal: Dict,
                          position_data: Dict,
                          market_data: Dict) -> Dict:
        """Execute order based on signal và position data"""
        
        # Check risk controls
        risk_check = self._check_risk_controls(signal, position_data)
        if not risk_check['allowed']:
            return {
                'success': False,
                'order_id': None,
                'error': risk_check['reason'],
                'timestamp': datetime.now()
            }
        
        # Generate order
        order = self._create_order_from_signal(signal, position_data, market_data)
        
        if order is None:
            return {
                'success': False,
                'order_id': None,
                'error': 'Failed to create order',
                'timestamp': datetime.now()
            }
        
        # Add to orders
        self.orders[order.order_id] = order
        
        # Execute order
        execution_result = await self._execute_order_async(order, market_data)
        
        # Update metrics
        self._update_execution_metrics(execution_result)
        
        # Update daily count
        self.daily_order_count += 1
        self.last_order_time = datetime.now()
        
        return execution_result
    
    def _check_risk_controls(self, 
                           signal: Dict,
                           position_data: Dict) -> Dict:
        """Check risk controls before execution"""
        
        # 1. Check daily order limit
        if self.daily_order_count >= self.risk_controls['max_daily_orders']:
            return {
                'allowed': False,
                'reason': f'Daily order limit ({self.risk_controls["max_daily_orders"]}) reached'
            }
        
        # 2. Check cooling period
        if self.last_order_time is not None:
            time_since_last = (datetime.now() - self.last_order_time).total_seconds()
            if time_since_last < self.risk_controls['cooling_period_seconds']:
                return {
                    'allowed': False,
                    'reason': f'Cooling period active. Wait {self.risk_controls["cooling_period_seconds"] - time_since_last:.1f} seconds'
                }
        
        # 3. Check position size
        position_value = position_data.get('position_value', 0)
        if position_value > self.risk_controls['max_position_size']:
            return {
                'allowed': False,
                'reason': f'Position size ({position_value:.2f}) exceeds maximum ({self.risk_controls["max_position_size"]:.2f})'
            }
        
        # 4. Check order size limits
        order_size = position_data.get('units', 0)
        if order_size < self.routing_config['min_order_size']:
            return {
                'allowed': False,
                'reason': f'Order size ({order_size:.4f}) below minimum ({self.routing_config["min_order_size"]:.4f})'
            }
        
        if order_size > self.routing_config['max_order_size']:
            return {
                'allowed': False,
                'reason': f'Order size ({order_size:.4f}) exceeds maximum ({self.routing_config["max_order_size"]:.4f})'
            }
        
        return {'allowed': True, 'reason': 'OK'}
    
    def _create_order_from_signal(self,
                                signal: Dict,
                                position_data: Dict,
                                market_data: Dict) -> Optional[Order]:
        """Create order từ signal"""
        
        symbol = signal.get('symbol', 'XAUUSD')
        order_side = signal.get('signal', 'BUY')
        quantity = position_data.get('units', 0)
        
        if quantity <= 0:
            logger.error(f"Invalid quantity: {quantity}")
            return None
        
        # Determine order type
        order_type = self._determine_order_type(signal, market_data)
        
        # Calculate prices
        prices = self._calculate_order_prices(signal, position_data, market_data, order_type)
        
        # Generate order ID
        order_id = self._generate_order_id(symbol, order_side)
        
        # Create order
        order = Order(
            order_id=order_id,
            symbol=symbol,
            order_type=order_type,
            side=order_side,
            quantity=quantity,
            price=prices.get('price'),
            stop_price=prices.get('stop_price'),
            limit_price=prices.get('limit_price')
        )
        
        logger.info(f"Created order: {order}")
        return order
    
    def _determine_order_type(self, 
                            signal: Dict,
                            market_data: Dict) -> OrderType:
        """Determine optimal order type"""
        
        # Check if limit orders are preferred
        if self.routing_config['prefer_limit_orders']:
            liquidity = market_data.get('liquidity_metrics', {}).get('score', 0.5)
            volatility = market_data.get('volatility', 0.02)
            
            # Use limit orders when liquidity is good and volatility is low
            if liquidity > 0.7 and volatility < 0.015:
                return OrderType.LIMIT
        
        # Default to market order
        return OrderType.MARKET
    
    def _calculate_order_prices(self,
                              signal: Dict,
                              position_data: Dict,
                              market_data: Dict,
                              order_type: OrderType) -> Dict:
        """Calculate order prices"""
        
        current_price = signal.get('price', 0)
        stop_loss = position_data.get('stop_loss', 0)
        take_profit = position_data.get('take_profit', 0)
        
        prices = {}
        
        if order_type == OrderType.MARKET:
            # Market order - use current price with slippage adjustment
            slippage = self._estimate_slippage(position_data, market_data)
            if signal.get('signal') == 'BUY':
                prices['price'] = current_price * (1 + slippage)
            else:
                prices['price'] = current_price * (1 - slippage)
        
        elif order_type == OrderType.LIMIT:
            # Limit order - set limit price
            if signal.get('signal') == 'BUY':
                # Buy limit below current price
                prices['limit_price'] = current_price * 0.995  # 0.5% below
            else:
                # Sell limit above current price
                prices['limit_price'] = current_price * 1.005  # 0.5% above
        
        elif order_type == OrderType.STOP:
            # Stop order
            prices['stop_price'] = stop_loss
        
        elif order_type == OrderType.STOP_LIMIT:
            # Stop limit order
            prices['stop_price'] = stop_loss
            if signal.get('signal') == 'BUY':
                prices['limit_price'] = stop_loss * 1.001  # 0.1% above stop
            else:
                prices['limit_price'] = stop_loss * 0.999  # 0.1% below stop
        
        return prices
    
    def _estimate_slippage(self,
                          position_data: Dict,
                          market_data: Dict) -> float:
        """Estimate slippage for order"""
        
        order_size = position_data.get('units', 0)
        liquidity_score = market_data.get('liquidity_metrics', {}).get('score', 0.5)
        volatility = market_data.get('volatility', 0.02)
        
        # Base slippage
        base_slippage = self.max_slippage_pct
        
        # Adjust for order size
        size_adjustment = min(1.0, order_size / 10000.0)  # Normalize
        size_factor = 1.0 + size_adjustment * 2.0  # Larger orders = more slippage
        
        # Adjust for liquidity
        liquidity_factor = 1.0 / max(liquidity_score, 0.1)  # Lower liquidity = more slippage
        
        # Adjust for volatility
        volatility_factor = 1.0 + volatility * 50.0  # Higher volatility = more slippage
        
        # Calculate estimated slippage
        estimated_slippage = base_slippage * size_factor * liquidity_factor * volatility_factor
        
        # Cap at reasonable maximum
        max_allowed_slippage = 0.01  # 1% maximum
        estimated_slippage = min(estimated_slippage, max_allowed_slippage)
        
        return estimated_slippage
    
    def _generate_order_id(self, symbol: str, side: str) -> str:
        """Generate unique order ID"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        return f"{symbol}_{side}_{timestamp}"
    
    async def _execute_order_async(self,
                                 order: Order,
                                 market_data: Dict) -> Dict:
        """Execute order asynchronously"""
        
        logger.info(f"Executing order: {order.order_id}")
        
        start_time = datetime.now()
        
        try:
            # Simulate order execution với retries
            for attempt in range(self.max_retries):
                try:
                    # Get current market price
                    current_price = await self._get_current_price(order.symbol)
                    
                    # Execute based on order type
                    if order.order_type == OrderType.MARKET:
                        execution = await self._execute_market_order(order, current_price, market_data)
                    elif order.order_type == OrderType.LIMIT:
                        execution = await self._execute_limit_order(order, current_price)
                    elif order.order_type == OrderType.STOP:
                        execution = await self._execute_stop_order(order, current_price)
                    elif order.order_type == OrderType.STOP_LIMIT:
                        execution = await self._execute_stop_limit_order(order, current_price)
                    else:
                        execution = {
                            'success': False,
                            'error': f'Unknown order type: {order.order_type}'
                        }
                    
                    if execution['success']:
                        # Update order
                        order.update_status(
                            status=OrderStatus.FILLED,
                            filled_qty=order.quantity,
                            fill_price=execution['fill_price'],
                            fees=execution['fees'],
                            slippage=execution['slippage']
                        )
                        
                        # Add to history
                        self.order_history.append(order)
                        
                        # Calculate execution time
                        execution_time = (datetime.now() - start_time).total_seconds()
                        
                        return {
                            'success': True,
                            'order_id': order.order_id,
                            'status': 'FILLED',
                            'filled_quantity': order.quantity,
                            'fill_price': execution['fill_price'],
                            'fees': execution['fees'],
                            'slippage': execution['slippage'],
                            'execution_time': execution_time,
                            'timestamp': datetime.now()
                        }
                    
                    else:
                        logger.warning(f"Execution attempt {attempt + 1} failed: {execution['error']}")
                        
                        # Wait before retry
                        if attempt < self.max_retries - 1:
                            await asyncio.sleep(0.5 * (attempt + 1))
                
                except Exception as e:
                    logger.error(f"Error during execution attempt {attempt + 1}: {str(e)}")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep(1.0 * (attempt + 1))
            
            # All retries failed
            order.update_status(status=OrderStatus.REJECTED)
            
            return {
                'success': False,
                'order_id': order.order_id,
                'status': 'REJECTED',
                'error': 'All execution attempts failed',
                'timestamp': datetime.now()
            }
        
        except Exception as e:
            logger.error(f"Unexpected error executing order {order.order_id}: {str(e)}")
            order.update_status(status=OrderStatus.REJECTED)
            
            return {
                'success': False,
                'order_id': order.order_id,
                'status': 'REJECTED',
                'error': f'Unexpected error: {str(e)}',
                'timestamp': datetime.now()
            }
    
    async def _get_current_price(self, symbol: str) -> float:
        """Get current market price"""
        
        # Check cache
        cache_key = f"price_{symbol}"
        if cache_key in self.market_data_cache:
            cached_price, cached_time = self.market_data_cache[cache_key]
            if datetime.now() - cached_time < self.cache_ttl:
                return cached_price
        
        # Get fresh price (simulated for now)
        # In practice, would call API
        price = 1800.0  # Placeholder for XAUUSD
        
        # Update cache
        self.market_data_cache[cache_key] = (price, datetime.now())
        
        return price
    
    async def _execute_market_order(self,
                                  order: Order,
                                  current_price: float,
                                  market_data: Dict) -> Dict:
        """Execute market order"""
        
        # Calculate fill price with slippage
        estimated_slippage = self._estimate_slippage(
            {'units': order.quantity},
            market_data
        )
        
        if order.side == 'BUY':
            fill_price = current_price * (1 + estimated_slippage)
        else:
            fill_price = current_price * (1 - estimated_slippage)
        
        # Calculate fees
        fees = self._calculate_fees(order, fill_price)
        
        return {
            'success': True,
            'fill_price': fill_price,
            'fees': fees,
            'slippage': estimated_slippage
        }
    
    async def _execute_limit_order(self,
                                 order: Order,
                                 current_price: float) -> Dict:
        """Execute limit order"""
        
        if order.limit_price is None:
            return {'success': False, 'error': 'No limit price specified'}
        
        # Check if limit price is reachable
        if order.side == 'BUY':
            # Buy limit - price must be below current
            if order.limit_price > current_price:
                return {'success': False, 'error': 'Limit price above current price'}
        else:
            # Sell limit - price must be above current
            if order.limit_price < current_price:
                return {'success': False, 'error': 'Limit price below current price'}
        
        # Simulate fill at limit price
        fill_price = order.limit_price
        fees = self._calculate_fees(order, fill_price)
        
        return {
            'success': True,
            'fill_price': fill_price,
            'fees': fees,
            'slippage': 0.0  # Limit orders have no slippage
        }
    
    async def _execute_stop_order(self,
                                order: Order,
                                current_price: float) -> Dict:
        """Execute stop order"""
        
        if order.stop_price is None:
            return {'success': False, 'error': 'No stop price specified'}
        
        # Check if stop price is triggered
        if order.side == 'BUY':
            # Buy stop - price must be above stop
            if current_price < order.stop_price:
                return {'success': False, 'error': 'Stop price not triggered'}
        else:
            # Sell stop - price must be below stop
            if current_price > order.stop_price:
                return {'success': False, 'error': 'Stop price not triggered'}
        
        # Execute as market order once triggered
        # In practice, would convert to market order
        
        return {
            'success': True,
            'fill_price': current_price,
            'fees': self._calculate_fees(order, current_price),
            'slippage': self.max_slippage_pct
        }
    
    async def _execute_stop_limit_order(self,
                                      order: Order,
                                      current_price: float) -> Dict:
        """Execute stop limit order"""
        
        if order.stop_price is None or order.limit_price is None:
            return {'success': False, 'error': 'Missing stop or limit price'}
        
        # Check if stop price is triggered
        if order.side == 'BUY':
            # Buy stop limit - price must be above stop
            if current_price < order.stop_price:
                return {'success': False, 'error': 'Stop price not triggered'}
        else:
            # Sell stop limit - price must be below stop
            if current_price > order.stop_price:
                return {'success': False, 'error': 'Stop price not triggered'}
        
        # Once triggered, execute as limit order
        return await self._execute_limit_order(order, current_price)
    
    def _calculate_fees(self, order: Order, fill_price: float) -> float:
        """Calculate fees for order"""
        
        # Fee structure (can be customized)
        base_fee_pct = 0.001  # 0.1%
        min_fee = 1.0  # $1 minimum
        max_fee = 50.0  # $50 maximum
        
        # Calculate fee
        order_value = order.quantity * fill_price
        fee = order_value * base_fee_pct
        
        # Apply min/max
        fee = max(min_fee, min(fee, max_fee))
        
        return fee
    
    def _update_execution_metrics(self, execution_result: Dict):
        """Update execution metrics"""
        
        self.execution_metrics['total_orders'] += 1
        
        if execution_result.get('success', False):
            self.execution_metrics['successful_orders'] += 1
            
            # Update average slippage
            current_avg = self.execution_metrics['avg_slippage']
            total_successful = self.execution_metrics['successful_orders']
            new_slippage = execution_result.get('slippage', 0.0)
            
            self.execution_metrics['avg_slippage'] = (
                (current_avg * (total_successful - 1) + new_slippage) / total_successful
            )
            
            # Update average fill time
            current_avg_time = self.execution_metrics['avg_fill_time']
            new_fill_time = execution_result.get('execution_time', 0.0)
            
            self.execution_metrics['avg_fill_time'] = (
                (current_avg_time * (total_successful - 1) + new_fill_time) / total_successful
            )
            
            # Update total fees
            self.execution_metrics['total_fees'] += execution_result.get('fees', 0.0)
        else:
            self.execution_metrics['failed_orders'] += 1
    
    async def cancel_order(self, order_id: str) -> Dict:
        """Cancel pending order"""
        
        if order_id not in self.orders:
            return {
                'success': False,
                'error': f'Order {order_id} not found',
                'timestamp': datetime.now()
            }
        
        order = self.orders[order_id]
        
        # Check if order can be cancelled
        if order.status != OrderStatus.PENDING:
            return {
                'success': False,
                'error': f'Order {order_id} cannot be cancelled (status: {order.status.value})',
                'timestamp': datetime.now()
            }
        
        # Cancel order
        order.update_status(status=OrderStatus.CANCELLED)
        
        logger.info(f"Cancelled order: {order_id}")
        
        return {
            'success': True,
            'order_id': order_id,
            'status': 'CANCELLED',
            'timestamp': datetime.now()
        }
    
    def get_order_status(self, order_id: str) -> Optional[Dict]:
        """Get order status"""
        
        if order_id not in self.orders:
            return None
        
        order = self.orders[order_id]
        
        return {
            'order_id': order.order_id,
            'symbol': order.symbol,
            'order_type': order.order_type.value,
            'side': order.side,
            'quantity': order.quantity,
            'filled_quantity': order.filled_quantity,
            'status': order.status.value,
            'price': order.price,
            'limit_price': order.limit_price,
            'stop_price': order.stop_price,
            'average_fill_price': order.average_fill_price,
            'fees': order.fees,
            'slippage': order.slippage,
            'created_at': order.created_at,
            'updated_at': order.updated_at
        }
    
    def get_execution_report(self) -> Dict:
        """Get execution report"""
        
        return {
            'execution_metrics': self.execution_metrics,
            'daily_orders': self.daily_order_count,
            'pending_orders': len([o for o in self.orders.values() 
                                 if o.status == OrderStatus.PENDING]),
            'open_orders': len([o for o in self.orders.values() 
                              if o.status in [OrderStatus.PENDING, OrderStatus.PARTIALLY_FILLED]]),
            'order_history_summary': {
                'total_orders': len(self.order_history),
                'recent_orders': [
                    {
                        'order_id': o.order_id,
                        'symbol': o.symbol,
                        'side': o.side,
                        'status': o.status.value,
                        'fill_price': o.average_fill_price,
                        'timestamp': o.updated_at
                    }
                    for o in self.order_history[-10:]  # Last 10 orders
                ]
            }
        }
    
    async def close(self):
        """Close execution engine"""
        
        # Cancel all pending orders
        pending_orders = [
            order_id for order_id, order in self.orders.items()
            if order.status == OrderStatus.PENDING
        ]
        
        for order_id in pending_orders:
            await self.cancel_order(order_id)
        
        # Close session
        if self.session:
            await self.session.close()
        
        logger.info("Execution engine closed")
# core/market_regime_detector.py
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from scipy import stats
from enum import Enum

class MarketRegime(Enum):
    """Market regime enumeration"""
    STRONG_BULL = "STRONG_BULL"
    BULL = "BULL"
    NEUTRAL = "NEUTRAL"
    BEAR = "BEAR"
    STRONG_BEAR = "STRONG_BEAR"
    HIGH_VOLATILITY = "HIGH_VOLATILITY"
    LOW_VOLATILITY = "LOW_VOLATILITY"
    RANGING = "RANGING"
    BREAKOUT = "BREAKOUT"
    TRENDING = "TRENDING"

class MarketRegimeDetector:
    """Advanced market regime detection với ML và statistical methods"""
    
    def __init__(self, 
                 lookback_period: int = 100,
                 volatility_threshold: float = 0.02,
                 adx_threshold: int = 25):
        
        self.lookback_period = lookback_period
        self.volatility_threshold = volatility_threshold
        self.adx_threshold = adx_threshold
        
        # Historical regime data
        self.regime_history = []
        self.regime_transitions = []
        
        # Statistical models
        self.hmm_model = None  # Hidden Markov Model
        self.gmm_model = None  # Gaussian Mixture Model
        
        # Feature cache
        self.feature_cache = {}
        
    def detect_regime(self, 
                     data: pd.DataFrame,
                     timeframe: str = '1h') -> Dict:
        """Detect current market regime với multiple methods"""
        
        if len(data) < self.lookback_period:
            return self._get_default_regime()
        
        # Calculate features
        features = self._calculate_features(data)
        
        # Detect regime using multiple methods
        regimes = {}
        
        # 1. Statistical method (mean-reversion vs trending)
        regimes['statistical'] = self._detect_statistical_regime(data, features)
        
        # 2. Volatility-based regime
        regimes['volatility'] = self._detect_volatility_regime(features)
        
        # 3. Trend-based regime
        regimes['trend'] = self._detect_trend_regime(data, features)
        
        # 4. Price action regime
        regimes['price_action'] = self._detect_price_action_regime(data)
        
        # 5. Volume-based regime
        if 'volume' in data.columns:
            regimes['volume'] = self._detect_volume_regime(data)
        
        # Combine regimes (voting system)
        final_regime = self._combine_regimes(regimes)
        
        # Store in history
        regime_record = {
            'timestamp': datetime.now(),
            'timeframe': timeframe,
            'regime': final_regime,
            'regime_detail': regimes,
            'features': features
        }
        
        self.regime_history.append(regime_record)
        
        # Keep history manageable
        if len(self.regime_history) > 1000:
            self.regime_history = self.regime_history[-1000:]
        
        return regime_record
    
    def _calculate_features(self, data: pd.DataFrame) -> Dict:
        """Tính toán features cho regime detection"""
        
        if len(data) < 50:
            return {}
        
        features = {}
        
        # Price features
        prices = data['close'].values
        
        # Returns features
        returns = np.diff(prices) / prices[:-1]
        features['returns_mean'] = np.mean(returns[-20:]) if len(returns) >= 20 else np.mean(returns)
        features['returns_std'] = np.std(returns[-20:]) if len(returns) >= 20 else np.std(returns)
        features['returns_skew'] = stats.skew(returns[-50:]) if len(returns) >= 50 else 0
        features['returns_kurtosis'] = stats.kurtosis(returns[-50:]) if len(returns) >= 50 else 0
        
        # Volatility features
        features['volatility_short'] = np.std(returns[-10:]) * np.sqrt(252) if len(returns) >= 10 else 0
        features['volatility_long'] = np.std(returns[-50:]) * np.sqrt(252) if len(returns) >= 50 else 0
        features['volatility_ratio'] = features['volatility_short'] / max(features['volatility_long'], 0.001)
        
        # Trend features
        if len(prices) >= 20:
            # Linear regression slope
            x = np.arange(len(prices[-20:]))
            slope, _ = np.polyfit(x, prices[-20:], 1)
            features['trend_slope'] = slope / np.mean(prices[-20:])
            
            # ADX calculation (simplified)
            features['trend_strength'] = self._calculate_adx_simplified(data.tail(20))
        
        # Range features
        if len(prices) >= 20:
            high_20 = np.max(prices[-20:])
            low_20 = np.min(prices[-20:])
            features['range_pct'] = (high_20 - low_20) / np.mean(prices[-20:])
            features['position_in_range'] = (prices[-1] - low_20) / (high_20 - low_20) if high_20 != low_20 else 0.5
        
        # Volume features (if available)
        if 'volume' in data.columns:
            volume = data['volume'].values
            if len(volume) >= 20:
                features['volume_mean'] = np.mean(volume[-20:])
                features['volume_std'] = np.std(volume[-20:])
                features['volume_ratio'] = volume[-1] / features['volume_mean'] if features['volume_mean'] > 0 else 1.0
        
        # Technical indicator features
        features['rsi'] = self._calculate_rsi(prices, 14) if len(prices) >= 15 else 50
        features['macd_signal'] = self._calculate_macd_signal(prices) if len(prices) >= 26 else 0
        
        return features
    
    def _calculate_adx_simplified(self, data: pd.DataFrame) -> float:
        """Simplified ADX calculation"""
        
        if len(data) < 14:
            return 0.0
        
        highs = data['high'].values
        lows = data['low'].values
        
        # Calculate directional movement
        plus_dm = highs[1:] - highs[:-1]
        minus_dm = lows[:-1] - lows[1:]
        
        plus_dm = np.where((plus_dm > minus_dm) & (plus_dm > 0), plus_dm, 0)
        minus_dm = np.where((minus_dm > plus_dm) & (minus_dm > 0), minus_dm, 0)
        
        # Calculate True Range
        tr1 = highs[1:] - lows[1:]
        tr2 = np.abs(highs[1:] - data['close'].values[:-1])
        tr3 = np.abs(lows[1:] - data['close'].values[:-1])
        tr = np.maximum(np.maximum(tr1, tr2), tr3)
        
        # Calculate smoothed averages
        atr = np.mean(tr[-14:]) if len(tr) >= 14 else np.mean(tr)
        
        plus_di = np.mean(plus_dm[-14:]) / atr * 100 if atr > 0 else 0
        minus_di = np.mean(minus_dm[-14:]) / atr * 100 if atr > 0 else 0
        
        # Calculate DX and ADX
        dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di) if (plus_di + minus_di) > 0 else 0
        adx = np.mean([dx] * 14)  # Simplified
        
        return adx
    
    def _calculate_rsi(self, prices: np.ndarray, period: int = 14) -> float:
        """Calculate RSI"""
        
        if len(prices) < period + 1:
            return 50.0
        
        deltas = np.diff(prices)
        seed = deltas[:period]
        
        up = seed[seed >= 0].sum() / period
        down = -seed[seed < 0].sum() / period
        
        for i in range(period, len(deltas)):
            delta = deltas[i]
            if delta > 0:
                upval = delta
                downval = 0.0
            else:
                upval = 0.0
                downval = -delta
            
            up = (up * (period - 1) + upval) / period
            down = (down * (period - 1) + downval) / period
        
        rs = up / down if down != 0 else 1.0
        rsi = 100.0 - 100.0 / (1.0 + rs)
        
        return rsi
    
    def _calculate_macd_signal(self, prices: np.ndarray) -> float:
        """Calculate MACD signal"""
        
        if len(prices) < 26:
            return 0.0
        
        # Calculate EMAs
        ema12 = self._calculate_ema(prices, 12)
        ema26 = self._calculate_ema(prices, 26)
        
        # MACD line
        macd_line = ema12 - ema26
        
        # Signal line (EMA of MACD)
        signal_line = self._calculate_ema(macd_line, 9)
        
        # MACD histogram
        histogram = macd_line - signal_line
        
        return histogram[-1] if len(histogram) > 0 else 0.0
    
    def _calculate_ema(self, data: np.ndarray, period: int) -> np.ndarray:
        """Calculate EMA"""
        
        if len(data) < period:
            return np.array([np.mean(data)] * len(data))
        
        alpha = 2.0 / (period + 1.0)
        ema = np.zeros_like(data)
        ema[0] = data[0]
        
        for i in range(1, len(data)):
            ema[i] = alpha * data[i] + (1 - alpha) * ema[i-1]
        
        return ema
    
    def _detect_statistical_regime(self, 
                                  data: pd.DataFrame,
                                  features: Dict) -> MarketRegime:
        """Detect regime using statistical methods"""
        
        returns_mean = features.get('returns_mean', 0)
        returns_std = features.get('returns_std', 0.01)
        trend_slope = features.get('trend_slope', 0)
        
        # Z-score of returns
        z_score = returns_mean / max(returns_std, 0.001)
        
        # Detect regime
        if abs(z_score) > 2.0:
            if z_score > 0:
                return MarketRegime.STRONG_BULL
            else:
                return MarketRegime.STRONG_BEAR
        elif abs(z_score) > 1.0:
            if z_score > 0:
                return MarketRegime.BULL
            else:
                return MarketRegime.BEAR
        elif abs(trend_slope) > 0.001:
            return MarketRegime.TRENDING
        else:
            return MarketRegime.RANGING
    
    def _detect_volatility_regime(self, features: Dict) -> MarketRegime:
        """Detect volatility regime"""
        
        volatility_short = features.get('volatility_short', 0)
        volatility_ratio = features.get('volatility_ratio', 1.0)
        
        if volatility_short > self.volatility_threshold * 1.5:
            return MarketRegime.HIGH_VOLATILITY
        elif volatility_short < self.volatility_threshold * 0.5:
            return MarketRegime.LOW_VOLATILITY
        elif volatility_ratio > 1.5:
            return MarketRegime.BREAKOUT
        else:
            return MarketRegime.NEUTRAL
    
    def _detect_trend_regime(self, 
                            data: pd.DataFrame,
                            features: Dict) -> MarketRegime:
        """Detect trend regime"""
        
        trend_strength = features.get('trend_strength', 0)
        rsi = features.get('rsi', 50)
        
        if trend_strength > self.adx_threshold:
            # Strong trend
            if rsi > 50:
                return MarketRegime.BULL
            else:
                return MarketRegime.BEAR
        elif trend_strength > self.adx_threshold * 0.5:
            # Moderate trend
            return MarketRegime.TRENDING
        else:
            # Weak or no trend
            return MarketRegime.RANGING
    
    def _detect_price_action_regime(self, data: pd.DataFrame) -> MarketRegime:
        """Detect regime from price action"""
        
        if len(data) < 20:
            return MarketRegime.NEUTRAL
        
        prices = data['close'].values[-20:]
        
        # Check for breakout patterns
        recent_high = np.max(prices[-5:])
        recent_low = np.min(prices[-5:])
        
        if prices[-1] > np.max(prices[-10:-5]) * 1.02:  # 2% breakout
            return MarketRegime.BREAKOUT
        elif prices[-1] < np.min(prices[-10:-5]) * 0.98:  # 2% breakdown
            return MarketRegime.BREAKOUT
        
        # Check for ranging
        range_pct = (np.max(prices) - np.min(prices)) / np.mean(prices)
        if range_pct < 0.02:  # Less than 2% range
            return MarketRegime.RANGING
        
        return MarketRegime.NEUTRAL
    
    def _detect_volume_regime(self, data: pd.DataFrame) -> MarketRegime:
        """Detect regime from volume"""
        
        if 'volume' not in data.columns or len(data) < 20:
            return MarketRegime.NEUTRAL
        
        volume = data['volume'].values[-20:]
        prices = data['close'].values[-20:]
        
        # Volume trend
        volume_ma_short = np.mean(volume[-5:])
        volume_ma_long = np.mean(volume[-20:])
        
        # Price-volume correlation
        price_changes = np.diff(prices) / prices[:-1]
        volume_changes = np.diff(volume) / volume[:-1]
        
        if len(price_changes) > 5 and len(volume_changes) > 5:
            correlation = np.corrcoef(price_changes[-5:], volume_changes[-5:])[0, 1]
        else:
            correlation = 0
        
        if volume_ma_short > volume_ma_long * 1.5 and correlation > 0.5:
            # High volume with positive correlation
            return MarketRegime.BREAKOUT
        elif volume_ma_short < volume_ma_long * 0.7:
            # Low volume
            return MarketRegime.RANGING
        
        return MarketRegime.NEUTRAL
    
    def _combine_regimes(self, regimes: Dict[str, MarketRegime]) -> MarketRegime:
        """Combine multiple regime detections"""
        
        if not regimes:
            return MarketRegime.NEUTRAL
        
        # Count occurrences of each regime
        regime_counts = {}
        for method, regime in regimes.items():
            regime_counts[regime] = regime_counts.get(regime, 0) + 1
        
        # Find most common regime
        most_common = max(regime_counts.items(), key=lambda x: x[1])[0]
        
        # Check for conflicting regimes
        if len(set(regimes.values())) > 1:
            # Multiple regimes detected, use priority system
            regime_priority = {
                MarketRegime.STRONG_BULL: 10,
                MarketRegime.STRONG_BEAR: 10,
                MarketRegime.HIGH_VOLATILITY: 9,
                MarketRegime.BREAKOUT: 8,
                MarketRegime.BULL: 7,
                MarketRegime.BEAR: 7,
                MarketRegime.TRENDING: 6,
                MarketRegime.RANGING: 5,
                MarketRegime.LOW_VOLATILITY: 4,
                MarketRegime.NEUTRAL: 3
            }
            
            # Get regime with highest priority
            most_common = max(regimes.values(), key=lambda r: regime_priority.get(r, 0))
        
        return most_common
    
    def _get_default_regime(self) -> Dict:
        """Get default regime when insufficient data"""
        
        return {
            'timestamp': datetime.now(),
            'regime': MarketRegime.NEUTRAL,
            'regime_detail': {'default': MarketRegime.NEUTRAL},
            'features': {},
            'confidence': 0.5
        }
    
    def get_regime_transitions(self, 
                              lookback_days: int = 30) -> List[Dict]:
        """Get recent regime transitions"""
        
        if not self.regime_history:
            return []
        
        transitions = []
        
        for i in range(1, len(self.regime_history)):
            prev_regime = self.regime_history[i-1]['regime']
            curr_regime = self.regime_history[i]['regime']
            
            if prev_regime != curr_regime:
                transition = {
                    'timestamp': self.regime_history[i]['timestamp'],
                    'from_regime': prev_regime.value,
                    'to_regime': curr_regime.value,
                    'duration_seconds': (
                        self.regime_history[i]['timestamp'] - 
                        self.regime_history[i-1]['timestamp']
                    ).total_seconds()
                }
                transitions.append(transition)
        
        # Filter by lookback period
        cutoff_time = datetime.now() - timedelta(days=lookback_days)
        recent_transitions = [
            t for t in transitions 
            if t['timestamp'] > cutoff_time
        ]
        
        return recent_transitions
    
    def get_regime_statistics(self) -> Dict:
        """Get regime statistics"""
        
        if not self.regime_history:
            return {}
        
        # Count regime occurrences
        regime_counts = {}
        for record in self.regime_history:
            regime = record['regime'].value
            regime_counts[regime] = regime_counts.get(regime, 0) + 1
        
        # Calculate regime durations
        regime_durations = {}
        current_regime = None
        regime_start = None
        
        for record in self.regime_history:
            regime = record['regime'].value
            
            if regime != current_regime:
                if current_regime is not None:
                    duration = (record['timestamp'] - regime_start).total_seconds()
                    if current_regime not in regime_durations:
                        regime_durations[current_regime] = []
                    regime_durations[current_regime].append(duration)
                
                current_regime = regime
                regime_start = record['timestamp']
        
        # Calculate average durations
        avg_durations = {}
        for regime, durations in regime_durations.items():
            avg_durations[regime] = np.mean(durations) if durations else 0
        
        return {
            'total_records': len(self.regime_history),
            'regime_counts': regime_counts,
            'regime_percentages': {
                regime: count / len(self.regime_history) 
                for regime, count in regime_counts.items()
            },
            'avg_durations': avg_durations,
            'recent_transitions': self.get_regime_transitions(lookback_days=7)
        }
# core/ml_pipeline.py
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any, Union
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ML Libraries
import joblib
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier, 
    StackingClassifier, VotingClassifier
)
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, roc_auc_score, confusion_matrix,
    classification_report
)
import xgboost as xgb
import lightgbm as lgb
import catboost as cb

# Deep Learning
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, callbacks
    HAS_TF = True
except ImportError:
    HAS_TF = False

class MLPipeline:
    """Advanced ML pipeline cho trading với multiple models và ensemble learning"""
    
    def __init__(self, 
                 feature_config: Optional[Dict] = None,
                 model_config: Optional[Dict] = None,
                 use_gpu: bool = False):
        
        self.feature_config = feature_config or self._default_feature_config()
        self.model_config = model_config or self._default_model_config()
        self.use_gpu = use_gpu
        
        # Feature engineering
        self.scaler = StandardScaler()
        self.feature_selector = None
        self.feature_importance = {}
        
        # Models
        self.models = {}
        self.ensemble_model = None
        self.best_model = None
        
        # Training data
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.y_test = None
        
        # Feature columns
        self.feature_columns = []
        
        # Model performance tracking
        self.model_performance = {}
        self.training_history = []
        
        # Cross-validation
        self.cv_scores = {}
        
        # Hyperparameter search
        self.best_params = {}
        
        # Prediction cache
        self.prediction_cache = {}
        self.cache_ttl = timedelta(minutes=5)
        
        # MLflow integration (optional)
        self.use_mlflow = False
        
    def _default_feature_config(self) -> Dict:
        """Default feature engineering configuration"""
        return {
            'feature_window': 50,
            'target_horizon': 5,
            'feature_types': ['price', 'volume', 'technical', 'statistical', 'time'],
            'feature_scaling': 'standard',  # standard, minmax, robust
            'feature_selection': True,
            'n_features': 50,
            'lag_features': [1, 2, 3, 5, 10, 20],
            'rolling_windows': [5, 10, 20, 50],
            'include_volatility_features': True,
            'include_correlation_features': True,
            'include_fourier_features': False
        }
    
    def _default_model_config(self) -> Dict:
        """Default model configuration"""
        return {
            'base_models': {
                'random_forest': {
                    'enabled': True,
                    'n_estimators': 100,
                    'max_depth': 10,
                    'random_state': 42
                },
                'xgboost': {
                    'enabled': True,
                    'n_estimators': 100,
                    'max_depth': 6,
                    'learning_rate': 0.1
                },
                'lightgbm': {
                    'enabled': True,
                    'n_estimators': 100,
                    'max_depth': 7,
                    'learning_rate': 0.1
                },
                'gradient_boosting': {
                    'enabled': True,
                    'n_estimators': 100,
                    'max_depth': 5,
                    'learning_rate': 0.1
                },
                'neural_network': {
                    'enabled': HAS_TF,
                    'hidden_layers': [64, 32, 16],
                    'dropout_rate': 0.2,
                    'epochs': 50,
                    'batch_size': 32
                }
            },
            'ensemble_method': 'voting',  # voting, stacking, blending
            'voting_type': 'soft',  # hard, soft
            'stacking_method': 'logistic_regression',
            'use_deep_learning': HAS_TORCH,
            'deep_learning_config': {
                'model_type': 'lstm',  # lstm, cnn, transformer
                'hidden_size': 64,
                'num_layers': 2,
                'dropout': 0.3,
                'learning_rate': 0.001,
                'epochs': 100,
                'batch_size': 32
            },
            'cross_validation': {
                'n_splits': 5,
                'test_size': 0.2
            },
            'hyperparameter_tuning': {
                'enabled': True,
                'method': 'random_search',  # grid, random, bayesian
                'n_iter': 50
            }
        }
    
    def prepare_features(self, 
                        data: pd.DataFrame,
                        target_column: str = 'target') -> Tuple[pd.DataFrame, pd.Series]:
        """Chuẩn bị features từ raw data"""
        
        if len(data) < self.feature_config['feature_window']:
            raise ValueError(f"Need at least {self.feature_config['feature_window']} data points")
        
        # 1. Create basic features
        features = self._create_basic_features(data)
        
        # 2. Create technical indicator features
        features = self._create_technical_features(features, data)
        
        # 3. Create statistical features
        features = self._create_statistical_features(features, data)
        
        # 4. Create time-based features
        features = self._create_time_features(features, data)
        
        # 5. Create volatility features
        if self.feature_config['include_volatility_features']:
            features = self._create_volatility_features(features, data)
        
        # 6. Create lag features
        features = self._create_lag_features(features)
        
        # 7. Create rolling window features
        features = self._create_rolling_features(features)
        
        # 8. Create target variable
        target = self._create_target_variable(data, target_column)
        
        # Align features and target
        aligned_data = self._align_features_target(features, target)
        
        # Store feature columns
        self.feature_columns = list(aligned_data['features'].columns)
        
        return aligned_data['features'], aligned_data['target']
    
    def _create_basic_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Tạo basic price và volume features"""
        
        features = pd.DataFrame(index=data.index)
        
        # Price features
        if 'close' in data.columns:
            features['price'] = data['close']
            features['log_price'] = np.log(data['close'])
            
            # Returns
            features['returns_1'] = data['close'].pct_change(1)
            features['returns_5'] = data['close'].pct_change(5)
            features['returns_10'] = data['close'].pct_change(10)
            features['returns_20'] = data['close'].pct_change(20)
            
            # Log returns
            features['log_returns_1'] = np.log(data['close'] / data['close'].shift(1))
            features['log_returns_5'] = np.log(data['close'] / data['close'].shift(5))
            
            # Price position
            if len(data) >= 20:
                features['price_position_20'] = (
                    data['close'] - data['close'].rolling(20).min()
                ) / (
                    data['close'].rolling(20).max() - data['close'].rolling(20).min()
                ).replace([np.inf, -np.inf], 0).fillna(0)
        
        # OHLC relationships
        if all(col in data.columns for col in ['open', 'high', 'low', 'close']):
            features['ohlc_ratio'] = (data['close'] - data['open']) / (data['high'] - data['low']).replace(0, 1)
            features['body_size'] = abs(data['close'] - data['open']) / data['close']
            features['upper_shadow'] = (data['high'] - data[['open', 'close']].max(axis=1)) / data['close']
            features['lower_shadow'] = (data[['open', 'close']].min(axis=1) - data['low']) / data['close']
        
        # Volume features
        if 'volume' in data.columns:
            features['volume'] = data['volume']
            features['log_volume'] = np.log(data['volume'] + 1)  # Add 1 to avoid log(0)
            
            # Volume indicators
            features['volume_sma_5'] = data['volume'].rolling(5).mean()
            features['volume_sma_20'] = data['volume'].rolling(20).mean()
            features['volume_ratio'] = data['volume'] / features['volume_sma_20'].replace(0, 1)
            features['volume_change'] = data['volume'].pct_change()
        
        return features.dropna()
    
    def _create_technical_features(self, 
                                  features: pd.DataFrame,
                                  data: pd.DataFrame) -> pd.DataFrame:
        """Tạo technical indicator features"""
        
        # Moving averages
        if 'close' in data.columns:
            for window in [5, 10, 20, 50, 100, 200]:
                if len(data) >= window:
                    features[f'sma_{window}'] = data['close'].rolling(window).mean()
                    features[f'ema_{window}'] = data['close'].ewm(span=window, adjust=False).mean()
                    
                    # Price relative to MA
                    features[f'price_vs_sma_{window}'] = data['close'] / features[f'sma_{window}'] - 1
                    features[f'price_vs_ema_{window}'] = data['close'] / features[f'ema_{window}'] - 1
        
        # RSI
        if 'close' in data.columns and len(data) >= 15:
            features['rsi'] = self._calculate_rsi(data['close'], 14)
        
        # MACD
        if 'close' in data.columns and len(data) >= 26:
            macd_line, signal_line, histogram = self._calculate_macd(data['close'])
            features['macd_line'] = macd_line
            features['macd_signal'] = signal_line
            features['macd_histogram'] = histogram
        
        # Bollinger Bands
        if 'close' in data.columns and len(data) >= 20:
            bb_upper, bb_middle, bb_lower = self._calculate_bollinger_bands(data['close'], 20, 2)
            features['bb_upper'] = bb_upper
            features['bb_middle'] = bb_middle
            features['bb_lower'] = bb_lower
            features['bb_position'] = (data['close'] - bb_lower) / (bb_upper - bb_lower).replace(0, 1)
        
        # ATR (Average True Range)
        if all(col in data.columns for col in ['high', 'low', 'close']):
            atr = self._calculate_atr(data['high'], data['low'], data['close'], 14)
            features['atr'] = atr
            features['atr_pct'] = atr / data['close']
        
        return features.dropna()
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """Calculate RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_macd(self, prices: pd.Series) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """Calculate MACD"""
        exp1 = prices.ewm(span=12, adjust=False).mean()
        exp2 = prices.ewm(span=26, adjust=False).mean()
        macd_line = exp1 - exp2
        signal_line = macd_line.ewm(span=9, adjust=False).mean()
        histogram = macd_line - signal_line
        return macd_line, signal_line, histogram
    
    def _calculate_bollinger_bands(self, 
                                  prices: pd.Series, 
                                  window: int = 20, 
                                  num_std: float = 2) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """Calculate Bollinger Bands"""
        middle_band = prices.rolling(window=window).mean()
        std = prices.rolling(window=window).std()
        upper_band = middle_band + (std * num_std)
        lower_band = middle_band - (std * num_std)
        return upper_band, middle_band, lower_band
    
    def _calculate_atr(self, 
                      high: pd.Series, 
                      low: pd.Series, 
                      close: pd.Series, 
                      period: int = 14) -> pd.Series:
        """Calculate Average True Range"""
        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = tr.rolling(period).mean()
        return atr
    
    def _create_statistical_features(self, 
                                    features: pd.DataFrame,
                                    data: pd.DataFrame) -> pd.DataFrame:
        """Tạo statistical features"""
        
        if 'close' in data.columns:
            # Volatility measures
            returns = data['close'].pct_change().dropna()
            
            # Historical volatility
            for window in [5, 10, 20, 50]:
                if len(returns) >= window:
                    features[f'volatility_{window}'] = returns.rolling(window).std() * np.sqrt(252)
            
            # Skewness and kurtosis
            for window in [20, 50]:
                if len(returns) >= window:
                    features[f'skewness_{window}'] = returns.rolling(window).skew()
                    features[f'kurtosis_{window}'] = returns.rolling(window).kurt()
            
            # Autocorrelation
            for lag in [1, 5, 10]:
                if len(returns) >= 20:
                    features[f'autocorr_{lag}'] = returns.rolling(20).apply(
                        lambda x: x.autocorr(lag=lag), raw=False
                    )
            
            # Hurst exponent (simplified)
            if len(returns) >= 100:
                features['hurst'] = returns.rolling(100).apply(
                    self._calculate_hurst_simplified, raw=False
                )
        
        return features.dropna()
    
    def _calculate_hurst_simplified(self, returns: pd.Series) -> float:
        """Simplified Hurst exponent calculation"""
        try:
            lags = range(2, 20)
            tau = [np.sqrt(np.std(np.subtract(returns[lag:], returns[:-lag]))) for lag in lags]
            poly = np.polyfit(np.log(lags), np.log(tau), 1)
            return poly[0] * 2.0
        except:
            return 0.5
    
    def _create_time_features(self, 
                             features: pd.DataFrame,
                             data: pd.DataFrame) -> pd.DataFrame:
        """Tạo time-based features"""
        
        if not data.index.empty:
            # Time of day features
            if hasattr(data.index, 'hour'):
                features['hour'] = data.index.hour
                features['minute'] = data.index.minute
                
                # Cyclical encoding for hour
                features['hour_sin'] = np.sin(2 * np.pi * data.index.hour / 24)
                features['hour_cos'] = np.cos(2 * np.pi * data.index.hour / 24)
            
            # Day of week features
            if hasattr(data.index, 'dayofweek'):
                features['dayofweek'] = data.index.dayofweek
                
                # Cyclical encoding for day of week
                features['day_sin'] = np.sin(2 * np.pi * data.index.dayofweek / 7)
                features['day_cos'] = np.cos(2 * np.pi * data.index.dayofweek / 7)
            
            # Month features
            if hasattr(data.index, 'month'):
                features['month'] = data.index.month
                
                # Cyclical encoding for month
                features['month_sin'] = np.sin(2 * np.pi * data.index.month / 12)
                features['month_cos'] = np.cos(2 * np.pi * data.index.month / 12)
            
            # Quarter features
            if hasattr(data.index, 'quarter'):
                features['quarter'] = data.index.quarter
        
        # Time since important events
        features['days_since_start'] = (data.index - data.index[0]).days
        
        return features
    
    def _create_volatility_features(self, 
                                   features: pd.DataFrame,
                                   data: pd.DataFrame) -> pd.DataFrame:
        """Tạo volatility features"""
        
        if 'close' in data.columns:
            returns = data['close'].pct_change().dropna()
            
            # GARCH-like volatility features
            for window in [5, 10, 20]:
                if len(returns) >= window:
                    # Rolling volatility
                    features[f'rolling_vol_{window}'] = returns.rolling(window).std()
                    
                    # Volatility ratio (short/long)
                    if window >= 10:
                        short_vol = returns.rolling(window//2).std()
                        long_vol = returns.rolling(window).std()
                        features[f'vol_ratio_{window}'] = short_vol / long_vol.replace(0, 1)
            
            # Parkinson volatility estimator
            if all(col in data.columns for col in ['high', 'low']):
                parkinson_vol = np.sqrt(
                    (1 / (4 * np.log(2))) * 
                    ((np.log(data['high'] / data['low'])) ** 2).rolling(20).mean()
                )
                features['parkinson_vol'] = parkinson_vol
            
            # Realized volatility
            features['realized_vol_5'] = returns.rolling(5).apply(
                lambda x: np.sqrt(np.sum(x ** 2)), raw=False
            )
            features['realized_vol_20'] = returns.rolling(20).apply(
                lambda x: np.sqrt(np.sum(x ** 2)), raw=False
            )
        
        return features.dropna()
    
    def _create_lag_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """Tạo lag features"""
        
        lagged_features = features.copy()
        
        for lag in self.feature_config['lag_features']:
            for column in features.columns:
                if column not in ['hour', 'dayofweek', 'month', 'quarter', 'days_since_start']:
                    lagged_features[f'{column}_lag_{lag}'] = features[column].shift(lag)
        
        return lagged_features.dropna()
    
    def _create_rolling_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """Tạo rolling window features"""
        
        rolling_features = features.copy()
        
        for window in self.feature_config['rolling_windows']:
            for column in features.columns:
                if column not in ['hour', 'dayofweek', 'month', 'quarter', 'days_since_start']:
                    # Rolling statistics
                    rolling_features[f'{column}_rolling_mean_{window}'] = features[column].rolling(window).mean()
                    rolling_features[f'{column}_rolling_std_{window}'] = features[column].rolling(window).std()
                    rolling_features[f'{column}_rolling_min_{window}'] = features[column].rolling(window).min()
                    rolling_features[f'{column}_rolling_max_{window}'] = features[column].rolling(window).max()
                    
                    # Rolling differences
                    rolling_features[f'{column}_rolling_diff_{window}'] = (
                        features[column] - features[column].rolling(window).mean()
                    )
        
        return rolling_features.dropna()
    
    def _create_target_variable(self, 
                               data: pd.DataFrame,
                               target_column: str = 'target') -> pd.Series:
        """Tạo target variable cho prediction"""
        
        if 'close' in data.columns:
            # Default: predict if price will go up in next N periods
            horizon = self.feature_config['target_horizon']
            
            # Future returns
            future_returns = data['close'].pct_change(horizon).shift(-horizon)
            
            # Binary classification: 1 if return > threshold, else 0
            threshold = 0.001  # 0.1% threshold
            target = (future_returns > threshold).astype(int)
            
            # For regression: use actual returns
            # target = future_returns
            
            target.name = target_column
            return target.dropna()
        else:
            raise ValueError("'close' column required for target creation")
    
    def _align_features_target(self, 
                              features: pd.DataFrame,
                              target: pd.Series) -> Dict[str, pd.DataFrame]:
        """Align features and target trên cùng index"""
        
        # Find common indices
        common_idx = features.index.intersection(target.index)
        
        if len(common_idx) == 0:
            raise ValueError("No common indices between features and target")
        
        # Align data
        aligned_features = features.loc[common_idx]
        aligned_target = target.loc[common_idx]
        
        # Remove any remaining NaN values
        aligned_features = aligned_features.dropna()
        aligned_target = aligned_target.loc[aligned_features.index]
        
        return {
            'features': aligned_features,
            'target': aligned_target
        }
    
    def train_test_split(self, 
                        features: pd.DataFrame,
                        target: pd.Series,
                        test_size: float = 0.2) -> Dict:
        """Split data thành train/test sets với time series awareness"""
        
        # Time-series split (avoid lookahead bias)
        n_samples = len(features)
        test_samples = int(n_samples * test_size)
        
        # Training data (earlier)
        X_train = features.iloc[:-test_samples]
        y_train = target.iloc[:-test_samples]
        
        # Testing data (later)
        X_test = features.iloc[-test_samples:]
        y_test = target.iloc[-test_samples:]
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Store data
        self.X_train = X_train_scaled
        self.y_train = y_train.values
        self.X_test = X_test_scaled
        self.y_test = y_test.values
        
        return {
            'X_train': X_train_scaled,
            'y_train': y_train.values,
            'X_test': X_test_scaled,
            'y_test': y_test.values,
            'train_dates': X_train.index,
            'test_dates': X_test.index
        }
    
    def train_models(self, 
                    X_train: Optional[np.ndarray] = None,
                    y_train: Optional[np.ndarray] = None):
        """Train multiple ML models"""
        
        if X_train is None or y_train is None:
            if self.X_train is None or self.y_train is None:
                raise ValueError("No training data provided")
            X_train = self.X_train
            y_train = self.y_train
        
        print(f"Training models on {len(X_train)} samples...")
        
        # 1. Train base models
        self._train_base_models(X_train, y_train)
        
        # 2. Train ensemble model
        if self.model_config['ensemble_method'] != 'none':
            self._train_ensemble_model(X_train, y_train)
        
        # 3. Train deep learning model (if enabled)
        if self.model_config['use_deep_learning'] and HAS_TORCH:
            self._train_deep_learning_model(X_train, y_train)
        
        # 4. Evaluate and select best model
        self._evaluate_models()
        
        print("Model training completed")
    
    def _train_base_models(self, X_train: np.ndarray, y_train: np.ndarray):
        """Train base models"""
        
        base_models_config = self.model_config['base_models']
        
        # Random Forest
        if base_models_config['random_forest']['enabled']:
            print("Training Random Forest...")
            rf_model = RandomForestClassifier(
                n_estimators=base_models_config['random_forest']['n_estimators'],
                max_depth=base_models_config['random_forest']['max_depth'],
                random_state=base_models_config['random_forest']['random_state'],
                n_jobs=-1
            )
            rf_model.fit(X_train, y_train)
            self.models['random_forest'] = rf_model
        
        # XGBoost
        if base_models_config['xgboost']['enabled']:
            print("Training XGBoost...")
            xgb_model = xgb.XGBClassifier(
                n_estimators=base_models_config['xgboost']['n_estimators'],
                max_depth=base_models_config['xgboost']['max_depth'],
                learning_rate=base_models_config['xgboost']['learning_rate'],
                use_label_encoder=False,
                eval_metric='logloss'
            )
            xgb_model.fit(X_train, y_train)
            self.models['xgboost'] = xgb_model
        
        # LightGBM
        if base_models_config['lightgbm']['enabled']:
            print("Training LightGBM...")
            lgb_model = lgb.LGBMClassifier(
                n_estimators=base_models_config['lightgbm']['n_estimators'],
                max_depth=base_models_config['lightgbm']['max_depth'],
                learning_rate=base_models_config['lightgbm']['learning_rate'],
                random_state=42
            )
            lgb_model.fit(X_train, y_train)
            self.models['lightgbm'] = lgb_model
        
        # Gradient Boosting
        if base_models_config['gradient_boosting']['enabled']:
            print("Training Gradient Boosting...")
            gb_model = GradientBoostingClassifier(
                n_estimators=base_models_config['gradient_boosting']['n_estimators'],
                max_depth=base_models_config['gradient_boosting']['max_depth'],
                learning_rate=base_models_config['gradient_boosting']['learning_rate'],
                random_state=42
            )
            gb_model.fit(X_train, y_train)
            self.models['gradient_boosting'] = gb_model
        
        # Neural Network
        if base_models_config['neural_network']['enabled'] and HAS_TF:
            print("Training Neural Network...")
            nn_model = self._create_neural_network(
                input_dim=X_train.shape[1],
                hidden_layers=base_models_config['neural_network']['hidden_layers'],
                dropout_rate=base_models_config['neural_network']['dropout_rate']
            )
            
            # Compile model
            nn_model.compile(
                optimizer='adam',
                loss='binary_crossentropy',
                metrics=['accuracy']
            )
            
            # Train with early stopping
            early_stopping = callbacks.EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True
            )
            
            # Train
            nn_model.fit(
                X_train, y_train,
                epochs=base_models_config['neural_network']['epochs'],
                batch_size=base_models_config['neural_network']['batch_size'],
                validation_split=0.2,
                callbacks=[early_stopping],
                verbose=0
            )
            
            self.models['neural_network'] = nn_model
    
    def _create_neural_network(self, 
                              input_dim: int,
                              hidden_layers: List[int],
                              dropout_rate: float) -> keras.Model:
        """Create neural network model"""
        
        model = keras.Sequential()
        
        # Input layer
        model.add(layers.Input(shape=(input_dim,)))
        
        # Hidden layers
        for units in hidden_layers:
            model.add(layers.Dense(units, activation='relu'))
            model.add(layers.Dropout(dropout_rate))
        
        # Output layer
        model.add(layers.Dense(1, activation='sigmoid'))
        
        return model
    
    def _train_ensemble_model(self, X_train: np.ndarray, y_train: np.ndarray):
        """Train ensemble model"""
        
        if not self.models:
            print("No base models trained for ensemble")
            return
        
        ensemble_method = self.model_config['ensemble_method']
        
        if ensemble_method == 'voting':
            self._train_voting_ensemble(X_train, y_train)
        elif ensemble_method == 'stacking':
            self._train_stacking_ensemble(X_train, y_train)
        elif ensemble_method == 'blending':
            self._train_blending_ensemble(X_train, y_train)
    
    def _train_voting_ensemble(self, X_train: np.ndarray, y_train: np.ndarray):
        """Train voting ensemble"""
        
        voting_type = self.model_config.get('voting_type', 'soft')
        
        # Create voting classifier
        estimators = [
            (name, model) for name, model in self.models.items()
            if name != 'ensemble'
        ]
        
        if estimators:
            voting_clf = VotingClassifier(
                estimators=estimators,
                voting=voting_type,
                n_jobs=-1
            )
            
            voting_clf.fit(X_train, y_train)
            self.ensemble_model = voting_clf
            self.models['ensemble'] = voting_clf
    
    def _train_stacking_ensemble(self, X_train: np.ndarray, y_train: np.ndarray):
        """Train stacking ensemble"""
        
        # Create base estimators
        base_estimators = [
            (name, model) for name, model in self.models.items()
            if name != 'ensemble'
        ]
        
        if len(base_estimators) >= 2:
            # Create stacking classifier
            stacking_clf = StackingClassifier(
                estimators=base_estimators,
                final_estimator=RandomForestClassifier(n_estimators=50),
                cv=5,
                n_jobs=-1
            )
            
            stacking_clf.fit(X_train, y_train)
            self.ensemble_model = stacking_clf
            self.models['ensemble'] = stacking_clf
    
    def _train_blending_ensemble(self, X_train: np.ndarray, y_train: np.ndarray):
        """Train blending ensemble"""
        
        # Create hold-out set for blending
        from sklearn.model_selection import train_test_split
        
        X_train_base, X_train_meta, y_train_base, y_train_meta = train_test_split(
            X_train, y_train, test_size=0.3, random_state=42
        )
        
        # Train base models on base set
        base_predictions = []
        for name, model in self.models.items():
            if name != 'ensemble':
                model.fit(X_train_base, y_train_base)
                preds = model.predict_proba(X_train_meta)[:, 1]
                base_predictions.append(preds)
        
        # Stack predictions
        X_meta = np.column_stack(base_predictions)
        
        # Train meta-model
        meta_model = RandomForestClassifier(n_estimators=100, random_state=42)
        meta_model.fit(X_meta, y_train_meta)
        
        self.ensemble_model = {
            'base_models': self.models.copy(),
            'meta_model': meta_model,
            'type': 'blending'
        }
        self.models['ensemble'] = self.ensemble_model
    
    def _train_deep_learning_model(self, X_train: np.ndarray, y_train: np.ndarray):
        """Train deep learning model (LSTM/CNN/Transformer)"""
        
        if not HAS_TORCH:
            print("PyTorch not available for deep learning")
            return
        
        dl_config = self.model_config['deep_learning_config']
        model_type = dl_config['model_type']
        
        print(f"Training {model_type.upper()} model...")
        
        # Reshape data for sequence models
        sequence_length = self.feature_config['feature_window']
        n_features = X_train.shape[1]
        
        # Create sequences
        X_sequences, y_sequences = self._create_sequences(
            X_train, y_train, sequence_length
        )
        
        # Create model
        if model_type == 'lstm':
            model = LSTMModel(
                input_size=n_features,
                hidden_size=dl_config['hidden_size'],
                num_layers=dl_config['num_layers'],
                dropout=dl_config['dropout']
            )
        elif model_type == 'cnn':
            model = CNNModel(
                input_size=n_features,
                sequence_length=sequence_length
            )
        elif model_type == 'transformer':
            model = TransformerModel(
                input_size=n_features,
                hidden_size=dl_config['hidden_size'],
                num_layers=dl_config['num_layers']
            )
        else:
            print(f"Unknown model type: {model_type}")
            return
        
        # Move to GPU if available
        if self.use_gpu and torch.cuda.is_available():
            model = model.cuda()
        
        # Train model
        trained_model = self._train_pytorch_model(
            model, X_sequences, y_sequences,
            learning_rate=dl_config['learning_rate'],
            epochs=dl_config['epochs'],
            batch_size=dl_config['batch_size']
        )
        
        self.models['deep_learning'] = trained_model
    
    def _create_sequences(self, 
                         X: np.ndarray, 
                         y: np.ndarray, 
                         sequence_length: int) -> Tuple[np.ndarray, np.ndarray]:
        """Create sequences for time series models"""
        
        X_sequences, y_sequences = [], []
        
        for i in range(len(X) - sequence_length):
            X_sequences.append(X[i:i+sequence_length])
            y_sequences.append(y[i+sequence_length-1])
        
        return np.array(X_sequences), np.array(y_sequences)
    
    def _train_pytorch_model(self, 
                            model: nn.Module,
                            X_train: np.ndarray,
                            y_train: np.ndarray,
                            learning_rate: float = 0.001,
                            epochs: int = 100,
                            batch_size: int = 32) -> nn.Module:
        """Train PyTorch model"""
        
        # Convert to tensors
        X_tensor = torch.FloatTensor(X_train)
        y_tensor = torch.FloatTensor(y_train).unsqueeze(1)
        
        # Create dataset and dataloader
        dataset = TensorDataset(X_tensor, y_tensor)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # Define loss and optimizer
        criterion = nn.BCELoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        
        # Training loop
        model.train()
        for epoch in range(epochs):
            epoch_loss = 0.0
            
            for batch_X, batch_y in dataloader:
                # Move to GPU if available
                if self.use_gpu and torch.cuda.is_available():
                    batch_X = batch_X.cuda()
                    batch_y = batch_y.cuda()
                
                # Forward pass
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(dataloader):.4f}")
        
        return model
    
    def _evaluate_models(self):
        """Evaluate và select best model"""
        
        if self.X_test is None or self.y_test is None:
            print("No test data for evaluation")
            return
        
        print("\nEvaluating models...")
        
        best_score = 0
        best_model_name = None
        
        for name, model in self.models.items():
            # Get predictions
            if name == 'deep_learning':
                # Special handling for deep learning models
                predictions = self._predict_deep_learning(model, self.X_test)
            elif name == 'ensemble' and isinstance(model, dict) and model['type'] == 'blending':
                # Special handling for blending ensemble
                predictions = self._predict_blending_ensemble(model, self.X_test)
            else:
                # Standard models
                try:
                    if hasattr(model, 'predict_proba'):
                        predictions = model.predict_proba(self.X_test)[:, 1]
                    else:
                        predictions = model.predict(self.X_test)
                except:
                    continue
            
            # Calculate metrics
            if len(np.unique(predictions)) > 2:
                # Continuous predictions, convert to binary
                binary_predictions = (predictions > 0.5).astype(int)
            else:
                binary_predictions = predictions
            
            # Calculate metrics
            accuracy = accuracy_score(self.y_test, binary_predictions)
            precision = precision_score(self.y_test, binary_predictions, zero_division=0)
            recall = recall_score(self.y_test, binary_predictions, zero_division=0)
            f1 = f1_score(self.y_test, binary_predictions, zero_division=0)
            
            # ROC AUC (if probabilities available)
            if hasattr(model, 'predict_proba') and name != 'deep_learning':
                prob_predictions = model.predict_proba(self.X_test)[:, 1]
                roc_auc = roc_auc_score(self.y_test, prob_predictions)
            else:
                roc_auc = 0.0
            
            # Store performance
            self.model_performance[name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'roc_auc': roc_auc
            }
            
            print(f"\n{name.upper()}:")
            print(f"  Accuracy:  {accuracy:.4f}")
            print(f"  Precision: {precision:.4f}")
            print(f"  Recall:    {recall:.4f}")
            print(f"  F1-Score:  {f1:.4f}")
            print(f"  ROC AUC:   {roc_auc:.4f}")
            
            # Update best model
            score = f1 * 0.4 + accuracy * 0.3 + roc_auc * 0.3
            if score > best_score:
                best_score = score
                best_model_name = name
        
        # Set best model
        if best_model_name:
            self.best_model = self.models[best_model_name]
            print(f"\nBest model: {best_model_name} (Score: {best_score:.4f})")
    
    def _predict_deep_learning(self, 
                              model: nn.Module,
                              X_test: np.ndarray) -> np.ndarray:
        """Predict with deep learning model"""
        
        # Reshape for sequence model
        sequence_length = self.feature_config['feature_window']
        X_sequences, _ = self._create_sequences(X_test, np.zeros(len(X_test)), sequence_length)
        
        # Convert to tensor
        X_tensor = torch.FloatTensor(X_sequences)
        
        if self.use_gpu and torch.cuda.is_available():
            X_tensor = X_tensor.cuda()
            model = model.cuda()
        
        # Predict
        model.eval()
        with torch.no_grad():
            predictions = model(X_tensor)
            predictions = predictions.cpu().numpy().flatten()
        
        # Pad predictions to match original length
        padded_predictions = np.zeros(len(X_test))
        padded_predictions[-len(predictions):] = predictions
        
        return padded_predictions
    
    def _predict_blending_ensemble(self, 
                                  ensemble: Dict,
                                  X_test: np.ndarray) -> np.ndarray:
        """Predict with blending ensemble"""
        
        # Get predictions from base models
        base_predictions = []
        for name, model in ensemble['base_models'].items():
            if name != 'ensemble' and hasattr(model, 'predict_proba'):
                preds = model.predict_proba(X_test)[:, 1]
                base_predictions.append(preds)
        
        # Stack predictions
        X_meta = np.column_stack(base_predictions)
        
        # Get meta-model predictions
        predictions = ensemble['meta_model'].predict_proba(X_meta)[:, 1]
        
        return predictions
    
    def predict(self, 
               features: pd.DataFrame,
               model_name: Optional[str] = None) -> Dict:
        """Dự đoán với trained model"""
        
        if not self.models:
            raise ValueError("No models trained")
        
        # Check cache
        cache_key = f"prediction_{features.shape}_{model_name}"
        if cache_key in self.prediction_cache:
            cached_pred, cached_time = self.prediction_cache[cache_key]
            if datetime.now() - cached_time < self.cache_ttl:
                return cached_pred
        
        # Scale features
        features_scaled = self.scaler.transform(features)
        
        # Select model
        if model_name is None:
            model = self.best_model
            model_name = 'best_model'
        elif model_name in self.models:
            model = self.models[model_name]
        else:
            raise ValueError(f"Model {model_name} not found")
        
        # Make predictions
        predictions = self._make_predictions(model, model_name, features_scaled)
        
        # Calculate confidence scores
        confidence_scores = self._calculate_confidence(predictions)
        
        # Create result
        result = {
            'predictions': predictions,
            'confidence': confidence_scores,
            'model_used': model_name,
            'timestamp': datetime.now(),
            'feature_importance': self._get_feature_importance(model, model_name)
        }
        
        # Update cache
        self.prediction_cache[cache_key] = (result, datetime.now())
        
        return result
    
    def _make_predictions(self, 
                         model: Any,
                         model_name: str,
                         features: np.ndarray) -> np.ndarray:
        """Make predictions với model cụ thể"""
        
        if model_name == 'deep_learning':
            return self._predict_deep_learning(model, features)
        elif model_name == 'ensemble' and isinstance(model, dict) and model['type'] == 'blending':
            return self._predict_blending_ensemble(model, features)
        else:
            # Standard model
            if hasattr(model, 'predict_proba'):
                return model.predict_proba(features)[:, 1]
            else:
                return model.predict(features)
    
    def _calculate_confidence(self, predictions: np.ndarray) -> np.ndarray:
        """Tính confidence scores từ predictions"""
        
        # For binary classification, confidence is distance from 0.5
        confidence = 2 * np.abs(predictions - 0.5)
        
        # Apply smoothing
        confidence = np.clip(confidence, 0, 1)
        
        return confidence
    
    def _get_feature_importance(self, 
                               model: Any,
                               model_name: str) -> Dict:
        """Lấy feature importance từ model"""
        
        if model_name in ['random_forest', 'xgboost', 'lightgbm', 'gradient_boosting']:
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
                
                # Create importance dictionary
                importance_dict = {}
                for idx, imp in enumerate(importances):
                    if idx < len(self.feature_columns):
                        feature_name = self.feature_columns[idx]
                        importance_dict[feature_name] = float(imp)
                
                # Sort by importance
                sorted_importance = dict(
                    sorted(importance_dict.items(), 
                          key=lambda x: x[1], 
                          reverse=True)[:20]  # Top 20 features
                )
                
                return sorted_importance
        
        return {}
    
    def cross_validate(self, 
                      features: pd.DataFrame,
                      target: pd.Series,
                      n_splits: int = 5) -> Dict:
        """Cross-validation cho models"""
        
        print(f"Performing {n_splits}-fold cross-validation...")
        
        # Time series cross-validation
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        cv_results = {}
        
        for name, model in self.models.items():
            if name == 'deep_learning' or (name == 'ensemble' and isinstance(model, dict)):
                # Skip complex models for CV
                continue
            
            # Perform cross-validation
            cv_scores = cross_val_score(
                model, features, target,
                cv=tscv,
                scoring='f1',
                n_jobs=-1
            )
            
            cv_results[name] = {
                'scores': cv_scores.tolist(),
                'mean_score': cv_scores.mean(),
                'std_score': cv_scores.std()
            }
            
            print(f"{name}: Mean F1 = {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
        
        self.cv_scores = cv_results
        return cv_results
    
    def optimize_hyperparameters(self, 
                                features: pd.DataFrame,
                                target: pd.Series,
                                model_name: str = 'xgboost') -> Dict:
        """Optimize hyperparameters cho model"""
        
        if not self.model_config['hyperparameter_tuning']['enabled']:
            print("Hyperparameter tuning disabled")
            return {}
        
        print(f"Optimizing hyperparameters for {model_name}...")
        
        # Define parameter grid
        param_grid = self._get_parameter_grid(model_name)
        
        # Perform search
        tuning_method = self.model_config['hyperparameter_tuning']['method']
        
        if tuning_method == 'random_search':
            best_params = self._random_search(
                features, target, model_name, param_grid
            )
        elif tuning_method == 'grid_search':
            best_params = self._grid_search(
                features, target, model_name, param_grid
            )
        elif tuning_method == 'bayesian':
            best_params = self._bayesian_optimization(
                features, target, model_name, param_grid
            )
        else:
            print(f"Unknown tuning method: {tuning_method}")
            return {}
        
        self.best_params[model_name] = best_params
        
        print(f"Best parameters for {model_name}: {best_params}")
        
        return best_params
    
    def _get_parameter_grid(self, model_name: str) -> Dict:
        """Lấy parameter grid cho model"""
        
        param_grids = {
            'random_forest': {
                'n_estimators': [50, 100, 200],
                'max_depth': [5, 10, 15, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            },
            'xgboost': {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 6, 9],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 0.9, 1.0]
            },
            'lightgbm': {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 7, 10],
                'learning_rate': [0.01, 0.1, 0.2],
                'num_leaves': [31, 63, 127]
            }
        }
        
        return param_grids.get(model_name, {})
    
    def _random_search(self, 
                      features: pd.DataFrame,
                      target: pd.Series,
                      model_name: str,
                      param_grid: Dict) -> Dict:
        """Random search hyperparameter optimization"""
        
        from sklearn.model_selection import RandomizedSearchCV
        
        # Get base model
        if model_name == 'random_forest':
            base_model = RandomForestClassifier(random_state=42)
        elif model_name == 'xgboost':
            base_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')
        elif model_name == 'lightgbm':
            base_model = lgb.LGBMClassifier(random_state=42)
        else:
            return {}
        
        # Perform random search
        random_search = RandomizedSearchCV(
            estimator=base_model,
            param_distributions=param_grid,
            n_iter=self.model_config['hyperparameter_tuning']['n_iter'],
            cv=TimeSeriesSplit(n_splits=3),
            scoring='f1',
            n_jobs=-1,
            random_state=42
        )
        
        random_search.fit(features, target)
        
        return random_search.best_params_
    
    def _grid_search(self, 
                    features: pd.DataFrame,
                    target: pd.Series,
                    model_name: str,
                    param_grid: Dict) -> Dict:
        """Grid search hyperparameter optimization"""
        
        from sklearn.model_selection import GridSearchCV
        
        # Get base model
        if model_name == 'random_forest':
            base_model = RandomForestClassifier(random_state=42)
        elif model_name == 'xgboost':
            base_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')
        elif model_name == 'lightgbm':
            base_model = lgb.LGBMClassifier(random_state=42)
        else:
            return {}
        
        # Perform grid search
        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=param_grid,
            cv=TimeSeriesSplit(n_splits=3),
            scoring='f1',
            n_jobs=-1
        )
        
        grid_search.fit(features, target)
        
        return grid_search.best_params_
    
    def _bayesian_optimization(self, 
                              features: pd.DataFrame,
                              target: pd.Series,
                              model_name: str,
                              param_grid: Dict) -> Dict:
        """Bayesian optimization hyperparameter tuning"""
        
        try:
            from skopt import BayesSearchCV
            from skopt.space import Real, Integer, Categorical
            
            # Convert param grid to skopt space
            param_space = {}
            for param, values in param_grid.items():
                if all(isinstance(v, int) for v in values):
                    param_space[param] = Integer(min(values), max(values))
                elif all(isinstance(v, float) for v in values):
                    param_space[param] = Real(min(values), max(values), prior='uniform')
                else:
                    param_space[param] = Categorical(values)
            
            # Get base model
            if model_name == 'random_forest':
                base_model = RandomForestClassifier(random_state=42)
            elif model_name == 'xgboost':
                base_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')
            elif model_name == 'lightgbm':
                base_model = lgb.LGBMClassifier(random_state=42)
            else:
                return {}
            
            # Perform Bayesian optimization
            bayes_search = BayesSearchCV(
                estimator=base_model,
                search_spaces=param_space,
                n_iter=self.model_config['hyperparameter_tuning']['n_iter'],
                cv=TimeSeriesSplit(n_splits=3),
                scoring='f1',
                n_jobs=-1,
                random_state=42
            )
            
            bayes_search.fit(features, target)
            
            return bayes_search.best_params_
        
        except ImportError:
            print("scikit-optimize not installed, falling back to random search")
            return self._random_search(features, target, model_name, param_grid)
    
    def save_model(self, 
                  filepath: str,
                  model_name: str = 'best_model'):
        """Save model to file"""
        
        if model_name == 'best_model':
            model_to_save = self.best_model
        elif model_name in self.models:
            model_to_save = self.models[model_name]
        else:
            raise ValueError(f"Model {model_name} not found")
        
        # Save model
        joblib.dump({
            'model': model_to_save,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'feature_config': self.feature_config,
            'model_config': self.model_config,
            'performance': self.model_performance.get(model_name, {})
        }, filepath)
        
        print(f"Model saved to {filepath}")
    
    def load_model(self, filepath: str):
        """Load model từ file"""
        
        loaded_data = joblib.load(filepath)
        
        model_name = 'loaded_model'
        self.models[model_name] = loaded_data['model']
        self.scaler = loaded_data['scaler']
        self.feature_columns = loaded_data['feature_columns']
        self.feature_config = loaded_data['feature_config']
        self.model_config = loaded_data['model_config']
        
        if 'performance' in loaded_data:
            self.model_performance[model_name] = loaded_data['performance']
        
        self.best_model = self.models[model_name]
        
        print(f"Model loaded from {filepath}")
    
    def get_model_summary(self) -> Dict:
        """Lấy model summary"""
        
        return {
            'models_trained': list(self.models.keys()),
            'best_model': 'best_model' if self.best_model else None,
            'feature_count': len(self.feature_columns) if self.feature_columns else 0,
            'performance': self.model_performance,
            'cv_scores': self.cv_scores,
            'best_params': self.best_params,
            'training_history': self.training_history[-5:] if self.training_history else []
        }


# Deep Learning Model Definitions
class LSTMModel(nn.Module):
    """LSTM model cho time series prediction"""
    
    def __init__(self, 
                 input_size: int,
                 hidden_size: int = 64,
                 num_layers: int = 2,
                 dropout: float = 0.3):
        super(LSTMModel, self).__init__()
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = self.dropout(lstm_out[:, -1, :])  # Take last time step
        out = self.fc(lstm_out)
        out = self.sigmoid(out)
        return out


class CNNModel(nn.Module):
    """CNN model cho time series prediction"""
    
    def __init__(self, 
                 input_size: int,
                 sequence_length: int,
                 num_filters: int = 64):
        super(CNNModel, self).__init__()
        
        self.conv1 = nn.Conv1d(
            in_channels=input_size,
            out_channels=num_filters,
            kernel_size=3,
            padding=1
        )
        
        self.conv2 = nn.Conv1d(
            in_channels=num_filters,
            out_channels=num_filters * 2,
            kernel_size=3,
            padding=1
        )
        
        self.pool = nn.MaxPool1d(kernel_size=2)
        
        # Calculate flattened size
        conv_output_size = sequence_length // 2  # After pooling
        conv_output_size = conv_output_size // 2  # After second pooling
        
        self.fc1 = nn.Linear(num_filters * 2 * conv_output_size, 128)
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(128, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        # x shape: (batch, sequence, features)
        x = x.permute(0, 2, 1)  # Change to (batch, features, sequence)
        
        x = torch.relu(self.conv1(x))
        x = self.pool(x)
        x = torch.relu(self.conv2(x))
        x = self.pool(x)
        
        x = x.view(x.size(0), -1)  # Flatten
        
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        
        return x


class TransformerModel(nn.Module):
    """Transformer model cho time series prediction"""
    
    def __init__(self, 
                 input_size: int,
                 hidden_size: int = 64,
                 num_layers: int = 2,
                 num_heads: int = 4):
        super(TransformerModel, self).__init__()
        
        self.embedding = nn.Linear(input_size, hidden_size)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=hidden_size * 4,
            dropout=0.3,
            batch_first=True
        )
        
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )
        
        self.fc = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        # Embed input
        x = self.embedding(x)
        
        # Transformer
        x = self.transformer(x)
        
        # Take last time step
        x = x[:, -1, :]
        
        # Output
        x = self.fc(x)
        x = self.sigmoid(x)
        
        return x
# core/performance_tracker.py
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class PerformanceTracker:
    """Advanced performance tracking và analytics"""
    
    def __init__(self, 
                 initial_capital: float = 10000,
                 benchmark_symbol: str = 'SPY'):
        
        self.initial_capital = initial_capital
        self.current_capital = initial_capital
        self.benchmark_symbol = benchmark_symbol
        
        # Data storage
        self.equity_curve = pd.Series([initial_capital], index=[datetime.now()])
        self.trade_history = []
        self.daily_returns = pd.Series()
        self.monthly_returns = pd.Series()
        
        # Performance metrics
        self.metrics = {
            'total_return': 0.0,
            'annualized_return': 0.0,
            'annualized_volatility': 0.0,
            'sharpe_ratio': 0.0,
            'sortino_ratio': 0.0,
            'calmar_ratio': 0.0,
            'max_drawdown': 0.0,
            'max_drawdown_duration': 0,
            'win_rate': 0.0,
            'profit_factor': 0.0,
            'average_win': 0.0,
            'average_loss': 0.0,
            'largest_win': 0.0,
            'largest_loss': 0.0,
            'avg_trade_duration': timedelta(0),
            'trades_per_day': 0.0,
            'skewness': 0.0,
            'kurtosis': 0.0
        }
        
        # Benchmark data
        self.benchmark_returns = pd.Series()
        self.benchmark_metrics = {}
        
        # Risk metrics
        self.risk_metrics = {
            'var_95': 0.0,
            'cvar_95': 0.0,
            'tail_ratio': 0.0,
            'gain_to_pain_ratio': 0.0,
            'ulcer_index': 0.0,
            'omega_ratio': 0.0
        }
        
        # Analytics cache
        self.analytics_cache = {}
        self.cache_ttl = timedelta(hours=1)
        
    def update_trade(self, trade_data: Dict):
        """Cập nhật trade data"""
        
        self.trade_history.append(trade_data)
        
        # Update capital
        pnl = trade_data.get('pnl', 0)
        self.current_capital += pnl
        
        # Update equity curve
        current_time = datetime.now()
        self.equity_curve[current_time] = self.current_capital
        
        # Update returns
        self._update_returns()
        
        # Update metrics
        self._update_metrics()
        
        # Clear cache
        self.analytics_cache.clear()
    
    def _update_returns(self):
        """Cập nhật returns series"""
        
        if len(self.equity_curve) < 2:
            return
        
        # Calculate daily returns
        equity_df = self.equity_curve.resample('D').last().ffill()
        self.daily_returns = equity_df.pct_change().dropna()
        
        # Calculate monthly returns
        self.monthly_returns = equity_df.resample('M').last().pct_change().dropna()
    
    def _update_metrics(self):
        """Cập nhật performance metrics"""
        
        if len(self.daily_returns) < 5:
            return
        
        # Basic metrics
        total_return = (self.current_capital - self.initial_capital) / self.initial_capital
        days = (datetime.now() - self.equity_curve.index[0]).days
        annualized_return = (1 + total_return) ** (365 / max(days, 1)) - 1
        
        # Volatility
        annualized_volatility = self.daily_returns.std() * np.sqrt(252)
        
        # Sharpe ratio
        risk_free_rate = 0.02
        sharpe_ratio = (annualized_return - risk_free_rate) / max(annualized_volatility, 0.001)
        
        # Sortino ratio
        downside_returns = self.daily_returns[self.daily_returns < 0]
        downside_deviation = downside_returns.std() * np.sqrt(252) if len(downside_returns) > 0 else 0.001
        sortino_ratio = (annualized_return - risk_free_rate) / max(downside_deviation, 0.001)
        
        # Max drawdown
        max_drawdown, drawdown_duration = self._calculate_max_drawdown()
        
        # Calmar ratio
        calmar_ratio = annualized_return / max(abs(max_drawdown), 0.001) if max_drawdown < 0 else 0
        
        # Trade statistics
        if self.trade_history:
            pnls = [t.get('pnl', 0) for t in self.trade_history]
            winning_trades = [p for p in pnls if p > 0]
            losing_trades = [p for p in pnls if p < 0]
            
            win_rate = len(winning_trades) / len(pnls) if pnls else 0
            profit_factor = sum(winning_trades) / abs(sum(losing_trades)) if losing_trades and sum(losing_trades) < 0 else 0
            
            avg_win = np.mean(winning_trades) if winning_trades else 0
            avg_loss = np.mean(losing_trades) if losing_trades else 0
            largest_win = max(winning_trades) if winning_trades else 0
            largest_loss = min(losing_trades) if losing_trades else 0
            
            # Trade duration
            durations = []
            for trade in self.trade_history:
                entry_time = trade.get('entry_time')
                exit_time = trade.get('exit_time')
                if entry_time and exit_time:
                    durations.append(exit_time - entry_time)
            
            avg_duration = np.mean(durations) if durations else timedelta(0)
            
            # Trades per day
            trading_days = len(self.daily_returns)
            trades_per_day = len(self.trade_history) / max(trading_days, 1)
            
            # Distribution statistics
            skewness = stats.skew(pnls) if len(pnls) > 2 else 0
            kurtosis = stats.kurtosis(pnls) if len(pnls) > 3 else 0
        else:
            win_rate = profit_factor = avg_win = avg_loss = largest_win = largest_loss = 0
            avg_duration = timedelta(0)
            trades_per_day = skewness = kurtosis = 0
        
        # Update metrics
        self.metrics.update({
            'total_return': total_return,
            'annualized_return': annualized_return,
            'annualized_volatility': annualized_volatility,
            'sharpe_ratio': sharpe_ratio,
            'sortino_ratio': sortino_ratio,
            'calmar_ratio': calmar_ratio,
            'max_drawdown': max_drawdown,
            'max_drawdown_duration': drawdown_duration,
            'win_rate': win_rate,
            'profit_factor': profit_factor,
            'average_win': avg_win,
            'average_loss': avg_loss,
            'largest_win': largest_win,
            'largest_loss': largest_loss,
            'avg_trade_duration': avg_duration,
            'trades_per_day': trades_per_day,
            'skewness': skewness,
            'kurtosis': kurtosis
        })
        
        # Update risk metrics
        self._update_risk_metrics()
    
    def _calculate_max_drawdown(self) -> Tuple[float, int]:
        """Tính maximum drawdown và duration"""
        
        if len(self.equity_curve) < 2:
            return 0.0, 0
        
        # Calculate running maximum
        equity_series = self.equity_curve.sort_index()
        running_max = equity_series.expanding().max()
        
        # Calculate drawdown
        drawdown = (running_max - equity_series) / running_max
        
        # Find maximum drawdown
        max_dd = drawdown.max()
        max_dd_idx = drawdown.idxmax()
        
        # Calculate drawdown duration
        if max_dd > 0:
            # Find recovery time
            recovery_mask = equity_series.loc[max_dd_idx:] >= running_max.loc[max_dd_idx]
            if recovery_mask.any():
                recovery_idx = recovery_mask[recovery_mask].index[0]
                duration = (recovery_idx - max_dd_idx).days
            else:
                duration = (datetime.now() - max_dd_idx).days
        else:
            duration = 0
        
        return float(max_dd), duration
    
    def _update_risk_metrics(self):
        """Cập nhật risk metrics"""
        
        if len(self.daily_returns) < 20:
            return
        
        # Value at Risk (95%)
        var_95 = np.percentile(self.daily_returns, 5)
        
        # Conditional VaR (Expected Shortfall)
        cvar_95 = self.daily_returns[self.daily_returns <= var_95].mean()
        
        # Tail ratio
        tail_ratio = abs(np.percentile(self.daily_returns, 95) / np.percentile(self.daily_returns, 5))
        
        # Gain to Pain ratio
        total_gain = self.daily_returns[self.daily_returns > 0].sum()
        total_pain = abs(self.daily_returns[self.daily_returns < 0].sum())
        gain_to_pain = total_gain / max(total_pain, 0.001)
        
        # Ulcer Index
        equity_series = self.equity_curve.resample('D').last().ffill()
        running_max = equity_series.expanding().max()
        drawdown = (running_max - equity_series) / running_max
        ulcer_index = np.sqrt(np.mean(drawdown ** 2))
        
        # Omega ratio
        threshold = 0.0
        gains = self.daily_returns[self.daily_returns > threshold].sum()
        losses = abs(self.daily_returns[self.daily_returns < threshold].sum())
        omega_ratio = gains / max(losses, 0.001)
        
        self.risk_metrics.update({
            'var_95': var_95,
            'cvar_95': cvar_95,
            'tail_ratio': tail_ratio,
            'gain_to_pain_ratio': gain_to_pain,
            'ulcer_index': ulcer_index,
            'omega_ratio': omega_ratio
        })
    
    def get_performance_report(self) -> Dict:
        """Lấy comprehensive performance report"""
        
        return {
            'capital_summary': {
                'initial_capital': self.initial_capital,
                'current_capital': self.current_capital,
                'total_pnl': self.current_capital - self.initial_capital,
                'total_return': self.metrics['total_return']
            },
            'performance_metrics': self.metrics,
            'risk_metrics': self.risk_metrics,
            'trade_statistics': {
                'total_trades': len(self.trade_history),
                'winning_trades': len([t for t in self.trade_history if t.get('pnl', 0) > 0]),
                'losing_trades': len([t for t in self.trade_history if t.get('pnl', 0) < 0]),
                'breakeven_trades': len([t for t in self.trade_history if t.get('pnl', 0) == 0]),
                'total_trading_days': len(self.daily_returns),
                'first_trade': self.trade_history[0].get('entry_time') if self.trade_history else None,
                'last_trade': self.trade_history[-1].get('exit_time') if self.trade_history else None
            },
            'returns_distribution': self._get_returns_distribution(),
            'benchmark_comparison': self._get_benchmark_comparison(),
            'recent_trades': self.trade_history[-10:] if self.trade_history else []
        }
    
    def _get_returns_distribution(self) -> Dict:
        """Lấy returns distribution statistics"""
        
        if len(self.daily_returns) < 10:
            return {}
        
        returns = self.daily_returns
        
        return {
            'mean': float(returns.mean()),
            'median': float(returns.median()),
            'std': float(returns.std()),
            'min': float(returns.min()),
            'max': float(returns.max()),
            'skewness': float(stats.skew(returns.dropna())),
            'kurtosis': float(stats.kurtosis(returns.dropna())),
            'positive_days': int((returns > 0).sum()),
            'negative_days': int((returns < 0).sum()),
            'zero_days': int((returns == 0).sum()),
            'best_day': float(returns.max()),
            'worst_day': float(returns.min()),
            'avg_positive_day': float(returns[returns > 0].mean()),
            'avg_negative_day': float(returns[returns < 0].mean())
        }
    
    def _get_benchmark_comparison(self) -> Dict:
        """So sánh với benchmark"""
        
        # Placeholder for benchmark comparison
        # In practice, would load actual benchmark data
        
        return {
            'benchmark_symbol': self.benchmark_symbol,
            'benchmark_return': 0.10,  # Placeholder
            'outperformance': self.metrics['annualized_return'] - 0.10,
            'correlation': 0.0,
            'beta': 0.0,
            'alpha': 0.0
        }
    
    def plot_equity_curve(self, 
                         save_path: Optional[str] = None,
                         show_drawdown: bool = True):
        """Vẽ equity curve"""
        
        if len(self.equity_curve) < 10:
            print("Insufficient data for plotting")
            return
        
        fig, axes = plt.subplots(2 if show_drawdown else 1, 1, 
                               figsize=(12, 8 if show_drawdown else 6),
                               gridspec_kw={'height_ratios': [3, 1]} if show_drawdown else None)
        
        if show_drawdown:
            ax1, ax2 = axes
        else:
            ax1 = axes
            ax2 = None
        
        # Plot equity curve
        equity_series = self.equity_curve.sort_index()
        ax1.plot(equity_series.index, equity_series.values, 
                linewidth=2, color='blue', label='Equity Curve')
        
        # Plot running maximum
        running_max = equity_series.expanding().max()
        ax1.plot(equity_series.index, running_max.values, 
                linestyle='--', color='green', alpha=0.7, label='Running Max')
        
        ax1.set_title('Equity Curve', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Date')
        ax1.set_ylabel('Capital ($)')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # Plot drawdown
        if show_drawdown and ax2:
            drawdown = (running_max - equity_series) / running_max
            ax2.fill_between(equity_series.index, 0, drawdown.values, 
                           color='red', alpha=0.3)
            ax2.plot(equity_series.index, drawdown.values, 
                   color='red', linewidth=1)
            
            ax2.set_title('Drawdown', fontsize=12)
            ax2.set_xlabel('Date')
            ax2.set_ylabel('Drawdown (%)')
            ax2.grid(True, alpha=0.3)
            ax2.set_ylim(bottom=0)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Equity curve saved to {save_path}")
        
        plt.show()
    
    def plot_returns_distribution(self, 
                                 save_path: Optional[str] = None):
        """Vẽ returns distribution"""
        
        if len(self.daily_returns) < 20:
            print("Insufficient data for distribution plot")
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # Histogram với KDE
        ax1 = axes[0]
        sns.histplot(self.daily_returns, kde=True, ax=ax1, bins=50)
        ax1.axvline(x=0, color='red', linestyle='--', alpha=0.7)
        ax1.axvline(x=self.daily_returns.mean(), color='green', 
                   linestyle='--', alpha=0.7, label=f'Mean: {self.daily_returns.mean():.4f}')
        ax1.set_title('Daily Returns Distribution', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Daily Return')
        ax1.set_ylabel('Frequency')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # QQ plot
        ax2 = axes[1]
        stats.probplot(self.daily_returns.dropna(), dist="norm", plot=ax2)
        ax2.set_title('Q-Q Plot (vs Normal Distribution)', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Returns distribution saved to {save_path}")
        
        plt.show()
    
    def plot_monthly_returns_heatmap(self, 
                                    save_path: Optional[str] = None):
        """Vẽ monthly returns heatmap"""
        
        if len(self.monthly_returns) < 12:
            print("Insufficient data for monthly heatmap")
            return
        
        # Reshape data for heatmap
        monthly_df = self.monthly_returns.copy()
        monthly_df.index = pd.to_datetime(monthly_df.index)
        
        monthly_df['Year'] = monthly_df.index.year
        monthly_df['Month'] = monthly_df.index.month
        
        # Create pivot table
        pivot_table = monthly_df.pivot_table(
            values=monthly_df.columns[0], 
            index='Year', 
            columns='Month', 
            aggfunc='sum'
        )
        
        # Create heatmap
        fig, ax = plt.subplots(figsize=(12, 8))
        
        cmap = plt.cm.RdYlGn  # Red-Yellow-Green colormap
        im = ax.imshow(pivot_table.values, cmap=cmap, aspect='auto')
        
        # Set labels
        ax.set_xticks(np.arange(12))
        ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
        ax.set_yticks(np.arange(len(pivot_table.index)))
        ax.set_yticklabels(pivot_table.index)
        
        # Add text annotations
        for i in range(len(pivot_table.index)):
            for j in range(12):
                if j < len(pivot_table.columns):
                    value = pivot_table.iloc[i, j]
                    if not np.isnan(value):
                        text = ax.text(j, i, f'{value:.1%}', 
                                     ha='center', va='center', 
                                     color='black' if abs(value) < 0.05 else 'white',
                                     fontsize=8)
        
        ax.set_title('Monthly Returns Heatmap (%)', fontsize=14, fontweight='bold')
        plt.colorbar(im, ax=ax)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Monthly heatmap saved to {save_path}")
        
        plt.show()
    
    def generate_performance_analysis(self) -> Dict:
        """Generate comprehensive performance analysis"""
        
        cache_key = 'performance_analysis'
        if cache_key in self.analytics_cache:
            cached_data, cached_time = self.analytics_cache[cache_key]
            if datetime.now() - cached_time < self.cache_ttl:
                return cached_data
        
        analysis = {
            'summary': self.get_performance_report(),
            'periodic_analysis': self._analyze_periodic_performance(),
            'risk_analysis': self._analyze_risk(),
            'trade_analysis': self._analyze_trades(),
            'market_regime_analysis': self._analyze_market_regimes(),
            'recommendations': self._generate_recommendations()
        }
        
        self.analytics_cache[cache_key] = (analysis, datetime.now())
        
        return analysis
    
    def _analyze_periodic_performance(self) -> Dict:
        """Phân tích performance theo period"""
        
        if len(self.daily_returns) < 60:
            return {}
        
        # Daily performance
        daily_stats = {
            'avg_daily_return': float(self.daily_returns.mean()),
            'daily_win_rate': float((self.daily_returns > 0).mean()),
            'best_day': float(self.daily_returns.max()),
            'worst_day': float(self.daily_returns.min())
        }
        
        # Weekly performance
        weekly_returns = self.equity_curve.resample('W').last().pct_change().dropna()
        weekly_stats = {
            'avg_weekly_return': float(weekly_returns.mean()),
            'weekly_win_rate': float((weekly_returns > 0).mean())
        }
        
        # Monthly performance
        monthly_stats = {
            'avg_monthly_return': float(self.monthly_returns.mean()),
            'monthly_win_rate': float((self.monthly_returns > 0).mean()),
            'best_month': float(self.monthly_returns.max()),
            'worst_month': float(self.monthly_returns.min())
        }
        
        # Yearly performance
        yearly_returns = self.equity_curve.resample('Y').last().pct_change().dropna()
        yearly_stats = {
            'avg_yearly_return': float(yearly_returns.mean()),
            'yearly_win_rate': float((yearly_returns > 0).mean())
        }
        
        return {
            'daily': daily_stats,
            'weekly': weekly_stats,
            'monthly': monthly_stats,
            'yearly': yearly_stats
        }
    
    def _analyze_risk(self) -> Dict:
        """Phân tích risk"""
        
        return {
            'value_at_risk': {
                'daily_95': self.risk_metrics['var_95'],
                'daily_99': np.percentile(self.daily_returns, 1) if len(self.daily_returns) > 0 else 0,
                'expected_shortfall_95': self.risk_metrics['cvar_95']
            },
            'drawdown_analysis': {
                'max_drawdown': self.metrics['max_drawdown'],
                'avg_drawdown': self._calculate_average_drawdown(),
                'drawdown_frequency': self._calculate_drawdown_frequency(),
                'time_to_recovery': self._calculate_avg_recovery_time()
            },
            'stress_periods': self._identify_stress_periods()
        }
    
    def _calculate_average_drawdown(self) -> float:
        """Tính average drawdown"""
        
        if len(self.equity_curve) < 10:
            return 0.0
        
        equity_series = self.equity_curve.sort_index()
        running_max = equity_series.expanding().max()
        drawdown = (running_max - equity_series) / running_max
        
        return float(drawdown.mean())
    
    def _calculate_drawdown_frequency(self) -> float:
        """Tính drawdown frequency"""
        
        if len(self.daily_returns) < 20:
            return 0.0
        
        # Count days with drawdown > 1%
        equity_series = self.equity_curve.resample('D').last().ffill()
        running_max = equity_series.expanding().max()
        drawdown = (running_max - equity_series) / running_max
        
        drawdown_days = (drawdown > 0.01).sum()
        total_days = len(drawdown)
        
        return float(drawdown_days / total_days) if total_days > 0 else 0.0
    
    def _calculate_avg_recovery_time(self) -> float:
        """Tính average recovery time từ drawdown"""
        
        # Placeholder implementation
        return 5.0  # days
    
    def _identify_stress_periods(self) -> List[Dict]:
        """Xác định stress periods"""
        
        stress_periods = []
        
        if len(self.daily_returns) < 60:
            return stress_periods
        
        # Identify periods with consecutive losses
        returns_series = self.daily_returns
        
        consecutive_losses = 0
        start_date = None
        
        for date, ret in returns_series.items():
            if ret < 0:
                if consecutive_losses == 0:
                    start_date = date
                consecutive_losses += 1
            else:
                if consecutive_losses >= 3:  # 3+ consecutive losing days
                    stress_periods.append({
                        'start_date': start_date,
                        'end_date': date - timedelta(days=1),
                        'duration_days': consecutive_losses,
                        'total_loss': returns_series.loc[start_date:date-timedelta(days=1)].sum()
                    })
                consecutive_losses = 0
        
        return stress_periods
    
    def _analyze_trades(self) -> Dict:
        """Phân tích trades"""
        
        if not self.trade_history:
            return {}
        
        trades_df = pd.DataFrame(self.trade_history)
        
        # Trade timing analysis
        if 'entry_time' in trades_df.columns and 'exit_time' in trades_df.columns:
            trades_df['entry_hour'] = pd.to_datetime(trades_df['entry_time']).dt.hour
            trades_df['exit_hour'] = pd.to_datetime(trades_df['exit_time']).dt.hour
            trades_df['holding_hours'] = (pd.to_datetime(trades_df['exit_time']) - 
                                         pd.to_datetime(trades_df['entry_time'])).dt.total_seconds() / 3600
            
            timing_stats = {
                'most_profitable_hour': trades_df.groupby('entry_hour')['pnl'].mean().idxmax(),
                'avg_holding_hours': float(trades_df['holding_hours'].mean()),
                'holding_hours_distribution': {
                    'short_term': len(trades_df[trades_df['holding_hours'] < 1]) / len(trades_df),
                    'medium_term': len(trades_df[(trades_df['holding_hours'] >= 1) & 
                                                (trades_df['holding_hours'] < 24)]) / len(trades_df),
                    'long_term': len(trades_df[trades_df['holding_hours'] >= 24]) / len(trades_df)
                }
            }
        else:
            timing_stats = {}
        
        # Strategy performance
        if 'strategy' in trades_df.columns:
            strategy_performance = trades_df.groupby('strategy').agg({
                'pnl': ['count', 'sum', 'mean', 'std'],
                'return_pct': 'mean'
            }).round(4).to_dict()
        else:
            strategy_performance = {}
        
        # Win/Loss streaks
        pnls = trades_df['pnl'].values if 'pnl' in trades_df.columns else []
        win_streak = 0
        loss_streak = 0
        max_win_streak = 0
        max_loss_streak = 0
        
        for pnl in pnls:
            if pnl > 0:
                win_streak += 1
                loss_streak = 0
                max_win_streak = max(max_win_streak, win_streak)
            elif pnl < 0:
                loss_streak += 1
                win_streak = 0
                max_loss_streak = max(max_loss_streak, loss_streak)
        
        return {
            'timing_analysis': timing_stats,
            'strategy_performance': strategy_performance,
            'streaks': {
                'max_win_streak': max_win_streak,
                'max_loss_streak': max_loss_streak
            },
            'correlation_analysis': self._analyze_trade_correlations()
        }
    
    def _analyze_trade_correlations(self) -> Dict:
        """Phân tích correlations giữa trades"""
        
        # Placeholder implementation
        return {}
    
    def _analyze_market_regimes(self) -> Dict:
        """Phân tích performance theo market regime"""
        
        # Placeholder - would use actual market regime data
        return {
            'trending_market': {
                'trades': 0,
                'win_rate': 0.0,
                'avg_return': 0.0
            },
            'ranging_market': {
                'trades': 0,
                'win_rate': 0.0,
                'avg_return': 0.0
            },
            'volatile_market': {
                'trades': 0,
                'win_rate': 0.0,
                'avg_return': 0.0
            }
        }
    
    def _generate_recommendations(self) -> List[Dict]:
        """Generate recommendations từ performance analysis"""
        
        recommendations = []
        
        # Risk-related recommendations
        if self.metrics['max_drawdown'] < -0.15:
            recommendations.append({
                'type': 'RISK_MANAGEMENT',
                'priority': 'HIGH',
                'message': 'Maximum drawdown exceeds 15%. Consider reducing position sizes or implementing tighter stop losses.',
                'suggestion': 'Reduce position sizes by 20% or implement trailing stops.'
            })
        
        if self.metrics['sharpe_ratio'] < 1.0:
            recommendations.append({
                'type': 'PERFORMANCE',
                'priority': 'MEDIUM',
                'message': f'Sharpe ratio ({self.metrics["sharpe_ratio"]:.2f}) is below 1.0. Consider improving risk-adjusted returns.',
                'suggestion': 'Focus on strategies with better risk/reward ratios or reduce portfolio volatility.'
            })
        
        if self.metrics['win_rate'] < 0.4:
            recommendations.append({
                'type': 'TRADING',
                'priority': 'HIGH',
                'message': f'Win rate ({self.metrics["win_rate"]:.1%}) is below 40%. Review entry signals and confirmation criteria.',
                'suggestion': 'Increase confirmation requirements or improve entry timing.'
            })
        
        # Trade frequency recommendations
        if self.metrics['trades_per_day'] > 5:
            recommendations.append({
                'type': 'TRADING_FREQUENCY',
                'priority': 'MEDIUM',
                'message': 'High trading frequency detected. Overtrading may reduce profitability.',
                'suggestion': 'Reduce trading frequency and focus on higher-quality signals.'
            })
        
        # Holding period recommendations
        if isinstance(self.metrics['avg_trade_duration'], timedelta):
            avg_hours = self.metrics['avg_trade_duration'].total_seconds() / 3600
            if avg_hours < 1:
                recommendations.append({
                    'type': 'HOLDING_PERIOD',
                    'priority': 'LOW',
                    'message': 'Very short average holding period. Consider if this aligns with strategy objectives.',
                    'suggestion': 'Evaluate if longer holding periods might improve risk/reward ratios.'
                })
        
        return recommendations
    
    def export_report(self, 
                     format: str = 'json',
                     filepath: Optional[str] = None) -> Optional[str]:
        """Export performance report"""
        
        report = self.generate_performance_analysis()
        
        if format == 'json':
            import json
            output = json.dumps(report, indent=2, default=str)
        elif format == 'csv':
            # Flatten report for CSV
            flat_report = self._flatten_report(report)
            output = flat_report.to_csv()
        else:
            raise ValueError(f"Unsupported format: {format}")
        
        if filepath:
            with open(filepath, 'w') as f:
                f.write(output)
            print(f"Report exported to {filepath}")
            return None
        else:
            return output
    
    def _flatten_report(self, report: Dict) -> pd.DataFrame:
        """Flatten report dictionary cho CSV export"""
        
        # Placeholder implementation
        return pd.DataFrame()
# core/portfolio_manager.py
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any, Set
from datetime import datetime, timedelta
from scipy.optimize import minimize
import warnings
warnings.filterwarnings('ignore')

class AdvancedPortfolioManager:
    """Advanced portfolio management với optimization và risk control"""
    
    def __init__(self, 
                 initial_capital: float = 10000,
                 max_drawdown: float = 0.20,
                 target_return: float = 0.15,
                 risk_free_rate: float = 0.02):
        
        self.initial_capital = initial_capital
        self.current_capital = initial_capital
        self.max_drawdown = max_drawdown
        self.target_return = target_return
        self.risk_free_rate = risk_free_rate
        
        # Portfolio components
        self.positions = {}
        self.pending_orders = []
        
        # Performance metrics
        self.performance_metrics = {
            'total_return': 0.0,
            'annualized_return': 0.0,
            'volatility': 0.0,
            'sharpe_ratio': 0.0,
            'max_drawdown': 0.0,
            'calmar_ratio': 0.0,
            'sortino_ratio': 0.0,
            'win_rate': 0.0,
            'profit_factor': 0.0
        }
        
        # Correlation matrix
        self.correlation_matrix = pd.DataFrame()
        
        # Optimization parameters
        self.optimization_method = 'sharpe'  # sharpe, min_variance, risk_parity
        
        # Rebalancing schedule
        self.rebalancing_frequency = 'weekly'  # daily, weekly, monthly
        self.last_rebalance = datetime.now()
        
        # Asset allocation limits
        self.allocation_limits = {
            'max_single_position': 0.20,  # 20% max
            'max_sector_exposure': 0.30,  # 30% max per sector
            'min_position_size': 0.01,    # 1% min
            'max_leverage': 1.0           # No leverage
        }
        
        # Transaction cost model
        self.transaction_cost = 0.001  # 0.1% per trade
        
        # Historical data
        self.historical_returns = pd.DataFrame()
        self.historical_volatility = {}
        
    def add_position(self, 
                    symbol: str,
                    position_data: Dict,
                    signal_data: Dict) -> bool:
        """Thêm position vào portfolio"""
        
        # Check allocation limits
        if not self._check_allocation_limits(symbol, position_data):
            return False
        
        # Check correlation risk
        if not self._check_correlation_risk(symbol, position_data):
            return False
        
        # Check market exposure
        if not self._check_market_exposure(symbol, position_data):
            return False
        
        # Calculate position metrics
        position_metrics = self._calculate_position_metrics(position_data, signal_data)
        
        # Add to positions
        self.positions[symbol] = {
            **position_data,
            **position_metrics,
            'entry_time': datetime.now(),
            'status': 'OPEN'
        }
        
        # Update portfolio metrics
        self._update_portfolio_metrics()
        
        return True
    
    def _check_allocation_limits(self, 
                               symbol: str,
                               position_data: Dict) -> bool:
        """Kiểm tra allocation limits"""
        
        position_size = position_data.get('position_size_pct', 0)
        position_value = position_data.get('position_value', 0)
        
        # 1. Check single position limit
        if position_size > self.allocation_limits['max_single_position']:
            print(f"Position size {position_size:.2%} exceeds single position limit")
            return False
        
        # 2. Check total exposure
        total_exposure = sum(p.get('position_size_pct', 0) for p in self.positions.values())
        total_exposure += position_size
        
        if total_exposure > self.allocation_limits['max_leverage']:
            print(f"Total exposure {total_exposure:.2%} exceeds leverage limit")
            return False
        
        # 3. Check minimum position size
        if position_size < self.allocation_limits['min_position_size']:
            print(f"Position size {position_size:.2%} below minimum")
            return False
        
        return True
    
    def _check_correlation_risk(self, 
                              symbol: str,
                              position_data: Dict) -> bool:
        """Kiểm tra correlation risk"""
        
        if not self.correlation_matrix.empty and symbol in self.correlation_matrix.columns:
            # Check correlation with existing positions
            high_correlation_count = 0
            
            for existing_symbol in self.positions:
                if existing_symbol in self.correlation_matrix.columns:
                    correlation = self.correlation_matrix.loc[symbol, existing_symbol]
                    if correlation > 0.7:  # High correlation threshold
                        high_correlation_count += 1
            
            # Allow maximum 2 highly correlated positions
            if high_correlation_count >= 2:
                print(f"Too many highly correlated positions with {symbol}")
                return False
        
        return True
    
    def _check_market_exposure(self, 
                             symbol: str,
                             position_data: Dict) -> bool:
        """Kiểm tra market exposure"""
        
        # Placeholder for market/sector exposure checks
        # In practice, would use sector classification
        
        return True
    
    def _calculate_position_metrics(self, 
                                  position_data: Dict,
                                  signal_data: Dict) -> Dict:
        """Tính position metrics"""
        
        entry_price = position_data.get('entry_price', 0)
        stop_loss = position_data.get('stop_loss', 0)
        take_profit = position_data.get('take_profit', 0)
        
        if entry_price <= 0:
            return {}
        
        # Calculate risk metrics
        risk_amount = position_data.get('risk_amount', 0)
        reward_amount = position_data.get('reward_amount', 0)
        
        risk_pct = risk_amount / self.current_capital if self.current_capital > 0 else 0
        reward_pct = reward_amount / self.current_capital if self.current_capital > 0 else 0
        
        # Calculate expected return
        confidence = signal_data.get('confidence', 0.5)
        expected_return = (confidence * reward_pct - (1 - confidence) * risk_pct)
        
        # Calculate position score
        position_score = self._calculate_position_score(position_data, signal_data)
        
        return {
            'risk_pct': risk_pct,
            'reward_pct': reward_pct,
            'expected_return': expected_return,
            'position_score': position_score,
            'signal_confidence': confidence,
            'risk_reward_ratio': position_data.get('risk_reward_ratio', 0)
        }
    
    def _calculate_position_score(self, 
                                position_data: Dict,
                                signal_data: Dict) -> float:
        """Tính position score"""
        
        score = 0.0
        
        # 1. Risk/Reward ratio contribution
        rr_ratio = position_data.get('risk_reward_ratio', 1.0)
        rr_score = min(1.0, rr_ratio / 3.0)  # Normalize to 0-1
        score += rr_score * 0.3
        
        # 2. Signal confidence contribution
        confidence = signal_data.get('confidence', 0.5)
        score += confidence * 0.25
        
        # 3. Portfolio diversification contribution
        diversification_score = self._calculate_diversification_score(position_data)
        score += diversification_score * 0.2
        
        # 4. Market regime alignment
        regime_score = self._calculate_regime_score(signal_data)
        score += regime_score * 0.15
        
        # 5. Liquidity score
        liquidity_score = position_data.get('liquidity_score', 0.7)
        score += liquidity_score * 0.1
        
        return min(1.0, max(0.0, score))
    
    def _calculate_diversification_score(self, position_data: Dict) -> float:
        """Tính diversification score"""
        
        if not self.positions:
            return 1.0  # First position provides maximum diversification
        
        # Calculate correlation with existing positions
        total_correlation = 0.0
        count = 0
        
        for existing_position in self.positions.values():
            # Simplified correlation calculation
            # In practice, would use actual correlation matrix
            correlation = 0.3  # Placeholder
            total_correlation += correlation
            count += 1
        
        avg_correlation = total_correlation / count if count > 0 else 0
        
        # Lower correlation = better diversification
        diversification_score = 1.0 - avg_correlation
        
        return max(0.0, min(1.0, diversification_score))
    
    def _calculate_regime_score(self, signal_data: Dict) -> float:
        """Tính regime alignment score"""
        
        # Placeholder - would use actual market regime data
        return 0.7
    
    def close_position(self, 
                      symbol: str,
                      exit_price: float,
                      exit_reason: str = "MANUAL") -> Dict:
        """Đóng position"""
        
        if symbol not in self.positions:
            return {'success': False, 'error': 'Position not found'}
        
        position = self.positions[symbol]
        entry_price = position.get('entry_price', 0)
        
        if entry_price <= 0:
            return {'success': False, 'error': 'Invalid entry price'}
        
        # Calculate P&L
        units = position.get('units', 0)
        pnl = (exit_price - entry_price) * units if position.get('signal', 'BUY') == 'BUY' \
              else (entry_price - exit_price) * units
        
        # Calculate returns
        position_value = position.get('position_value', 0)
        return_pct = pnl / position_value if position_value > 0 else 0
        
        # Update position
        position.update({
            'exit_price': exit_price,
            'exit_time': datetime.now(),
            'pnl': pnl,
            'return_pct': return_pct,
            'exit_reason': exit_reason,
            'status': 'CLOSED',
            'holding_period': (datetime.now() - position.get('entry_time', datetime.now())).total_seconds() / 3600
        })
        
        # Update capital
        self.current_capital += pnl
        
        # Remove from active positions
        closed_position = self.positions.pop(symbol)
        
        # Update performance metrics
        self._update_portfolio_metrics()
        
        return {
            'success': True,
            'symbol': symbol,
            'pnl': pnl,
            'return_pct': return_pct,
            'position': closed_position
        }
    
    def _update_portfolio_metrics(self):
        """Cập nhật portfolio metrics"""
        
        # Calculate current portfolio value
        portfolio_value = self.current_capital
        open_positions_value = 0
        
        for position in self.positions.values():
            if position.get('status') == 'OPEN':
                current_value = position.get('position_value', 0)
                open_positions_value += current_value
        
        total_value = portfolio_value + open_positions_value
        
        # Calculate unrealized P&L
        unrealized_pnl = 0
        for symbol, position in self.positions.items():
            if position.get('status') == 'OPEN':
                # Simplified - would use current market price
                entry_price = position.get('entry_price', 0)
                current_price = entry_price * 1.01  # Placeholder
                units = position.get('units', 0)
                
                position_pnl = (current_price - entry_price) * units if position.get('signal', 'BUY') == 'BUY' \
                              else (entry_price - current_price) * units
                
                unrealized_pnl += position_pnl
                position['unrealized_pnl'] = position_pnl
                position['current_value'] = position.get('position_value', 0) + position_pnl
        
        # Update performance metrics
        self._calculate_performance_metrics(total_value, unrealized_pnl)
        
        # Check for rebalancing
        self._check_rebalancing()
    
    def _calculate_performance_metrics(self, 
                                     total_value: float,
                                     unrealized_pnl: float):
        """Tính performance metrics"""
        
        # Calculate total return
        total_return = (total_value - self.initial_capital) / self.initial_capital
        
        # Calculate annualized return
        days_since_start = (datetime.now() - self._get_start_date()).days
        if days_since_start > 0:
            annualized_return = (1 + total_return) ** (365 / days_since_start) - 1
        else:
            annualized_return = 0.0
        
        # Calculate volatility (simplified)
        volatility = self._calculate_portfolio_volatility()
        
        # Calculate Sharpe ratio
        excess_return = annualized_return - self.risk_free_rate
        sharpe_ratio = excess_return / max(volatility, 0.001) if volatility > 0 else 0
        
        # Calculate max drawdown
        max_drawdown = self._calculate_max_drawdown()
        
        # Calculate Calmar ratio
        calmar_ratio = annualized_return / max(abs(max_drawdown), 0.001) if max_drawdown < 0 else 0
        
        # Update metrics
        self.performance_metrics.update({
            'total_return': total_return,
            'annualized_return': annualized_return,
            'volatility': volatility,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'calmar_ratio': calmar_ratio,
            'current_value': total_value,
            'unrealized_pnl': unrealized_pnl,
            'open_positions': len([p for p in self.positions.values() if p.get('status') == 'OPEN']),
            'total_positions': len(self.positions)
        })
    
    def _calculate_portfolio_volatility(self) -> float:
        """Tính portfolio volatility"""
        
        if not self.positions:
            return 0.0
        
        # Simplified volatility calculation
        # In practice, would use covariance matrix
        
        position_volatilities = []
        position_weights = []
        
        for position in self.positions.values():
            if position.get('status') == 'OPEN':
                # Use historical volatility or ATR-based estimate
                volatility = position.get('volatility', 0.02)
                weight = position.get('position_size_pct', 0)
                
                position_volatilities.append(volatility)
                position_weights.append(weight)
        
        if not position_volatilities:
            return 0.0
        
        # Weighted average volatility (simplified - ignores correlation)
        weighted_vol = sum(v * w for v, w in zip(position_volatilities, position_weights))
        
        return weighted_vol
    
    def _calculate_max_drawdown(self) -> float:
        """Tính maximum drawdown"""
        
        # Placeholder - would use historical equity curve
        return 0.05  # 5% placeholder
    
    def _get_start_date(self) -> datetime:
        """Lấy start date của portfolio"""
        
        # Find earliest position entry time
        if not self.positions:
            return datetime.now()
        
        entry_times = [p.get('entry_time', datetime.now()) for p in self.positions.values()]
        return min(entry_times)
    
    def _check_rebalancing(self):
        """Kiểm tra xem có cần rebalancing không"""
        
        current_time = datetime.now()
        
        # Check time-based rebalancing
        if self.rebalancing_frequency == 'daily':
            rebalance_interval = timedelta(days=1)
        elif self.rebalancing_frequency == 'weekly':
            rebalance_interval = timedelta(weeks=1)
        elif self.rebalancing_frequency == 'monthly':
            rebalance_interval = timedelta(days=30)
        else:
            return
        
        if current_time - self.last_rebalance >= rebalance_interval:
            self.rebalance_portfolio()
            self.last_rebalance = current_time
        
        # Check deviation-based rebalancing
        if self._check_allocation_deviations():
            self.rebalance_portfolio()
            self.last_rebalance = current_time
    
    def _check_allocation_deviations(self) -> bool:
        """Kiểm tra allocation deviations"""
        
        target_allocations = self._get_target_allocations()
        
        if not target_allocations:
            return False
        
        # Calculate current allocations
        current_allocations = {}
        total_value = self.performance_metrics.get('current_value', self.current_capital)
        
        for symbol, position in self.positions.items():
            if position.get('status') == 'OPEN':
                position_value = position.get('current_value', position.get('position_value', 0))
                allocation = position_value / total_value if total_value > 0 else 0
                current_allocations[symbol] = allocation
        
        # Check deviations
        max_deviation = 0.05  # 5% maximum deviation
        
        for symbol, target_allocation in target_allocations.items():
            current_allocation = current_allocations.get(symbol, 0)
            deviation = abs(current_allocation - target_allocation)
            
            if deviation > max_deviation:
                return True
        
        return False
    
    def _get_target_allocations(self) -> Dict[str, float]:
        """Lấy target allocations"""
        
        # Placeholder - would use optimization or fixed allocation
        if not self.positions:
            return {}
        
        # Equal weighting for now
        n_positions = len([p for p in self.positions.values() if p.get('status') == 'OPEN'])
        if n_positions == 0:
            return {}
        
        equal_weight = 1.0 / n_positions
        
        target_allocations = {}
        for symbol in self.positions:
            if self.positions[symbol].get('status') == 'OPEN':
                target_allocations[symbol] = equal_weight
        
        return target_allocations
    
    def rebalance_portfolio(self):
        """Rebalance portfolio"""
        
        print("Rebalancing portfolio...")
        
        # Get target allocations
        target_allocations = self._get_target_allocations()
        
        if not target_allocations:
            return
        
        # Calculate current allocations và needed adjustments
        total_value = self.performance_metrics.get('current_value', self.current_capital)
        adjustments = []
        
        for symbol, position in self.positions.items():
            if position.get('status') == 'OPEN':
                current_value = position.get('current_value', position.get('position_value', 0))
                current_allocation = current_value / total_value if total_value > 0 else 0
                
                target_allocation = target_allocations.get(symbol, 0)
                deviation = target_allocation - current_allocation
                
                if abs(deviation) > 0.01:  # 1% minimum adjustment threshold
                    adjustment_value = deviation * total_value
                    
                    adjustments.append({
                        'symbol': symbol,
                        'current_allocation': current_allocation,
                        'target_allocation': target_allocation,
                        'deviation': deviation,
                        'adjustment_value': adjustment_value,
                        'action': 'BUY' if adjustment_value > 0 else 'SELL'
                    })
        
        # Execute adjustments
        for adjustment in adjustments:
            if adjustment['action'] == 'BUY':
                # Add to position
                print(f"Adding {adjustment['adjustment_value']:.2f} to {adjustment['symbol']}")
            else:
                # Reduce position
                print(f"Reducing {adjustment['symbol']} by {abs(adjustment['adjustment_value']):.2f}")
        
        print("Rebalancing completed")
    
    def optimize_portfolio(self, 
                         method: str = 'sharpe',
                         constraints: Optional[Dict] = None) -> Dict:
        """Optimize portfolio allocation"""
        
        constraints = constraints or {}
        
        if not self.positions:
            return {'success': False, 'error': 'No positions to optimize'}
        
        # Get historical returns
        returns_data = self._prepare_optimization_data()
        
        if returns_data.empty:
            return {'success': False, 'error': 'Insufficient historical data'}
        
        # Define optimization problem
        if method == 'sharpe':
            result = self._optimize_sharpe_ratio(returns_data, constraints)
        elif method == 'min_variance':
            result = self._optimize_min_variance(returns_data, constraints)
        elif method == 'risk_parity':
            result = self._optimize_risk_parity(returns_data, constraints)
        else:
            return {'success': False, 'error': f'Unknown optimization method: {method}'}
        
        return result
    
    def _prepare_optimization_data(self) -> pd.DataFrame:
        """Chuẩn bị data cho optimization"""
        
        # Placeholder - would use actual historical returns
        return pd.DataFrame()
    
    def _optimize_sharpe_ratio(self, 
                              returns_data: pd.DataFrame,
                              constraints: Dict) -> Dict:
        """Optimize for maximum Sharpe ratio"""
        
        # Placeholder implementation
        n_assets = len(returns_data.columns)
        
        # Initial guess (equal weights)
        initial_weights = np.ones(n_assets) / n_assets
        
        # Define constraints
        bounds = [(0.01, 0.3) for _ in range(n_assets)]  # 1% to 30% per position
        constraints_list = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]  # Sum to 1
        
        # Define objective function (negative Sharpe ratio for minimization)
        def negative_sharpe_ratio(weights):
            portfolio_return = np.sum(returns_data.mean() * weights) * 252
            portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(returns_data.cov() * 252, weights)))
            sharpe_ratio = (portfolio_return - self.risk_free_rate) / portfolio_volatility
            return -sharpe_ratio
        
        # Optimize
        result = minimize(negative_sharpe_ratio, 
                         initial_weights, 
                         method='SLSQP',
                         bounds=bounds,
                         constraints=constraints_list)
        
        if result.success:
            optimal_weights = result.x
            portfolio_return = np.sum(returns_data.mean() * optimal_weights) * 252
            portfolio_volatility = np.sqrt(np.dot(optimal_weights.T, 
                                                 np.dot(returns_data.cov() * 252, optimal_weights)))
            sharpe_ratio = (portfolio_return - self.risk_free_rate) / portfolio_volatility
            
            return {
                'success': True,
                'method': 'sharpe',
                'optimal_weights': dict(zip(returns_data.columns, optimal_weights)),
                'expected_return': portfolio_return,
                'expected_volatility': portfolio_volatility,
                'sharpe_ratio': sharpe_ratio
            }
        else:
            return {'success': False, 'error': 'Optimization failed'}
    
    def _optimize_min_variance(self, 
                              returns_data: pd.DataFrame,
                              constraints: Dict) -> Dict:
        """Optimize for minimum variance"""
        
        # Similar implementation to Sharpe ratio optimization
        # but with different objective function
        
        return {'success': False, 'error': 'Not implemented'}
    
    def _optimize_risk_parity(self, 
                             returns_data: pd.DataFrame,
                             constraints: Dict) -> Dict:
        """Optimize for risk parity"""
        
        # Similar implementation
        return {'success': False, 'error': 'Not implemented'}
    
    def get_portfolio_report(self) -> Dict:
        """Lấy comprehensive portfolio report"""
        
        open_positions = []
        closed_positions = []
        
        for symbol, position in self.positions.items():
            if position.get('status') == 'OPEN':
                open_positions.append(position)
            else:
                closed_positions.append(position)
        
        # Calculate position statistics
        if closed_positions:
            closed_pnls = [p.get('pnl', 0) for p in closed_positions]
            winning_trades = sum(1 for p in closed_pnls if p > 0)
            losing_trades = sum(1 for p in closed_pnls if p < 0)
            
            avg_win = np.mean([p for p in closed_pnls if p > 0]) if any(p > 0 for p in closed_pnls) else 0
            avg_loss = np.mean([p for p in closed_pnls if p < 0]) if any(p < 0 for p in closed_pnls) else 0
            win_rate = winning_trades / len(closed_positions) if closed_positions else 0
            profit_factor = sum(p for p in closed_pnls if p > 0) / abs(sum(p for p in closed_pnls if p < 0)) \
                if sum(p for p in closed_pnls if p < 0) < 0 else 0
        else:
            winning_trades = losing_trades = avg_win = avg_loss = win_rate = profit_factor = 0
        
        return {
            'summary': {
                'initial_capital': self.initial_capital,
                'current_capital': self.current_capital,
                'total_pnl': self.current_capital - self.initial_capital,
                'total_return': (self.current_capital - self.initial_capital) / self.initial_capital,
                'open_positions_count': len(open_positions),
                'closed_positions_count': len(closed_positions),
                'winning_trades': winning_trades,
                'losing_trades': losing_trades,
                'win_rate': win_rate,
                'profit_factor': profit_factor,
                'avg_win': avg_win,
                'avg_loss': avg_loss
            },
            'performance_metrics': self.performance_metrics,
            'allocation': {
                'current_exposure': sum(p.get('position_size_pct', 0) for p in open_positions),
                'single_position_limit': self.allocation_limits['max_single_position'],
                'leverage_limit': self.allocation_limits['max_leverage']
            },
            'positions': {
                'open': open_positions,
                'closed': closed_positions[-10:]  # Last 10 closed positions
            },
            'risk_metrics': {
                'max_drawdown': self.performance_metrics['max_drawdown'],
                'portfolio_var': self._calculate_portfolio_var(),
                'expected_shortfall': self._calculate_expected_shortfall()
            }
        }
    
    def _calculate_portfolio_var(self, 
                               confidence_level: float = 0.95,
                               horizon_days: int = 1) -> float:
        """Tính portfolio Value at Risk"""
        
        if not self.positions:
            return 0.0
        
        # Simplified VaR calculation
        portfolio_value = self.performance_metrics.get('current_value', self.current_capital)
        portfolio_volatility = self.performance_metrics.get('volatility', 0.02)
        
        z_score = 1.645  # 95% confidence (normal distribution)
        
        var = portfolio_value * z_score * portfolio_volatility * np.sqrt(horizon_days/252)
        
        return var
    
    def _calculate_expected_shortfall(self,
                                    confidence_level: float = 0.95) -> float:
        """Tính Expected Shortfall (CVaR)"""
        
        # Simplified calculation
        var = self._calculate_portfolio_var(confidence_level)
        
        # ES is typically 1.25-1.5 times VaR for normal distribution
        es = var * 1.4
        
        return es
# core/position_sizer.py
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class AdvancedPositionSizer:
    """Advanced position sizing với multiple methods và risk management"""
    
    def __init__(self, 
                 initial_capital: float = 10000,
                 risk_per_trade: float = 0.02,
                 max_portfolio_risk: float = 0.10,
                 use_kelly: bool = True,
                 use_volatility_scaling: bool = True):
        
        self.initial_capital = initial_capital
        self.current_capital = initial_capital
        self.risk_per_trade = risk_per_trade
        self.max_portfolio_risk = max_portfolio_risk
        self.use_kelly = use_kelly
        self.use_volatility_scaling = use_volatility_scaling
        
        # Position sizing methods
        self.methods = {
            'fixed_fractional': self._fixed_fractional,
            'kelly_criterion': self._kelly_criterion,
            'volatility_scaling': self._volatility_scaling,
            'risk_parity': self._risk_parity,
            'regime_based': self._regime_based
        }
        
        # Weights for method combination
        self.method_weights = {
            'fixed_fractional': 0.2,
            'kelly_criterion': 0.3,
            'volatility_scaling': 0.3,
            'risk_parity': 0.1,
            'regime_based': 0.1
        }
        
        # Performance tracking
        self.trade_history = []
        self.position_history = []
        
        # Risk metrics
        self.current_risk = 0.0
        self.portfolio_var = 0.0
        
    def calculate_position_size(self, 
                              signal: Dict,
                              market_data: Dict,
                              account_info: Dict,
                              current_positions: Dict = None) -> Dict:
        """Tính position size với multiple methods"""
        
        current_positions = current_positions or {}
        
        # Get individual method calculations
        method_results = {}
        
        for method_name, method_func in self.methods.items():
            try:
                method_results[method_name] = method_func(
                    signal, market_data, account_info, current_positions
                )
            except Exception as e:
                print(f"Error in {method_name}: {e}")
                method_results[method_name] = self.risk_per_trade
        
        # Calculate weighted position size
        weighted_size = self._calculate_weighted_size(method_results)
        
        # Apply portfolio constraints
        final_size = self._apply_portfolio_constraints(
            weighted_size, current_positions, account_info
        )
        
        # Calculate position details
        position_details = self._calculate_position_details(
            final_size, signal, market_data, account_info
        )
        
        # Update current risk
        self._update_current_risk(position_details, current_positions)
        
        return position_details
    
    def _fixed_fractional(self, 
                         signal: Dict,
                         market_data: Dict,
                         account_info: Dict,
                         current_positions: Dict) -> float:
        """Fixed fractional position sizing"""
        return self.risk_per_trade
    
    def _kelly_criterion(self,
                        signal: Dict,
                        market_data: Dict,
                        account_info: Dict,
                        current_positions: Dict) -> float:
        """Kelly Criterion position sizing"""
        
        win_probability = signal.get('confidence', 0.5)
        avg_win = signal.get('expected_return', 0.02)
        avg_loss = signal.get('max_loss_pct', 0.01)
        
        if avg_loss <= 0:
            return self.risk_per_trade
        
        # Kelly formula
        kelly_f = (win_probability * avg_win - (1 - win_probability) * avg_loss) / (avg_win * avg_loss)
        
        # Use half-Kelly for conservative approach
        half_kelly = kelly_f * 0.5
        
        # Apply bounds
        bounded_kelly = max(0.01, min(0.25, half_kelly))
        
        return bounded_kelly
    
    def _volatility_scaling(self,
                          signal: Dict,
                          market_data: Dict,
                          account_info: Dict,
                          current_positions: Dict) -> float:
        """Volatility-based position sizing"""
        
        volatility = market_data.get('volatility', 0.02)
        
        # Base risk adjusted for volatility
        base_risk = self.risk_per_trade
        
        # Scale inversely with volatility
        # Higher volatility = smaller position
        if volatility > 0.03:  # High volatility
            scaled_risk = base_risk * 0.5
        elif volatility > 0.015:  # Medium volatility
            scaled_risk = base_risk * 0.8
        else:  # Low volatility
            scaled_risk = base_risk * 1.2
        
        # Ensure within bounds
        scaled_risk = max(0.01, min(0.15, scaled_risk))
        
        return scaled_risk
    
    def _risk_parity(self,
                    signal: Dict,
                    market_data: Dict,
                    account_info: Dict,
                    current_positions: Dict) -> float:
        """Risk parity position sizing"""
        
        # Simplified risk parity
        # Equal risk contribution across positions
        
        num_positions = len(current_positions)
        
        if num_positions == 0:
            return self.risk_per_trade
        
        # Allocate remaining risk budget equally
        current_risk = sum(pos.get('risk_pct', 0) for pos in current_positions.values())
        remaining_risk = self.max_portfolio_risk - current_risk
        
        if remaining_risk <= 0:
            return 0.0
        
        # Equal allocation among potential new positions
        risk_parity_size = remaining_risk / (num_positions + 1)
        
        return max(0.01, min(risk_parity_size, self.risk_per_trade))
    
    def _regime_based(self,
                     signal: Dict,
                     market_data: Dict,
                     account_info: Dict,
                     current_positions: Dict) -> float:
        """Regime-based position sizing"""
        
        regime = market_data.get('market_regime', 'normal')
        regime_factors = {
            'high_volatility': 0.5,
            'low_volatility': 1.2,
            'normal': 1.0,
            'trending': 0.8,
            'ranging': 1.0
        }
        
        regime_factor = regime_factors.get(regime, 1.0)
        
        return self.risk_per_trade * regime_factor
    
    def _calculate_weighted_size(self, method_results: Dict) -> float:
        """Tính weighted position size từ multiple methods"""
        
        valid_methods = {k: v for k, v in method_results.items() 
                        if k in self.method_weights}
        
        if not valid_methods:
            return self.risk_per_trade
        
        # Weighted average
        weighted_sum = sum(valid_methods[k] * self.method_weights[k] 
                          for k in valid_methods)
        weight_sum = sum(self.method_weights[k] for k in valid_methods)
        
        weighted_size = weighted_sum / weight_sum
        
        # Ensure reasonable bounds
        weighted_size = max(0.01, min(0.25, weighted_size))
        
        return weighted_size
    
    def _apply_portfolio_constraints(self,
                                   position_size: float,
                                   current_positions: Dict,
                                   account_info: Dict) -> float:
        """Áp dụng portfolio constraints"""
        
        capital = account_info.get('capital', self.current_capital)
        
        # 1. Check total portfolio risk
        current_risk = sum(pos.get('risk_pct', 0) for pos in current_positions.values())
        proposed_risk = current_risk + position_size
        
        if proposed_risk > self.max_portfolio_risk:
            position_size = max(0, self.max_portfolio_risk - current_risk)
        
        # 2. Check concentration risk
        max_concentration = 0.3  # 30% max in one position
        position_value = position_size * capital
        
        if position_value > capital * max_concentration:
            position_size = max_concentration
        
        # 3. Check minimum position size
        min_position = 0.01  # 1% minimum
        if position_size < min_position:
            position_size = 0.0
        
        return position_size
    
    def _calculate_position_details(self,
                                  position_size: float,
                                  signal: Dict,
                                  market_data: Dict,
                                  account_info: Dict) -> Dict:
        """Tính position details"""
        
        capital = account_info.get('capital', self.current_capital)
        current_price = signal.get('price', 0)
        
        if current_price <= 0:
            return {}
        
        # Calculate position value
        position_value = capital * position_size
        
        # Calculate number of units/shares
        if 'lot_size' in signal:
            lot_size = signal['lot_size']
            units = (position_value / current_price) / lot_size
            units = int(units) * lot_size
        else:
            # Assume forex with standard lots
            units = (position_value / current_price) * 100000  # Standard lot
            units = round(units / 1000) * 1000  # Round to nearest 0.01 lot
        
        # Recalculate actual position value
        actual_position_value = units * current_price
        actual_position_size = actual_position_value / capital
        
        # Calculate risk metrics
        stop_loss = signal.get('stop_loss', current_price * 0.98)
        risk_amount = abs(current_price - stop_loss) * units
        risk_pct = risk_amount / capital
        
        return {
            'position_size_pct': actual_position_size,
            'position_value': actual_position_value,
            'units': units,
            'entry_price': current_price,
            'stop_loss': stop_loss,
            'take_profit': signal.get('take_profit', current_price * 1.02),
            'risk_amount': risk_amount,
            'risk_pct': risk_pct,
            'reward_amount': abs(signal.get('take_profit', current_price * 1.02) - current_price) * units,
            'risk_reward_ratio': signal.get('risk_reward_ratio', 1.5),
            'position_methods': self._get_method_breakdown(position_size)
        }
    
    def _get_method_breakdown(self, final_size: float) -> Dict:
        """Lấy breakdown của methods contributing to final size"""
        return {
            'final_size': final_size,
            'method_weights': self.method_weights
        }
    
    def _update_current_risk(self, 
                           new_position: Dict,
                           current_positions: Dict):
        """Cập nhật current portfolio risk"""
        
        # Add new position risk
        new_risk = new_position.get('risk_pct', 0)
        
        # Calculate total risk
        total_risk = sum(pos.get('risk_pct', 0) for pos in current_positions.values())
        total_risk += new_risk
        
        self.current_risk = total_risk
        
        # Update portfolio VaR (simplified)
        self._update_portfolio_var(new_position, current_positions)
    
    def _update_portfolio_var(self,
                            new_position: Dict,
                            current_positions: Dict):
        """Cập nhật portfolio VaR"""
        
        # Simplified VaR calculation
        # In practice, would use correlation matrix and more sophisticated methods
        
        position_var = 0.0
        
        # Add new position VaR
        position_value = new_position.get('position_value', 0)
        volatility = 0.02  # Assume 2% daily volatility
        z_score = stats.norm.ppf(0.95)
        
        new_position_var = position_value * z_score * volatility
        position_var += new_position_var
        
        # Add existing positions VaR
        for pos in current_positions.values():
            pos_value = pos.get('position_value', 0)
            pos_var = pos_value * z_score * volatility
            position_var += pos_var
        
        # Simple diversification benefit
        diversification_benefit = 0.3  # 30% diversification benefit
        self.portfolio_var = position_var * (1 - diversification_benefit)
    
    def update_trade_result(self, trade_result: Dict):
        """Cập nhật trade result và adjust position sizing"""
        
        self.trade_history.append(trade_result)
        
        # Update current capital
        pnl = trade_result.get('pnl', 0)
        self.current_capital += pnl
        
        # Update position sizing based on performance
        self._adjust_position_sizing(trade_result)
        
        # Update method weights based on performance
        self._update_method_weights(trade_result)
    
    def _adjust_position_sizing(self, trade_result: Dict):
        """Điều chỉnh position sizing dựa trên performance"""
        
        # Simple adjustment based on recent performance
        recent_trades = self.trade_history[-10:]  # Last 10 trades
        
        if len(recent_trades) >= 5:
            winning_trades = sum(1 for t in recent_trades if t.get('pnl', 0) > 0)
            win_rate = winning_trades / len(recent_trades)
            
            # Adjust risk per trade based on win rate
            if win_rate < 0.4:
                # Reduce risk if losing
                self.risk_per_trade *= 0.8
            elif win_rate > 0.6:
                # Increase risk if winning
                self.risk_per_trade = min(0.03, self.risk_per_trade * 1.1)
            
            # Ensure bounds
            self.risk_per_trade = max(0.01, min(0.05, self.risk_per_trade))
    
    def _update_method_weights(self, trade_result: Dict):
        """Cập nhật method weights dựa trên performance"""
        
        # Placeholder for adaptive weight adjustment
        # In practice, would use reinforcement learning or optimization
        
        pass
    
    def get_position_sizing_report(self) -> Dict:
        """Lấy position sizing report"""
        
        return {
            'current_capital': self.current_capital,
            'risk_per_trade': self.risk_per_trade,
            'max_portfolio_risk': self.max_portfolio_risk,
            'current_portfolio_risk': self.current_risk,
            'portfolio_var': self.portfolio_var,
            'method_weights': self.method_weights,
            'trade_count': len(self.trade_history),
            'performance_metrics': self._calculate_performance_metrics()
        }
    
    def _calculate_performance_metrics(self) -> Dict:
        """Tính performance metrics"""
        
        if not self.trade_history:
            return {}
        
        pnls = [t.get('pnl', 0) for t in self.trade_history]
        
        return {
            'total_pnl': sum(pnls),
            'average_pnl': np.mean(pnls),
            'std_pnl': np.std(pnls),
            'sharpe_ratio': np.mean(pnls) / np.std(pnls) * np.sqrt(252) if np.std(pnls) > 0 else 0,
            'max_drawdown': self._calculate_max_drawdown(),
            'win_rate': sum(1 for p in pnls if p > 0) / len(pnls),
            'profit_factor': sum(p for p in pnls if p > 0) / abs(sum(p for p in pnls if p < 0)) 
            if sum(p for p in pnls if p < 0) < 0 else 0
        }
    
    def _calculate_max_drawdown(self) -> float:
        """Tính maximum drawdown"""
        
        if not self.trade_history:
            return 0.0
        
        equity_curve = [self.initial_capital]
        for trade in self.trade_history:
            equity_curve.append(equity_curve[-1] + trade.get('pnl', 0))
        
        equity_series = pd.Series(equity_curve)
        peak = equity_series.expanding().max()
        drawdown = (peak - equity_series) / peak
        
        return float(drawdown.max()) if not drawdown.empty else 0.0
    
    def reset(self):
        """Reset position sizer"""
        self.current_capital = self.initial_capital
        self.trade_history = []
        self.position_history = []
        self.current_risk = 0.0
        self.portfolio_var = 0.0
# core/risk_manager.py
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class AdvancedRiskManager:
    """Advanced risk management với Monte Carlo simulation và stress testing"""
    
    def __init__(self, 
                 initial_capital: float = 10000,
                 max_drawdown_limit: float = 0.20,
                 max_position_risk: float = 0.02,
                 max_correlation_risk: float = 0.30):
        
        self.initial_capital = initial_capital
        self.current_capital = initial_capital
        self.max_drawdown_limit = max_drawdown_limit
        self.max_position_risk = max_position_risk
        self.max_correlation_risk = max_correlation_risk
        
        # Risk metrics
        self.risk_metrics = {
            'sharpe_ratio': 0.0,
            'max_drawdown': 0.0,
            'volatility': 0.0,
            'var_95': 0.0,
            'cvar_95': 0.0
        }
        
        # Position tracking
        self.positions = {}
        self.position_history = []
        
        # Monte Carlo simulation
        self.monte_carlo_sims = 1000
        self.monte_carlo_results = {}
        
        # Stress testing scenarios
        self.stress_scenarios = self._initialize_stress_scenarios()
        
        # Correlation matrix
        self.correlation_matrix = pd.DataFrame()
        
        # Risk limits
        self.risk_limits = {
            'daily_loss_limit': 0.05,
            'weekly_loss_limit': 0.10,
            'monthly_loss_limit': 0.20,
            'consecutive_losses': 3
        }
        
        # Performance tracking
        self.trade_history = []
        self.daily_pnl = []
        
    def calculate_position_risk(self, 
                              signal: Dict,
                              market_data: Dict,
                              account_info: Dict) -> Dict:
        """Tính toán risk cho position"""
        
        position_risk = {
            'position_size': 0.0,
            'stop_loss': 0.0,
            'take_profit': 0.0,
            'risk_amount': 0.0,
            'reward_amount': 0.0,
            'risk_reward_ratio': 0.0,
            'probability_weighted_return': 0.0,
            'risk_adjusted_return': 0.0,
            'expected_max_loss': 0.0
        }
        
        current_price = signal.get('price', 0)
        if current_price <= 0:
            return position_risk
        
        # 1. Calculate position size using multiple methods
        position_size = self._calculate_position_size(
            signal, market_data, account_info
        )
        
        # 2. Calculate stop loss
        stop_loss = self._calculate_stop_loss(signal, market_data)
        
        # 3. Calculate take profit
        take_profit = self._calculate_take_profit(signal, market_data, stop_loss)
        
        # 4. Calculate risk metrics
        risk_amount = abs(current_price - stop_loss) * position_size
        reward_amount = abs(take_profit - current_price) * position_size
        
        risk_reward_ratio = reward_amount / risk_amount if risk_amount > 0 else 0
        
        # 5. Probability weighted return
        win_probability = signal.get('confidence', 0.5)
        probability_weighted_return = (
            win_probability * reward_amount - 
            (1 - win_probability) * risk_amount
        )
        
        # 6. Risk adjusted return (Sharpe-like)
        expected_return = probability_weighted_return / (account_info.get('capital', 10000) * position_size)
        volatility = market_data.get('volatility', 0.02)
        risk_adjusted_return = expected_return / max(volatility, 0.001)
        
        # 7. Expected max loss (VaR-like)
        expected_max_loss = self._calculate_expected_max_loss(
            position_size, current_price, stop_loss, market_data
        )
        
        position_risk.update({
            'position_size': position_size,
            'stop_loss': stop_loss,
            'take_profit': take_profit,
            'risk_amount': risk_amount,
            'reward_amount': reward_amount,
            'risk_reward_ratio': risk_reward_ratio,
            'probability_weighted_return': probability_weighted_return,
            'risk_adjusted_return': risk_adjusted_return,
            'expected_max_loss': expected_max_loss
        })
        
        return position_risk
    
    def _calculate_position_size(self, 
                               signal: Dict,
                               market_data: Dict,
                               account_info: Dict) -> float:
        """Tính position size với multiple methods"""
        
        capital = account_info.get('capital', self.current_capital)
        methods = {}
        
        # 1. Fixed fractional
        methods['fixed_fractional'] = self.max_position_risk
        
        # 2. Kelly Criterion
        win_prob = signal.get('confidence', 0.5)
        avg_win = signal.get('expected_return', 0.02)
        avg_loss = signal.get('max_loss', 0.01)
        
        if avg_loss > 0:
            kelly_f = (win_prob * avg_win - (1 - win_prob) * avg_loss) / (avg_win * avg_loss)
            methods['kelly'] = max(0.01, min(0.25, kelly_f * 0.5))  # Half-Kelly
        
        # 3. Volatility-based
        volatility = market_data.get('volatility', 0.02)
        vol_position = min(0.1, 0.02 / max(volatility, 0.001))
        methods['volatility'] = vol_position
        
        # 4. Risk parity (simplified)
        methods['risk_parity'] = 0.03  # 3% default
        
        # 5. Regime-based adjustment
        regime = market_data.get('market_regime', 'normal')
        regime_factors = {
            'high_volatility': 0.5,
            'low_volatility': 0.8,
            'normal': 1.0
        }
        methods['regime'] = 0.03 * regime_factors.get(regime, 1.0)
        
        # Weighted average
        weights = {
            'fixed_fractional': 0.2,
            'kelly': 0.3,
            'volatility': 0.3,
            'risk_parity': 0.1,
            'regime': 0.1
        }
        
        valid_methods = {k: v for k, v in methods.items() if k in weights}
        
        if valid_methods:
            weighted_avg = sum(valid_methods[k] * weights[k] for k in valid_methods)
            weighted_avg /= sum(weights[k] for k in valid_methods)
        else:
            weighted_avg = self.max_position_risk
        
        # Apply account-level limits
        max_allowed = min(
            self.max_position_risk,
            self.risk_limits['daily_loss_limit'] * 0.5
        )
        
        position_size = min(weighted_avg, max_allowed)
        
        return position_size
    
    def _calculate_stop_loss(self, signal: Dict, market_data: Dict) -> float:
        """Tính stop loss với multiple methods"""
        
        current_price = signal.get('price', 0)
        if current_price <= 0:
            return current_price * 0.95
        
        stop_methods = {}
        
        # 1. ATR-based stop
        atr = market_data.get('atr', 0)
        if atr > 0:
            if signal.get('signal') == 'BUY':
                stop_methods['atr'] = current_price - atr * 2
            else:
                stop_methods['atr'] = current_price + atr * 2
        
        # 2. Percentage stop
        percentage_stop = 0.02  # 2%
        if signal.get('signal') == 'BUY':
            stop_methods['percentage'] = current_price * (1 - percentage_stop)
        else:
            stop_methods['percentage'] = current_price * (1 + percentage_stop)
        
        # 3. Support/Resistance stop
        support_resistance = market_data.get('support_resistance', {})
        if support_resistance:
            if signal.get('signal') == 'BUY':
                supports = support_resistance.get('supports', [])
                if supports:
                    nearest_support = max([s['price'] for s in supports if s['price'] < current_price], 
                                         default=current_price * 0.95)
                    stop_methods['support'] = nearest_support * 0.98
            else:
                resistances = support_resistance.get('resistances', [])
                if resistances:
                    nearest_resistance = min([r['price'] for r in resistances if r['price'] > current_price],
                                           default=current_price * 1.05)
                    stop_methods['resistance'] = nearest_resistance * 1.02
        
        # 4. Volatility-based stop
        volatility = market_data.get('volatility', 0.02)
        vol_stop = volatility * 2.5  # 2.5 standard deviations
        if signal.get('signal') == 'BUY':
            stop_methods['volatility'] = current_price * (1 - vol_stop)
        else:
            stop_methods['volatility'] = current_price * (1 + vol_stop)
        
        # Choose the most conservative stop
        if stop_methods:
            if signal.get('signal') == 'BUY':
                stop_loss = max(stop_methods.values())
            else:
                stop_loss = min(stop_methods.values())
        else:
            # Default stop
            if signal.get('signal') == 'BUY':
                stop_loss = current_price * 0.98
            else:
                stop_loss = current_price * 1.02
        
        return stop_loss
    
    def _calculate_take_profit(self, 
                             signal: Dict, 
                             market_data: Dict,
                             stop_loss: float) -> float:
        """Tính take profit với risk/reward ratio"""
        
        current_price = signal.get('price', 0)
        if current_price <= 0:
            return current_price
        
        # Base risk/reward ratio
        base_rr_ratio = 1.5
        
        # Adjust based on market regime
        regime = market_data.get('market_regime', 'normal')
        regime_adjustments = {
            'high_volatility': 2.0,
            'low_volatility': 1.2,
            'normal': 1.5
        }
        
        rr_ratio = regime_adjustments.get(regime, base_rr_ratio)
        
        # Calculate take profit
        risk = abs(current_price - stop_loss)
        reward = risk * rr_ratio
        
        if signal.get('signal') == 'BUY':
            take_profit = current_price + reward
        else:
            take_profit = current_price - reward
        
        # Check against resistance/support levels
        support_resistance = market_data.get('support_resistance', {})
        
        if signal.get('signal') == 'BUY':
            resistances = support_resistance.get('resistances', [])
            if resistances:
                nearest_resistance = min([r['price'] for r in resistances if r['price'] > current_price],
                                       default=take_profit)
                # Don't take profit beyond strong resistance
                take_profit = min(take_profit, nearest_resistance * 0.99)
        else:
            supports = support_resistance.get('supports', [])
            if supports:
                nearest_support = max([s['price'] for s in supports if s['price'] < current_price],
                                    default=take_profit)
                # Don't take profit beyond strong support
                take_profit = max(take_profit, nearest_support * 1.01)
        
        return take_profit
    
    def _calculate_expected_max_loss(self, 
                                   position_size: float,
                                   entry_price: float,
                                   stop_loss: float,
                                   market_data: Dict) -> float:
        """Tính expected maximum loss (VaR-like)"""
        
        volatility = market_data.get('volatility', 0.02)
        position_value = entry_price * position_size
        
        # Simplified VaR calculation
        confidence_level = 0.95
        z_score = stats.norm.ppf(confidence_level)
        
        # Daily VaR
        daily_var = position_value * z_score * volatility
        
        # Account for gap risk
        gap_risk_factor = 1.5  # Assume gaps can be 1.5x daily volatility
        expected_max_loss = daily_var * gap_risk_factor
        
        # Ensure it's not more than stop loss
        stop_loss_loss = abs(entry_price - stop_loss) * position_size
        expected_max_loss = min(expected_max_loss, stop_loss_loss)
        
        return expected_max_loss
    
    def run_monte_carlo_simulation(self, 
                                 returns_series: pd.Series,
                                 num_simulations: int = 1000,
                                 horizon_days: int = 30) -> Dict:
        """Chạy Monte Carlo simulation cho portfolio"""
        
        if len(returns_series) < 20:
            return {}
        
        # Convert to numpy array
        returns = returns_series.dropna().values
        
        simulation_results = {
            'final_values': [],
            'max_drawdowns': [],
            'sharpe_ratios': [],
            'var_95': 0.0,
            'cvar_95': 0.0
        }
        
        for _ in range(num_simulations):
            # Generate random returns path
            sim_returns = np.random.choice(returns, size=horizon_days, replace=True)
            
            # Calculate cumulative returns
            sim_cumulative = np.cumprod(1 + sim_returns)
            
            # Calculate metrics
            final_value = sim_cumulative[-1]
            simulation_results['final_values'].append(final_value)
            
            # Calculate max drawdown
            peak = np.maximum.accumulate(sim_cumulative)
            drawdown = (peak - sim_cumulative) / peak
            max_drawdown = np.max(drawdown) if len(drawdown) > 0 else 0
            simulation_results['max_drawdowns'].append(max_drawdown)
            
            # Calculate Sharpe ratio (simplified)
            if np.std(sim_returns) > 0:
                sharpe = np.mean(sim_returns) / np.std(sim_returns) * np.sqrt(252)
                simulation_results['sharpe_ratios'].append(sharpe)
        
        # Calculate VaR and CVaR
        final_values = np.array(simulation_results['final_values'])
        var_95 = np.percentile(final_values - 1, 5)  # 5th percentile loss
        cvar_95 = final_values[final_values <= np.percentile(final_values, 5)].mean() - 1
        
        simulation_results.update({
            'var_95': float(var_95),
            'cvar_95': float(cvar_95),
            'expected_return': float(np.mean(final_values) - 1),
            'return_std': float(np.std(final_values)),
            'max_drawdown_95': float(np.percentile(simulation_results['max_drawdowns'], 95))
        })
        
        self.monte_carlo_results = simulation_results
        return simulation_results
    
    def run_stress_test(self, 
                       portfolio_value: float,
                       market_conditions: Dict) -> Dict:
        """Chạy stress testing cho various scenarios"""
        
        stress_results = {}
        
        for scenario_name, scenario in self.stress_scenarios.items():
            # Apply stress factors
            stressed_value = portfolio_value
            
            # Market crash scenario
            if 'market_crash' in scenario:
                crash_factor = scenario['market_crash']
                stressed_value *= (1 - crash_factor)
            
            # Volatility spike scenario
            if 'volatility_spike' in scenario:
                vol_factor = scenario['volatility_spike']
                # Increase expected losses
                stressed_value *= (1 - vol_factor * 0.1)
            
            # Liquidity crisis scenario
            if 'liquidity_crisis' in scenario:
                liquidity_factor = scenario['liquidity_crisis']
                # Additional slippage costs
                stressed_value *= (1 - liquidity_factor * 0.05)
            
            stress_results[scenario_name] = {
                'stressed_value': stressed_value,
                'loss_pct': (portfolio_value - stressed_value) / portfolio_value,
                'scenario_details': scenario
            }
        
        return stress_results
    
    def _initialize_stress_scenarios(self) -> Dict:
        """Khởi tạo các stress testing scenarios"""
        
        return {
            'mild_recession': {
                'market_crash': 0.15,
                'volatility_spike': 1.5,
                'liquidity_crisis': 0.1
            },
            'severe_recession': {
                'market_crash': 0.30,
                'volatility_spike': 2.5,
                'liquidity_crisis': 0.3
            },
            'flash_crash': {
                'market_crash': 0.10,
                'volatility_spike': 4.0,
                'liquidity_crisis': 0.5
            },
            'volatility_spike': {
                'market_crash': 0.05,
                'volatility_spike': 3.0,
                'liquidity_crisis': 0.2
            },
            'liquidity_dry_up': {
                'market_crash': 0.08,
                'volatility_spike': 1.2,
                'liquidity_crisis': 0.8
            }
        }
    
    def update_correlation_matrix(self, 
                                asset_returns: Dict[str, pd.Series]):
        """Cập nhật correlation matrix cho portfolio"""
        
        if not asset_returns:
            return
        
        # Create returns DataFrame
        returns_df = pd.DataFrame(asset_returns)
        
        # Calculate correlation matrix
        self.correlation_matrix = returns_df.corr()
        
        # Calculate portfolio concentration risk
        if not self.correlation_matrix.empty:
            avg_correlation = self.correlation_matrix.values.mean()
            self.risk_metrics['avg_correlation'] = avg_correlation
    
    def calculate_portfolio_var(self, 
                              positions: Dict[str, Dict],
                              confidence_level: float = 0.95) -> float:
        """Tính Value at Risk cho portfolio"""
        
        if not positions:
            return 0.0
        
        total_var = 0.0
        
        for symbol, position in positions.items():
            position_value = position.get('value', 0)
            volatility = position.get('volatility', 0.02)
            
            # Individual VaR
            z_score = stats.norm.ppf(confidence_level)
            individual_var = position_value * z_score * volatility
            
            total_var += individual_var
        
        # Simple diversification benefit (no correlation considered for now)
        diversification_benefit = 0.3  # Assume 30% diversification benefit
        portfolio_var = total_var * (1 - diversification_benefit)
        
        self.risk_metrics['var_95'] = portfolio_var
        
        return portfolio_var
    
    def check_risk_limits(self, 
                         new_position: Dict,
                         current_positions: Dict[str, Dict]) -> Tuple[bool, str]:
        """Kiểm tra risk limits trước khi thêm position mới"""
        
        # 1. Check individual position limit
        position_size = new_position.get('position_size', 0)
        if position_size > self.max_position_risk:
            return False, f"Position size {position_size:.2%} exceeds limit {self.max_position_risk:.2%}"
        
        # 2. Check daily loss limit
        daily_pnl = self._calculate_daily_pnl()
        if abs(daily_pnl) > self.risk_limits['daily_loss_limit'] * self.current_capital:
            return False, f"Daily P&L {daily_pnl:.2f} exceeds daily loss limit"
        
        # 3. Check consecutive losses
        if self._check_consecutive_losses():
            return False, "Too many consecutive losses"
        
        # 4. Check correlation risk
        if self._check_correlation_risk(new_position, current_positions):
            return False, "High correlation risk detected"
        
        # 5. Check drawdown limit
        current_drawdown = self._calculate_current_drawdown()
        if current_drawdown > self.max_drawdown_limit:
            return False, f"Current drawdown {current_drawdown:.2%} exceeds limit {self.max_drawdown_limit:.2%}"
        
        return True, "Risk limits OK"
    
    def _calculate_daily_pnl(self) -> float:
        """Tính daily P&L"""
        if not self.daily_pnl:
            return 0.0
        return sum(self.daily_pnl)
    
    def _check_consecutive_losses(self) -> bool:
        """Kiểm tra consecutive losses"""
        if len(self.trade_history) < 3:
            return False
        
        recent_trades = self.trade_history[-3:]
        consecutive_losses = sum(1 for trade in recent_trades if trade.get('pnl', 0) < 0)
        
        return consecutive_losses >= self.risk_limits['consecutive_losses']
    
    def _check_correlation_risk(self, 
                              new_position: Dict,
                              current_positions: Dict[str, Dict]) -> bool:
        """Kiểm tra correlation risk"""
        # Simplified correlation check
        # In practice, would use correlation matrix
        
        if not current_positions:
            return False
        
        # Count similar positions
        similar_positions = 0
        new_direction = new_position.get('signal', 'NEUTRAL')
        
        for pos in current_positions.values():
            if pos.get('signal') == new_direction:
                similar_positions += 1
        
        correlation_risk = similar_positions / max(len(current_positions), 1)
        
        return correlation_risk > self.max_correlation_risk
    
    def _calculate_current_drawdown(self) -> float:
        """Tính current drawdown"""
        if not self.trade_history:
            return 0.0
        
        equity_curve = [self.initial_capital]
        for trade in self.trade_history:
            equity_curve.append(equity_curve[-1] + trade.get('pnl', 0))
        
        equity_series = pd.Series(equity_curve)
        peak = equity_series.expanding().max()
        drawdown = (peak - equity_series) / peak
        
        return float(drawdown.iloc[-1]) if not drawdown.empty else 0.0
    
    def update_trade_history(self, trade_result: Dict):
        """Cập nhật trade history"""
        self.trade_history.append(trade_result)
        
        # Update capital
        pnl = trade_result.get('pnl', 0)
        self.current_capital += pnl
        
        # Update daily P&L
        trade_time = trade_result.get('timestamp', datetime.now())
        if self.daily_pnl and (trade_time.date() == datetime.now().date()):
            self.daily_pnl.append(pnl)
        else:
            self.daily_pnl = [pnl]
        
        # Update risk metrics periodically
        if len(self.trade_history) % 10 == 0:
            self._update_risk_metrics()
    
    def _update_risk_metrics(self):
        """Cập nhật risk metrics"""
        if not self.trade_history:
            return
        
        # Calculate returns
        returns = pd.Series([t.get('pnl', 0) / self.initial_capital 
                           for t in self.trade_history])
        
        if len(returns) >= 10:
            self.risk_metrics.update({
                'sharpe_ratio': self._calculate_sharpe_ratio(returns),
                'max_drawdown': self._calculate_historical_max_drawdown(),
                'volatility': returns.std() * np.sqrt(252),
                'win_rate': self._calculate_win_rate()
            })
    
    def _calculate_sharpe_ratio(self, returns: pd.Series) -> float:
        """Tính Sharpe ratio"""
        if len(returns) < 2 or returns.std() == 0:
            return 0.0
        
        risk_free_rate = 0.02 / 252  # Daily risk-free rate
        excess_returns = returns - risk_free_rate
        
        return excess_returns.mean() / returns.std() * np.sqrt(252)
    
    def _calculate_historical_max_drawdown(self) -> float:
        """Tính historical max drawdown"""
        if not self.trade_history:
            return 0.0
        
        equity_curve = [self.initial_capital]
        for trade in self.trade_history:
            equity_curve.append(equity_curve[-1] + trade.get('pnl', 0))
        
        equity_series = pd.Series(equity_curve)
        peak = equity_series.expanding().max()
        drawdown = (peak - equity_series) / peak
        
        return float(drawdown.max()) if not drawdown.empty else 0.0
    
    def _calculate_win_rate(self) -> float:
        """Tính win rate"""
        if not self.trade_history:
            return 0.0
        
        winning_trades = sum(1 for t in self.trade_history if t.get('pnl', 0) > 0)
        
        return winning_trades / len(self.trade_history)
    
    def get_risk_report(self) -> Dict:
        """Lấy comprehensive risk report"""
        
        return {
            'capital': {
                'initial': self.initial_capital,
                'current': self.current_capital,
                'total_pnl': self.current_capital - self.initial_capital,
                'total_return': (self.current_capital - self.initial_capital) / self.initial_capital
            },
            'risk_metrics': self.risk_metrics,
            'position_summary': {
                'open_positions': len(self.positions),
                'total_exposure': sum(p.get('value', 0) for p in self.positions.values()),
                'avg_position_size': np.mean([p.get('position_size', 0) for p in self.positions.values()]) 
                if self.positions else 0
            },
            'performance': {
                'total_trades': len(self.trade_history),
                'winning_trades': sum(1 for t in self.trade_history if t.get('pnl', 0) > 0),
                'losing_trades': sum(1 for t in self.trade_history if t.get('pnl', 0) < 0),
                'avg_win': np.mean([t.get('pnl', 0) for t in self.trade_history if t.get('pnl', 0) > 0]) 
                if any(t.get('pnl', 0) > 0 for t in self.trade_history) else 0,
                'avg_loss': np.mean([t.get('pnl', 0) for t in self.trade_history if t.get('pnl', 0) < 0]) 
                if any(t.get('pnl', 0) < 0 for t in self.trade_history) else 0,
                'largest_win': max([t.get('pnl', 0) for t in self.trade_history], default=0),
                'largest_loss': min([t.get('pnl', 0) for t in self.trade_history], default=0)
            },
            'limits': {
                'max_drawdown': self.max_drawdown_limit,
                'max_position_risk': self.max_position_risk,
                'daily_loss_limit': self.risk_limits['daily_loss_limit']
            },
            'monte_carlo_results': self.monte_carlo_results
        }
# core/signal_generator.py
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Union
from datetime import datetime, timedelta
import concurrent.futures
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

from strategies.trend_strategy import TrendStrategy
from strategies.mean_reversion_strategy import MeanReversionStrategy
from strategies.breakout_strategy import BreakoutStrategy
from core.data_manager import AdvancedDataManager
from core.risk_manager import AdvancedRiskManager

@dataclass
class SignalMetadata:
    """Metadata cho trading signal"""
    signal_id: str
    strategy_name: str
    symbol: str
    timeframe: str
    generation_time: datetime
    confidence: float
    priority: int
    expiration_time: datetime
    tags: List[str]

class AdvancedSignalGenerator:
    """Advanced signal generation với multi-strategy và confirmation"""
    
    def __init__(self, 
                 data_manager: AdvancedDataManager,
                 risk_manager: AdvancedRiskManager,
                 config: Optional[Dict] = None):
        
        self.data_manager = data_manager
        self.risk_manager = risk_manager
        self.config = config or self._default_config()
        
        # Strategy instances
        self.strategies = self._initialize_strategies()
        
        # Signal queue
        self.signal_queue = []
        self.signal_history = []
        
        # Performance tracking
        self.strategy_performance = {}
        
        # Signal filtering
        self.signal_filters = self._initialize_signal_filters()
        
        # Market regime detector
        self.market_regime = "NORMAL"
        
        # ML signal enhancer
        self.ml_enhancer_enabled = True
        
    def _default_config(self) -> Dict:
        """Default configuration"""
        return {
            'signal_expiration_minutes': 5,
            'min_confidence_threshold': 0.65,
            'max_signals_per_timeframe': 3,
            'confirmation_required': True,
            'confirmation_timeframes': ['15m', '5m'],
            'use_parallel_processing': True,
            'enable_signal_scoring': True,
            'risk_adjusted_filtering': True,
            'market_regime_filtering': True
        }
    
    def _initialize_strategies(self) -> Dict[str, Any]:
        """Khởi tạo trading strategies"""
        strategies = {
            'TREND': TrendStrategy(use_ml=True),
            'MEAN_REVERSION': MeanReversionStrategy(),
            'BREAKOUT': BreakoutStrategy(),
            # Có thể thêm strategies khác
        }
        
        # Initialize performance tracking
        for strategy_name in strategies:
            self.strategy_performance[strategy_name] = {
                'total_signals': 0,
                'profitable_signals': 0,
                'total_pnl': 0.0,
                'win_rate': 0.0,
                'avg_confidence': 0.0
            }
        
        return strategies
    
    def _initialize_signal_filters(self) -> List:
        """Khởi tạo signal filters"""
        filters = [
            self._confidence_filter,
            self._risk_filter,
            self._market_regime_filter,
            self._timeframe_alignment_filter,
            self._volume_filter,
            self._volatility_filter
        ]
        
        return filters
    
    def generate_signals(self, 
                        symbol: str,
                        timeframes: Optional[List[str]] = None) -> List[Dict]:
        """Generate signals cho symbol với multiple timeframes"""
        
        if timeframes is None:
            timeframes = ['1h', '30m', '15m', '5m']
        
        all_signals = []
        
        # Lấy data cho tất cả timeframes
        timeframe_data = {}
        for timeframe in timeframes:
            data = self.data_manager.get_data_for_strategy('TREND_ML_ENHANCED')
            if timeframe in data:
                timeframe_data[timeframe] = data[timeframe]
        
        # Generate signals với parallel processing
        if self.config['use_parallel_processing']:
            signals = self._generate_signals_parallel(symbol, timeframe_data)
        else:
            signals = self._generate_signals_sequential(symbol, timeframe_data)
        
        # Filter và score signals
        filtered_signals = self._filter_signals(signals)
        scored_signals = self._score_signals(filtered_signals)
        
        # Sort by score và limit quantity
        scored_signals.sort(key=lambda x: x['score'], reverse=True)
        final_signals = scored_signals[:self.config['max_signals_per_timeframe']]
        
        # Add to queue và history
        for signal in final_signals:
            signal_obj = self._create_signal_object(signal, symbol)
            self.signal_queue.append(signal_obj)
            self.signal_history.append(signal)
        
        return final_signals
    
    def _generate_signals_parallel(self, 
                                 symbol: str,
                                 timeframe_data: Dict) -> List[Dict]:
        """Generate signals với parallel processing"""
        signals = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            future_to_strategy = {}
            
            for strategy_name, strategy in self.strategies.items():
                for timeframe, data_dict in timeframe_data.items():
                    future = executor.submit(
                        self._generate_single_strategy_signal,
                        strategy, strategy_name, timeframe, data_dict
                    )
                    future_to_strategy[future] = (strategy_name, timeframe)
            
            for future in concurrent.futures.as_completed(future_to_strategy):
                strategy_name, timeframe = future_to_strategy[future]
                try:
                    strategy_signals = future.result()
                    signals.extend(strategy_signals)
                except Exception as e:
                    print(f"Error generating {strategy_name} signal for {timeframe}: {e}")
        
        return signals
    
    def _generate_signals_sequential(self,
                                   symbol: str,
                                   timeframe_data: Dict) -> List[Dict]:
        """Generate signals sequential"""
        signals = []
        
        for strategy_name, strategy in self.strategies.items():
            for timeframe, data_dict in timeframe_data.items():
                try:
                    strategy_signals = self._generate_single_strategy_signal(
                        strategy, strategy_name, timeframe, data_dict
                    )
                    signals.extend(strategy_signals)
                except Exception as e:
                    print(f"Error generating {strategy_name} signal for {timeframe}: {e}")
        
        return signals
    
    def _generate_single_strategy_signal(self,
                                       strategy: Any,
                                       strategy_name: str,
                                       timeframe: str,
                                       data_dict: Dict) -> List[Dict]:
        """Generate signal từ single strategy"""
        signals = []
        
        data = data_dict.get('data')
        indicators = data_dict.get('indicators', {})
        market_analysis = data_dict.get('market_analysis', {})
        
        if data is None or len(data) < 50:
            return signals
        
        # Check if strategy is applicable
        if not strategy.is_strategy_applicable(market_analysis):
            return signals
        
        # Generate signal
        signal = strategy.generate_signal(data, market_analysis)
        
        if signal:
            # Add metadata
            signal.update({
                'strategy': strategy_name,
                'timeframe': timeframe,
                'generation_time': datetime.now(),
                'symbol': 'XAUUSD',  # Hardcoded for now
                'signal_id': f"{strategy_name}_{timeframe}_{datetime.now().timestamp()}"
            })
            
            # Calculate additional metrics
            signal['score'] = self._calculate_signal_score(signal, market_analysis)
            signal['priority'] = self._determine_signal_priority(signal)
            
            signals.append(signal)
            
            # Update strategy performance
            self._update_strategy_performance(strategy_name, signal)
        
        return signals
    
    def _calculate_signal_score(self, signal: Dict, market_analysis: Dict) -> float:
        """Tính signal score tổng hợp"""
        score = 0.0
        weights = self._get_scoring_weights()
        
        # 1. Confidence score
        confidence = signal.get('confidence', 0.5)
        score += confidence * weights['confidence']
        
        # 2. Risk/Reward score
        rr_ratio = signal.get('risk_reward_ratio', 1.0)
        rr_score = min(1.0, rr_ratio / 2.0)  # Normalize to 0-1
        score += rr_score * weights['risk_reward']
        
        # 3. Market regime alignment
        regime_alignment = self._calculate_regime_alignment(signal, market_analysis)
        score += regime_alignment * weights['regime_alignment']
        
        # 4. Volume confirmation score
        volume_score = self._calculate_volume_confirmation(signal, market_analysis)
        score += volume_score * weights['volume']
        
        # 5. Timeframe alignment score
        timeframe_score = signal.get('alignment_score', 0.5)
        score += timeframe_score * weights['timeframe_alignment']
        
        # 6. ML confidence boost
        if self.ml_enhancer_enabled:
            ml_confidence = signal.get('ml_confidence', 0.5)
            score += ml_confidence * weights['ml_boost']
        
        # 7. Historical performance bonus
        strategy_name = signal.get('strategy', '')
        hist_performance = self.strategy_performance.get(strategy_name, {})
        win_rate = hist_performance.get('win_rate', 0.5)
        score += win_rate * weights['historical_performance']
        
        # Normalize score
        score = min(1.0, max(0.0, score))
        
        return score
    
    def _get_scoring_weights(self) -> Dict[str, float]:
        """Lấy weights cho signal scoring"""
        return {
            'confidence': 0.25,
            'risk_reward': 0.20,
            'regime_alignment': 0.15,
            'volume': 0.10,
            'timeframe_alignment': 0.10,
            'ml_boost': 0.10,
            'historical_performance': 0.10
        }
    
    def _calculate_regime_alignment(self, signal: Dict, market_analysis: Dict) -> float:
        """Tính regime alignment score"""
        signal_type = signal.get('signal', '')
        market_state = market_analysis.get('market_state', 'NEUTRAL')
        
        # Mapping between signal types và market states
        alignment_map = {
            'BUY': ['BULLISH', 'STRONG_BULLISH', 'OVERSOLD'],
            'SELL': ['BEARISH', 'STRONG_BEARISH', 'OVERBOUGHT']
        }
        
        expected_states = alignment_map.get(signal_type, [])
        
        if market_state in expected_states:
            return 1.0
        elif 'NEUTRAL' in market_state or 'SIDEWAYS' in market_state:
            return 0.5
        else:
            return 0.2
    
    def _calculate_volume_confirmation(self, signal: Dict, market_analysis: Dict) -> float:
        """Tính volume confirmation score"""
        volume_analysis = market_analysis.get('volume_analysis', {})
        
        if not volume_analysis:
            return 0.5
        
        volume_ratio = volume_analysis.get('ratio_to_avg', 1.0)
        volume_trend = volume_analysis.get('trend', 'NEUTRAL')
        
        # Higher volume is better for confirmation
        if volume_ratio > 1.2:
            volume_score = 0.8
        elif volume_ratio > 0.8:
            volume_score = 0.6
        else:
            volume_score = 0.4
        
        # Volume trend alignment
        signal_type = signal.get('signal', '')
        if (signal_type == 'BUY' and volume_trend == 'INCREASING') or \
           (signal_type == 'SELL' and volume_trend == 'INCREASING'):
            volume_score *= 1.2
        
        return min(1.0, volume_score)
    
    def _determine_signal_priority(self, signal: Dict) -> int:
        """Xác định signal priority"""
        score = signal.get('score', 0.5)
        
        if score > 0.8:
            return 1  # High priority
        elif score > 0.6:
            return 2  # Medium priority
        else:
            return 3  # Low priority
    
    def _update_strategy_performance(self, strategy_name: str, signal: Dict):
        """Cập nhật strategy performance"""
        if strategy_name not in self.strategy_performance:
            self.strategy_performance[strategy_name] = {
                'total_signals': 0,
                'profitable_signals': 0,
                'total_pnl': 0.0,
                'win_rate': 0.0,
                'avg_confidence': 0.0
            }
        
        perf = self.strategy_performance[strategy_name]
        perf['total_signals'] += 1
        perf['avg_confidence'] = (
            (perf['avg_confidence'] * (perf['total_signals'] - 1) + signal.get('confidence', 0.5)) 
            / perf['total_signals']
        )
    
    def _filter_signals(self, signals: List[Dict]) -> List[Dict]:
        """Filter signals với multiple criteria"""
        filtered_signals = []
        
        for signal in signals:
            if self._apply_all_filters(signal):
                filtered_signals.append(signal)
        
        return filtered_signals
    
    def _apply_all_filters(self, signal: Dict) -> bool:
        """Áp dụng tất cả filters cho signal"""
        for filter_func in self.signal_filters:
            if not filter_func(signal):
                return False
        return True
    
    def _confidence_filter(self, signal: Dict) -> bool:
        """Filter based on confidence"""
        min_confidence = self.config['min_confidence_threshold']
        confidence = signal.get('confidence', 0)
        
        return confidence >= min_confidence
    
    def _risk_filter(self, signal: Dict) -> bool:
        """Filter based on risk metrics"""
        if not self.config['risk_adjusted_filtering']:
            return True
        
        rr_ratio = signal.get('risk_reward_ratio', 1.0)
        max_loss = signal.get('max_loss_pct', 0.05)
        
        # Minimum risk/reward ratio
        if rr_ratio < 0.8:
            return False
        
        # Maximum loss per trade
        if max_loss > 0.03:  # 3% max loss
            return False
        
        return True
    
    def _market_regime_filter(self, signal: Dict) -> bool:
        """Filter based on market regime"""
        if not self.config['market_regime_filtering']:
            return True
        
        # Get current market regime từ data manager
        market_analysis = self.data_manager.get_multi_timeframe_analysis()
        overall_analysis = market_analysis.get('overall_analysis', {})
        regime = overall_analysis.get('consensus_state', 'NEUTRAL')
        
        signal_type = signal.get('signal', '')
        
        # Avoid trading against strong trend
        if 'STRONG_BULLISH' in regime and signal_type == 'SELL':
            return False
        elif 'STRONG_BEARISH' in regime and signal_type == 'BUY':
            return False
        
        return True
    
    def _timeframe_alignment_filter(self, signal: Dict) -> bool:
        """Filter based on timeframe alignment"""
        if not self.config['confirmation_required']:
            return True
        
        alignment_score = signal.get('alignment_score', 0.5)
        
        return alignment_score >= 0.6
    
    def _volume_filter(self, signal: Dict) -> bool:
        """Filter based on volume"""
        # Check if volume is adequate
        return True  # Placeholder
    
    def _volatility_filter(self, signal: Dict) -> bool:
        """Filter based on volatility"""
        # Check if volatility is within acceptable range
        return True  # Placeholder
    
    def _score_signals(self, signals: List[Dict]) -> List[Dict]:
        """Score signals với enhanced scoring"""
        if not self.config['enable_signal_scoring']:
            return signals
        
        scored_signals = []
        
        for signal in signals:
            # Calculate composite score
            composite_score = self._calculate_composite_score(signal)
            signal['composite_score'] = composite_score
            scored_signals.append(signal)
        
        return scored_signals
    
    def _calculate_composite_score(self, signal: Dict) -> float:
        """Tính composite score cho signal"""
        base_score = signal.get('score', 0.5)
        
        # Adjust based on additional factors
        adjustments = []
        
        # 1. Time of day adjustment
        time_adjustment = self._calculate_time_adjustment()
        adjustments.append(time_adjustment)
        
        # 2. News/sentiment adjustment (placeholder)
        sentiment_adjustment = 0.5  # Neutral
        adjustments.append(sentiment_adjustment)
        
        # 3. Liquidity adjustment
        liquidity_adjustment = self._calculate_liquidity_adjustment(signal)
        adjustments.append(liquidity_adjustment)
        
        # Calculate weighted adjustments
        weights = [0.3, 0.2, 0.5]  # Weights for each adjustment
        weighted_adjustment = sum(a * w for a, w in zip(adjustments[:len(weights)], weights))
        
        # Apply adjustment
        composite_score = base_score * 0.7 + weighted_adjustment * 0.3
        
        return min(1.0, max(0.0, composite_score))
    
    def _calculate_time_adjustment(self) -> float:
        """Tính time-based adjustment"""
        current_hour = datetime.now().hour
        
        # Market hours adjustment
        # Higher during active trading hours
        if 9 <= current_hour <= 17:  # Market hours
            return 0.8
        elif 0 <= current_hour <= 5:  # Low liquidity hours
            return 0.3
        else:
            return 0.5
    
    def _calculate_liquidity_adjustment(self, signal: Dict) -> float:
        """Tính liquidity adjustment"""
        # Placeholder - would use actual liquidity data
        return 0.7
    
    def _create_signal_object(self, signal_data: Dict, symbol: str) -> SignalMetadata:
        """Tạo signal object từ signal data"""
        
        return SignalMetadata(
            signal_id=signal_data.get('signal_id', ''),
            strategy_name=signal_data.get('strategy', ''),
            symbol=symbol,
            timeframe=signal_data.get('timeframe', ''),
            generation_time=signal_data.get('generation_time', datetime.now()),
            confidence=signal_data.get('confidence', 0.5),
            priority=signal_data.get('priority', 3),
            expiration_time=datetime.now() + timedelta(
                minutes=self.config['signal_expiration_minutes']
            ),
            tags=self._generate_signal_tags(signal_data)
        )
    
    def _generate_signal_tags(self, signal_data: Dict) -> List[str]:
        """Generate tags cho signal"""
        tags = []
        
        # Strategy tag
        strategy = signal_data.get('strategy', '')
        if strategy:
            tags.append(f"strategy:{strategy}")
        
        # Signal type tag
        signal_type = signal_data.get('signal', '')
        if signal_type:
            tags.append(f"signal:{signal_type}")
        
        # Timeframe tag
        timeframe = signal_data.get('timeframe', '')
        if timeframe:
            tags.append(f"timeframe:{timeframe}")
        
        # Confidence level tag
        confidence = signal_data.get('confidence', 0.5)
        if confidence > 0.8:
            tags.append("confidence:high")
        elif confidence > 0.6:
            tags.append("confidence:medium")
        else:
            tags.append("confidence:low")
        
        # Risk/Reward tag
        rr_ratio = signal_data.get('risk_reward_ratio', 1.0)
        if rr_ratio > 2.0:
            tags.append("rr:excellent")
        elif rr_ratio > 1.5:
            tags.append("rr:good")
        
        return tags
    
    def get_next_signal(self) -> Optional[Dict]:
        """Lấy next signal từ queue"""
        if not self.signal_queue:
            return None
        
        # Clean expired signals
        self._clean_expired_signals()
        
        if not self.signal_queue:
            return None
        
        # Get highest priority signal
        self.signal_queue.sort(key=lambda x: (x.priority, -x.confidence))
        next_signal_meta = self.signal_queue.pop(0)
        
        # Find corresponding signal data
        signal_data = self._find_signal_data(next_signal_meta.signal_id)
        
        if signal_data:
            # Add metadata
            signal_data['metadata'] = {
                'signal_id': next_signal_meta.signal_id,
                'expiration_time': next_signal_meta.expiration_time,
                'tags': next_signal_meta.tags
            }
        
        return signal_data
    
    def _clean_expired_signals(self):
        """Xóa expired signals khỏi queue"""
        current_time = datetime.now()
        self.signal_queue = [
            s for s in self.signal_queue 
            if s.expiration_time > current_time
        ]
    
    def _find_signal_data(self, signal_id: str) -> Optional[Dict]:
        """Tìm signal data từ history bằng signal_id"""
        for signal in self.signal_history:
            if signal.get('signal_id') == signal_id:
                return signal.copy()
        return None
    
    def update_signal_performance(self, signal_id: str, pnl: float):
        """Cập nhật performance cho signal"""
        for signal in self.signal_history:
            if signal.get('signal_id') == signal_id:
                signal['actual_pnl'] = pnl
                signal['executed'] = True
                signal['execution_time'] = datetime.now()
                
                # Update strategy performance
                strategy_name = signal.get('strategy', '')
                if strategy_name in self.strategy_performance:
                    perf = self.strategy_performance[strategy_name]
                    if pnl > 0:
                        perf['profitable_signals'] += 1
                    perf['total_pnl'] += pnl
                    perf['win_rate'] = perf['profitable_signals'] / perf['total_signals']
                
                break
    
    def get_signal_statistics(self) -> Dict:
        """Lấy signal statistics"""
        total_signals = len(self.signal_history)
        executed_signals = sum(1 for s in self.signal_history if s.get('executed', False))
        profitable_signals = sum(1 for s in self.signal_history 
                               if s.get('executed', False) and s.get('actual_pnl', 0) > 0)
        
        avg_confidence = np.mean([s.get('confidence', 0) for s in self.signal_history]) \
            if self.signal_history else 0
        
        avg_score = np.mean([s.get('score', 0) for s in self.signal_history]) \
            if self.signal_history else 0
        
        return {
            'total_signals_generated': total_signals,
            'signals_executed': executed_signals,
            'execution_rate': executed_signals / total_signals if total_signals > 0 else 0,
            'profitable_signals': profitable_signals,
            'profitability_rate': profitable_signals / executed_signals if executed_signals > 0 else 0,
            'average_confidence': avg_confidence,
            'average_score': avg_score,
            'queue_size': len(self.signal_queue),
            'strategy_performance': self.strategy_performance
        }
    
    def get_strategy_recommendations(self, 
                                   market_analysis: Dict) -> List[Dict]:
        """Đề xuất strategies dựa trên market conditions"""
        recommendations = []
        
        market_state = market_analysis.get('market_state', 'NEUTRAL')
        volatility = market_analysis.get('volatility', {}).get('current', 0.02)
        trend_strength = market_analysis.get('trend_strength', 0.5)
        
        # Evaluate each strategy
        for strategy_name, strategy in self.strategies.items():
            score = 0.0
            
            # Trend strategy
            if strategy_name == 'TREND':
                if 'BULLISH' in market_state or 'BEARISH' in market_state:
                    score = trend_strength * 0.8 + 0.2
                else:
                    score = 0.3
            
            # Mean reversion strategy
            elif strategy_name == 'MEAN_REVERSION':
                if 'SIDEWAYS' in market_state or 'RANGING' in market_state:
                    score = 0.8
                elif volatility < 0.015:
                    score = 0.6
                else:
                    score = 0.3
            
            # Breakout strategy
            elif strategy_name == 'BREAKOUT':
                if 'BREAKOUT' in market_state or volatility > 0.025:
                    score = 0.7
                else:
                    score = 0.4
            
            if score > 0.5:
                recommendations.append({
                    'strategy': strategy_name,
                    'score': score,
                    'recommendation': 'RECOMMENDED' if score > 0.7 else 'CONSIDER',
                    'reason': self._get_strategy_reason(strategy_name, market_analysis)
                })
        
        # Sort by score
        recommendations.sort(key=lambda x: x['score'], reverse=True)
        
        return recommendations
    
    def _get_strategy_reason(self, strategy_name: str, market_analysis: Dict) -> str:
        """Lấy lý do recommend strategy"""
        market_state = market_analysis.get('market_state', 'NEUTRAL')
        
        reasons = {
            'TREND': f"Market is in {market_state} trend",
            'MEAN_REVERSION': f"Market is ranging/sideways",
            'BREAKOUT': f"Market showing breakout potential"
        }
        
        return reasons.get(strategy_name, "Market conditions favorable")

indicators\


# indicators/__init__.py
from .advanced_indicators import AdvancedIndicators
from .market_analyzer import EnhancedMarketAnalyzer, MarketAnalyzer

# Có thể thêm compatibility wrapper
class MarketAnalyzerWrapper(MarketAnalyzer):
    """Wrapper để đảm bảo backward compatibility"""
    def __init__(self, data_manager=None):
        super().__init__(data_manager)
        self.enhanced_analyzer = EnhancedMarketAnalyzer(data_manager)
    
    def get_market_state(self, df):
        # Chuyển sang enhanced analyzer
        return self.enhanced_analyzer.get_market_state(df)

__all__ = ['AdvancedIndicators', 'EnhancedMarketAnalyzer', 'MarketAnalyzer']
# indicators/advanced_indicators.py
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class AdvancedIndicators:
    """Advanced technical indicators với vectorized operations"""
    
    @staticmethod
    def calculate_ichimoku_cloud(data: pd.DataFrame, 
                               periods: Tuple[int, int, int] = (9, 26, 52)) -> Dict:
        """Ichimoku Cloud Indicator"""
        high = data['high'].values
        low = data['low'].values
        close = data['close'].values
        
        tenkan_period, kijun_period, senkou_period = periods
        
        # Tenkan-sen (Conversion Line)
        tenkan_high = pd.Series(high).rolling(tenkan_period).max()
        tenkan_low = pd.Series(low).rolling(tenkan_period).min()
        tenkan_sen = (tenkan_high + tenkan_low) / 2
        
        # Kijun-sen (Base Line)
        kijun_high = pd.Series(high).rolling(kijun_period).max()
        kijun_low = pd.Series(low).rolling(kijun_period).min()
        kijun_sen = (kijun_high + kijun_low) / 2
        
        # Senkou Span A (Leading Span A)
        senkou_span_a = ((tenkan_sen + kijun_sen) / 2).shift(kijun_period)
        
        # Senkou Span B (Leading Span B)
        senkou_high = pd.Series(high).rolling(senkou_period).max()
        senkou_low = pd.Series(low).rolling(senkou_period).min()
        senkou_span_b = ((senkou_high + senkou_low) / 2).shift(kijun_period)
        
        # Chikou Span (Lagging Span)
        chikou_span = pd.Series(close).shift(-kijun_period)
        
        # Cloud
        cloud_top = pd.concat([senkou_span_a, senkou_span_b], axis=1).max(axis=1)
        cloud_bottom = pd.concat([senkou_span_a, senkou_span_b], axis=1).min(axis=1)
        
        return {
            'tenkan_sen': tenkan_sen,
            'kijun_sen': kijun_sen,
            'senkou_span_a': senkou_span_a,
            'senkou_span_b': senkou_span_b,
            'chikou_span': chikou_span,
            'cloud_top': cloud_top,
            'cloud_bottom': cloud_bottom,
            'cloud_color': senkou_span_a > senkou_span_b,  # True = green, False = red
            'price_in_cloud': (close >= cloud_bottom) & (close <= cloud_top)
        }
    
    @staticmethod
    def calculate_supertrend(data: pd.DataFrame, period: int = 10, 
                           multiplier: float = 3.0) -> Dict:
        """Supertrend Indicator"""
        high = data['high'].values
        low = data['low'].values
        close = data['close'].values
        
        # Calculate ATR
        tr = pd.DataFrame({
            'hl': high - low,
            'hc': abs(high - np.roll(close, 1)),
            'lc': abs(low - np.roll(close, 1))
        }).max(axis=1)
        atr = tr.rolling(period).mean()
        
        # Basic upper and lower bands
        hl2 = (high + low) / 2
        basic_upper = hl2 + (multiplier * atr)
        basic_lower = hl2 - (multiplier * atr)
        
        # Final upper and lower bands
        final_upper = basic_upper.copy()
        final_lower = basic_lower.copy()
        
        # Vectorized adjustment
        for i in range(period, len(close)):
            if close[i-1] <= final_upper[i-1]:
                final_upper[i] = min(basic_upper[i], final_upper[i-1])
            else:
                final_upper[i] = basic_upper[i]
            
            if close[i-1] >= final_lower[i-1]:
                final_lower[i] = max(basic_lower[i], final_lower[i-1])
            else:
                final_lower[i] = basic_lower[i]
        
        # Supertrend
        supertrend = pd.Series(np.nan, index=data.index)
        trend_direction = pd.Series(1, index=data.index)  # 1 = uptrend, -1 = downtrend
        
        for i in range(period, len(close)):
            if close[i] <= final_upper[i]:
                supertrend[i] = final_upper[i]
                trend_direction[i] = 1
            else:
                supertrend[i] = final_lower[i]
                trend_direction[i] = -1
        
        return {
            'supertrend': supertrend,
            'trend_direction': trend_direction,
            'final_upper': final_upper,
            'final_lower': final_lower,
            'atr': atr
        }
    
    @staticmethod
    def calculate_vwap(data: pd.DataFrame) -> pd.Series:
        """Volume Weighted Average Price"""
        if 'volume' not in data.columns:
            raise ValueError("VWAP requires volume data")
        
        typical_price = (data['high'] + data['low'] + data['close']) / 3
        vwap = (typical_price * data['volume']).cumsum() / data['volume'].cumsum()
        
        return vwap
    
    @staticmethod
    def calculate_rsi_advanced(data: pd.DataFrame, periods: List[int] = [7, 14, 21]) -> Dict:
        """Multi-period RSI"""
        rsis = {}
        close = data['close']
        
        for period in periods:
            delta = close.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            rsis[f'rsi_{period}'] = rsi
        
        # RSI divergence detection
        if len(close) >= 50:
            rsis['divergence'] = AdvancedIndicators._detect_rsi_divergence(
                close.values, rsis['rsi_14'].values
            )
        
        return rsis
    
    @staticmethod
    def _detect_rsi_divergence(prices: np.ndarray, rsi: np.ndarray, 
                              lookback: int = 20) -> Dict:
        """Detect RSI divergence"""
        if len(prices) < lookback * 2:
            return {'bullish_div': False, 'bearish_div': False}
        
        # Find peaks and troughs
        price_peaks = AdvancedIndicators._find_peaks(prices[-lookback:])
        price_troughs = AdvancedIndicators._find_troughs(prices[-lookback:])
        rsi_peaks = AdvancedIndicators._find_peaks(rsi[-lookback:])
        rsi_troughs = AdvancedIndicators._find_troughs(rsi[-lookback:])
        
        bullish_div = False
        bearish_div = False
        
        # Bearish divergence (price higher high, RSI lower high)
        if len(price_peaks) >= 2 and len(rsi_peaks) >= 2:
            if (prices[-lookback:][price_peaks[-1]] > prices[-lookback:][price_peaks[-2]] and
                rsi[-lookback:][rsi_peaks[-1]] < rsi[-lookback:][rsi_peaks[-2]]):
                bearish_div = True
        
        # Bullish divergence (price lower low, RSI higher low)
        if len(price_troughs) >= 2 and len(rsi_troughs) >= 2:
            if (prices[-lookback:][price_troughs[-1]] < prices[-lookback:][price_troughs[-2]] and
                rsi[-lookback:][rsi_troughs[-1]] > rsi[-lookback:][rsi_troughs[-2]]):
                bullish_div = True
        
        return {'bullish_divergence': bullish_div, 'bearish_divergence': bearish_div}
    
    @staticmethod
    def _find_peaks(data: np.ndarray, order: int = 3) -> List[int]:
        """Find local peaks"""
        from scipy.signal import argrelextrema
        peaks = argrelextrema(data, np.greater, order=order)[0]
        return peaks.tolist()
    
    @staticmethod
    def _find_troughs(data: np.ndarray, order: int = 3) -> List[int]:
        """Find local troughs"""
        from scipy.signal import argrelextrema
        troughs = argrelextrema(data, np.less, order=order)[0]
        return troughs.tolist()
    
    @staticmethod
    def calculate_fibonacci_levels(data: pd.DataFrame, 
                                  lookback: int = 100) -> Dict[str, float]:
        """Calculate Fibonacci retracement levels"""
        if len(data) < lookback:
            lookback = len(data)
        
        recent_data = data.iloc[-lookback:]
        high = recent_data['high'].max()
        low = recent_data['low'].min()
        diff = high - low
        
        levels = {
            '0.0': low,
            '0.236': low + diff * 0.236,
            '0.382': low + diff * 0.382,
            '0.5': low + diff * 0.5,
            '0.618': low + diff * 0.618,
            '0.786': low + diff * 0.786,
            '1.0': high,
            '1.272': high + diff * 0.272,
            '1.618': high + diff * 0.618
        }
        
        return levels
    
    @staticmethod
    def calculate_macd_advanced(data: pd.DataFrame, 
                              fast: int = 12, 
                              slow: int = 26, 
                              signal: int = 9) -> Dict:
        """MACD with histogram and signal line"""
        exp1 = data['close'].ewm(span=fast, adjust=False).mean()
        exp2 = data['close'].ewm(span=slow, adjust=False).mean()
        macd_line = exp1 - exp2
        signal_line = macd_line.ewm(span=signal, adjust=False).mean()
        histogram = macd_line - signal_line
        
        # MACD signals
        macd_cross = macd_line > signal_line
        histogram_trend = histogram > 0
        zero_cross = (macd_line > 0) & (macd_line.shift(1) <= 0)
        
        return {
            'macd_line': macd_line,
            'signal_line': signal_line,
            'histogram': histogram,
            'macd_cross': macd_cross,
            'histogram_trend': histogram_trend,
            'zero_cross': zero_cross,
            'divergence': AdvancedIndicators._detect_macd_divergence(
                data['close'].values, macd_line.values
            )
        }
    
    @staticmethod
    def _detect_macd_divergence(prices: np.ndarray, macd: np.ndarray) -> Dict:
        """Detect MACD divergence"""
        # Similar to RSI divergence logic
        return {'bullish_div': False, 'bearish_div': False}
    
    @staticmethod
    def calculate_bollinger_bands_advanced(data: pd.DataFrame, 
                                         period: int = 20, 
                                         std_dev: float = 2.0) -> Dict:
        """Bollinger Bands with %B and bandwidth"""
        middle_band = data['close'].rolling(window=period).mean()
        std = data['close'].rolling(window=period).std()
        
        upper_band = middle_band + (std * std_dev)
        lower_band = middle_band - (std * std_dev)
        
        # %B indicator
        percent_b = (data['close'] - lower_band) / (upper_band - lower_band)
        
        # Bandwidth
        bandwidth = (upper_band - lower_band) / middle_band
        
        # Squeeze indicator
        squeeze = bandwidth.rolling(20).mean() > bandwidth
        
        return {
            'upper_band': upper_band,
            'middle_band': middle_band,
            'lower_band': lower_band,
            'percent_b': percent_b,
            'bandwidth': bandwidth,
            'squeeze': squeeze,
            'price_position': data['close'] > middle_band  # Above/below middle band
        }
    
    @staticmethod
    def calculate_volume_profile(data: pd.DataFrame, 
                               resolution: int = 20) -> Dict:
        """Volume Profile analysis"""
        if 'volume' not in data.columns:
            return {}
        
        price_range = data['close'].max() - data['close'].min()
        bin_size = price_range / resolution
        
        # Create price bins
        bins = np.arange(data['close'].min(), data['close'].max() + bin_size, bin_size)
        
        # Digitize prices
        price_bins = np.digitize(data['close'], bins)
        
        # Aggregate volume per bin
        volume_profile = {}
        for i in range(1, len(bins)):
            mask = price_bins == i
            volume_profile[bins[i-1]] = data['volume'][mask].sum()
        
        # Find Point of Control (POC)
        poc_price = max(volume_profile, key=volume_profile.get)
        
        # Value Area (70% of volume)
        sorted_profile = sorted(volume_profile.items(), key=lambda x: x[1], reverse=True)
        cumulative_volume = 0
        total_volume = sum(volume_profile.values())
        value_area = []
        
        for price, volume in sorted_profile:
            cumulative_volume += volume
            value_area.append(price)
            if cumulative_volume >= total_volume * 0.7:
                break
        
        return {
            'volume_profile': volume_profile,
            'poc_price': poc_price,
            'poc_volume': volume_profile[poc_price],
            'value_area': value_area,
            'value_area_high': max(value_area) if value_area else poc_price,
            'value_area_low': min(value_area) if value_area else poc_price
        }
    
    @staticmethod
    def calculate_market_regime(data: pd.DataFrame, 
                              short_period: int = 20, 
                              long_period: int = 50) -> Dict:
        """Detect market regime (trending, ranging, volatile)"""
        returns = data['close'].pct_change().dropna()
        
        # Volatility
        volatility = returns.rolling(short_period).std()
        
        # Trend strength using ADX calculation
        high = data['high']
        low = data['low']
        close = data['close']
        
        # Calculate +DM and -DM
        up_move = high.diff()
        down_move = low.diff().abs()
        
        plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0)
        minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0)
        
        # True Range
        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        
        # Smooth the DMs and TR
        atr = tr.rolling(14).mean()
        plus_di = 100 * pd.Series(plus_dm).rolling(14).mean() / atr
        minus_di = 100 * pd.Series(minus_dm).rolling(14).mean() / atr
        
        # ADX
        dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)
        adx = dx.rolling(14).mean()
        
        # Determine regime
        regime = pd.Series('RANGING', index=data.index)
        regime[(adx > 25) & (plus_di > minus_di)] = 'BULLISH_TREND'
        regime[(adx > 25) & (plus_di < minus_di)] = 'BEARISH_TREND'
        regime[volatility > volatility.rolling(50).mean() * 1.5] = 'HIGH_VOLATILITY'
        
        return {
            'regime': regime.iloc[-1],
            'adx': adx.iloc[-1],
            'plus_di': plus_di.iloc[-1],
            'minus_di': minus_di.iloc[-1],
            'volatility': volatility.iloc[-1],
            'volatility_rank': (volatility.iloc[-1] / volatility.rolling(100).mean().iloc[-1])
        }
    
    @staticmethod
    def calculate_all_indicators(data: pd.DataFrame) -> Dict:
        """Calculate all advanced indicators at once (optimized)"""
        indicators = {}
        
        # Ichimoku Cloud
        indicators['ichimoku'] = AdvancedIndicators.calculate_ichimoku_cloud(data)
        
        # Supertrend
        indicators['supertrend'] = AdvancedIndicators.calculate_supertrend(data)
        
        # VWAP
        if 'volume' in data.columns:
            indicators['vwap'] = AdvancedIndicators.calculate_vwap(data)
        
        # RSI Advanced
        indicators['rsi'] = AdvancedIndicators.calculate_rsi_advanced(data)
        
        # Fibonacci Levels
        indicators['fibonacci'] = AdvancedIndicators.calculate_fibonacci_levels(data)
        
        # MACD Advanced
        indicators['macd'] = AdvancedIndicators.calculate_macd_advanced(data)
        
        # Bollinger Bands Advanced
        indicators['bollinger'] = AdvancedIndicators.calculate_bollinger_bands_advanced(data)
        
        # Volume Profile
        if 'volume' in data.columns:
            indicators['volume_profile'] = AdvancedIndicators.calculate_volume_profile(data)
        
        # Market Regime
        indicators['market_regime'] = AdvancedIndicators.calculate_market_regime(data)
        
        return indicators
# indicators/market_analyzer.py
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

from config.settings import IndicatorSettings
from .advanced_indicators import AdvancedIndicators

# ==================== ORIGINAL MARKET ANALYZER (FOR BACKWARD COMPATIBILITY) ====================

class MarketAnalyzer:
    """Phân tích trạng thái thị trường (Original version - for backward compatibility)"""
    
    def __init__(self, data_manager=None):
        self.data_manager = data_manager
        self.current_state = "TRENDING"
        self.range_high = 0.0
        self.range_low = 0.0
        self.enhanced_analyzer = EnhancedMarketAnalyzer(data_manager, use_ml=False)
    
    def get_market_state(self, df):
        """Xác định trạng thái thị trường"""
        # Use enhanced analyzer but return simplified states
        analysis = self.enhanced_analyzer.analyze_market(df)
        enhanced_state = analysis['market_state']
        
        # Map enhanced states to original states
        state_mapping = {
            'STRONG_BULLISH': 'BULLISH',
            'BULLISH': 'BULLISH',
            'BEARISH': 'BEARISH',
            'STRONG_BEARISH': 'BEARISH',
            'SIDEWAYS': 'SIDEWAYS',
            'RANGING': 'SIDEWAYS',
            'IN_CLOUD': 'TRENDING',
            'OVERBOUGHT': 'BULLISH',
            'OVERSOLD': 'BEARISH',
            'NEUTRAL': 'TRENDING'
        }
        
        return state_mapping.get(enhanced_state, 'TRENDING')
    
    def get_trend_direction(self, df):
        """Xác định hướng xu hướng"""
        if len(df) < 5:
            return "NEUTRAL"
        
        # Check for EMA columns
        if 'ema_fast' in df.columns and 'ema_slow' in df.columns:
            ema_fast = df['ema_fast'].iloc[-1]
            ema_slow = df['ema_slow'].iloc[-1]
            
            if ema_fast > ema_slow:
                return "BULLISH"
            elif ema_fast < ema_slow:
                return "BEARISH"
        
        # Fallback to price-based trend
        if len(df) >= 20:
            price_trend = df['close'].iloc[-1] > df['close'].iloc[-20]
            return "BULLISH" if price_trend else "BEARISH"
        
        return "NEUTRAL"
    
    def get_volatility(self, df):
        """Đo lường volatility"""
        if 'atr' in df.columns:
            return df['atr'].iloc[-1] / df['close'].iloc[-1]
        else:
            recent_high = df['high'].tail(14).max()
            recent_low = df['low'].tail(14).min()
            return (recent_high - recent_low) / df['close'].iloc[-1]
    
    def get_trading_zones(self, df):
        """Xác định vùng giao dịch cho sideways market"""
        if self.current_state != "SIDEWAYS":
            return None
        
        range_height = self.range_high - self.range_low
        
        return {
            'buy_zone': self.range_low + (range_height * 0.3),
            'sell_zone': self.range_high - (range_height * 0.3),
            'range_high': self.range_high,
            'range_low': self.range_low,
            'center': (self.range_high + self.range_low) / 2
        }
    
    # Helper method for original implementation
    def _calculate_range(self, df, period=20):
        """Tính range cho sideways detection"""
        if len(df) < period:
            return None, None
        
        recent_data = df.tail(period)
        self.range_high = recent_data['high'].max()
        self.range_low = recent_data['low'].min()
        
        return self.range_high, self.range_low

# ==================== ENHANCED MARKET ANALYZER ====================

class EnhancedMarketAnalyzer:
    """Enhanced market analysis với multi-timeframe và ML features"""
    
    def __init__(self, data_manager=None, use_ml: bool = False):
        self.data_manager = data_manager
        self.current_state = "NEUTRAL"
        self.regime_state = "NORMAL"
        self.volatility_regime = "MEDIUM"
        self.trend_strength = 0.5
        self.memory_length = 100  # Số period lưu trong memory
        
        # Historical states for pattern recognition
        self.state_history = []
        self.regime_history = []
        
        # ML features
        self.use_ml = use_ml
        self.feature_store = {}
        
        # Support/Resistance levels
        self.support_levels = []
        self.resistance_levels = []
        
        # Volume analysis
        self.volume_profile = {}
        
        # Multi-timeframe analysis
        self.multi_timeframe_states = {}
        
    def analyze_market(self, df: pd.DataFrame, timeframe: str = '1h') -> Dict:
        """Phân tích thị trường toàn diện"""
        if len(df) < 50:
            return self._get_default_analysis()
        
        analysis = {
            'timestamp': datetime.now(),
            'timeframe': timeframe,
            'price': float(df['close'].iloc[-1]),
            'volume': float(df['volume'].iloc[-1]) if 'volume' in df.columns else 0.0
        }
        
        # 1. Calculate advanced indicators
        indicators = AdvancedIndicators.calculate_all_indicators(df)
        analysis['indicators'] = indicators
        
        # 2. Market State Analysis
        analysis['market_state'] = self._determine_market_state(df, indicators)
        analysis['trend_direction'] = self._determine_trend_direction(df, indicators)
        analysis['trend_strength'] = self._calculate_trend_strength(df, indicators)
        
        # 3. Volatility Analysis
        analysis['volatility'] = self._analyze_volatility(df, indicators)
        analysis['volatility_regime'] = self._determine_volatility_regime(analysis['volatility'])
        
        # 4. Market Regime Detection
        analysis['market_regime'] = indicators['market_regime']['regime']
        analysis['adx_strength'] = indicators['market_regime']['adx']
        
        # 5. Support & Resistance
        analysis['support_resistance'] = self._identify_support_resistance(df, indicators)
        
        # 6. Volume Analysis
        if 'volume' in df.columns:
            analysis['volume_analysis'] = self._analyze_volume(df, indicators)
        
        # 7. Price Action Patterns
        analysis['price_patterns'] = self._detect_price_patterns(df, indicators)
        
        # 8. Market Sentiment (từ indicators)
        analysis['market_sentiment'] = self._calculate_market_sentiment(df, indicators)
        
        # 9. Risk Assessment
        analysis['risk_level'] = self._assess_market_risk(df, indicators)
        
        # 10. Trading Zones
        analysis['trading_zones'] = self._calculate_trading_zones(df, analysis)
        
        # Update internal state
        self.current_state = analysis['market_state']
        self.regime_state = analysis['market_regime']
        self.trend_strength = analysis['trend_strength']
        self.volatility_regime = analysis['volatility_regime']
        
        # Store in history
        self._update_history(analysis)
        
        return analysis
    
    def analyze_multi_timeframe(self, data_dict: Dict[str, pd.DataFrame]) -> Dict:
        """Phân tích multi-timeframe market"""
        timeframe_analysis = {}
        
        for timeframe, data in data_dict.items():
            if len(data) < 50:
                continue
                
            analysis = self.analyze_market(data, timeframe)
            timeframe_analysis[timeframe] = analysis
        
        # Overall market assessment
        overall_analysis = self._synthesize_multi_timeframe_analysis(timeframe_analysis)
        
        return {
            'timeframe_analysis': timeframe_analysis,
            'overall_analysis': overall_analysis,
            'timeframe_alignment': self._calculate_timeframe_alignment(timeframe_analysis)
        }
    
    def _determine_market_state(self, df: pd.DataFrame, indicators: Dict) -> str:
        """Xác định trạng thái thị trường với advanced logic"""
        price = df['close'].iloc[-1]
        
        # Lấy Ichimoku Cloud data
        ichimoku = indicators.get('ichimoku', {})
        
        # Kiểm tra nếu có Ichimoku Cloud
        if ichimoku and 'price_in_cloud' in ichimoku:
            if ichimoku['price_in_cloud'].iloc[-1]:
                return "IN_CLOUD"
        
        # Kiểm tra với Bollinger Bands
        bollinger = indicators.get('bollinger', {})
        if bollinger and 'percent_b' in bollinger:
            percent_b = bollinger['percent_b'].iloc[-1]
            if percent_b > 0.8:
                return "OVERBOUGHT"
            elif percent_b < 0.2:
                return "OVERSOLD"
        
        # ADX-based trend strength
        adx = indicators['market_regime']['adx']
        plus_di = indicators['market_regime']['plus_di']
        minus_di = indicators['market_regime']['minus_di']
        
        if adx > 25:
            if plus_di > minus_di:
                return "STRONG_BULLISH"
            else:
                return "STRONG_BEARISH"
        
        # Supertrend direction
        supertrend = indicators.get('supertrend', {})
        if supertrend and 'trend_direction' in supertrend:
            if supertrend['trend_direction'].iloc[-1] == 1:
                return "BULLISH"
            else:
                return "BEARISH"
        
        # Fallback to traditional analysis
        return self._traditional_market_state(df, indicators)
    
    def _traditional_market_state(self, df: pd.DataFrame, indicators: Dict) -> str:
        """Traditional market state determination"""
        if len(df) < IndicatorSettings.RANGE_PERIOD:
            return "TRENDING"
        
        recent_data = df.tail(IndicatorSettings.RANGE_PERIOD)
        
        range_high = recent_data['high'].max()
        range_low = recent_data['low'].min()
        range_center = (range_high + range_low) / 2
        
        current_price = df['close'].iloc[-1]
        range_height = range_high - range_low
        
        # Advanced volatility calculation
        volatility = self._calculate_advanced_volatility(df)
        
        # Xác định trạng thái với adaptive thresholds
        is_sideways = (
            volatility < IndicatorSettings.VOLATILITY_THRESHOLD * 1.2 and
            range_height / current_price < IndicatorSettings.RANGE_THRESHOLD * 1.5 and
            abs(current_price - range_center) / range_center < 0.015
        )
        
        # Check for breakout
        breakout_threshold = range_height * 0.05  # 5% của range
        
        if current_price > range_high + breakout_threshold:
            return "BULLISH_BREAKOUT"
        elif current_price < range_low - breakout_threshold:
            return "BEARISH_BREAKOUT"
        elif current_price > range_high:
            return "BULLISH"
        elif current_price < range_low:
            return "BEARISH"
        elif is_sideways:
            return "SIDEWAYS"
        else:
            return "TRENDING"
    
    def _determine_trend_direction(self, df: pd.DataFrame, indicators: Dict) -> str:
        """Xác định hướng xu hướng với multiple confirmations"""
        confirmations = []
        
        # 1. EMA confirmation
        if 'ema_fast' in df.columns and 'ema_slow' in df.columns:
            ema_fast = df['ema_fast'].iloc[-1]
            ema_slow = df['ema_slow'].iloc[-1]
            if ema_fast > ema_slow:
                confirmations.append('BULLISH')
            else:
                confirmations.append('BEARISH')
        
        # 2. MACD confirmation
        macd = indicators.get('macd', {})
        if macd and 'macd_cross' in macd:
            if macd['macd_cross'].iloc[-1]:
                confirmations.append('BULLISH')
            else:
                confirmations.append('BEARISH')
        
        # 3. Supertrend confirmation
        supertrend = indicators.get('supertrend', {})
        if supertrend and 'trend_direction' in supertrend:
            if supertrend['trend_direction'].iloc[-1] == 1:
                confirmations.append('BULLISH')
            else:
                confirmations.append('BEARISH')
        
        # 4. Ichimoku confirmation
        ichimoku = indicators.get('ichimoku', {})
        if ichimoku:
            tenkan = ichimoku['tenkan_sen'].iloc[-1]
            kijun = ichimoku['kijun_sen'].iloc[-1]
            if tenkan > kijun:
                confirmations.append('BULLISH')
            else:
                confirmations.append('BEARISH')
        
        # Count confirmations
        if not confirmations:
            return "NEUTRAL"
        
        bull_count = confirmations.count('BULLISH')
        bear_count = confirmations.count('BEARISH')
        
        if bull_count > bear_count:
            return "BULLISH"
        elif bear_count > bull_count:
            return "BEARISH"
        else:
            return "NEUTRAL"
    
    def _calculate_trend_strength(self, df: pd.DataFrame, indicators: Dict) -> float:
        """Tính toán strength của trend (0.0 - 1.0)"""
        strength_factors = []
        
        # 1. ADX strength
        adx = indicators['market_regime']['adx']
        adx_strength = min(1.0, adx / 50.0)  # Normalize to 0-1
        strength_factors.append(adx_strength * 0.3)
        
        # 2. EMA slope strength
        if len(df) >= 20:
            ema_slope = self._calculate_ema_slope(df, 'ema_fast', 20)
            ema_strength = min(1.0, abs(ema_slope) * 100)
            strength_factors.append(ema_strength * 0.2)
        
        # 3. Price consistency
        price_consistency = self._calculate_price_consistency(df, 10)
        strength_factors.append(price_consistency * 0.2)
        
        # 4. Volume trend confirmation
        if 'volume' in df.columns:
            volume_trend = self._analyze_volume_trend(df, 20)
            strength_factors.append(volume_trend * 0.15)
        
        # 5. Indicator alignment
        indicator_alignment = self._calculate_indicator_alignment(indicators)
        strength_factors.append(indicator_alignment * 0.15)
        
        # Weighted average
        trend_strength = sum(strength_factors)
        
        return min(1.0, max(0.0, trend_strength))
    
    def _calculate_ema_slope(self, df: pd.DataFrame, ema_column: str, period: int) -> float:
        """Tính slope của EMA"""
        if ema_column not in df.columns or len(df) < period:
            return 0.0
        
        ema_values = df[ema_column].tail(period).values
        x = np.arange(len(ema_values))
        
        # Linear regression
        slope, _ = np.polyfit(x, ema_values, 1)
        
        # Normalize by price
        current_price = df['close'].iloc[-1]
        if current_price != 0:
            return slope / current_price
        return 0.0
    
    def _calculate_price_consistency(self, df: pd.DataFrame, lookback: int) -> float:
        """Tính độ nhất quán của price movement"""
        if len(df) < lookback:
            return 0.5
        
        prices = df['close'].tail(lookback).values
        ema_fast = df['ema_fast'].tail(lookback).values if 'ema_fast' in df.columns else prices
        ema_slow = df['ema_slow'].tail(lookback).values if 'ema_slow' in df.columns else prices
        
        consistent_days = 0
        
        for i in range(len(prices)):
            # Check if price is consistently above/below EMAs
            if prices[i] > ema_fast[i] > ema_slow[i]:
                consistent_days += 1
            elif prices[i] < ema_fast[i] < ema_slow[i]:
                consistent_days += 1
        
        return consistent_days / lookback
    
    def _analyze_volume_trend(self, df: pd.DataFrame, period: int) -> float:
        """Phân tích volume trend"""
        if 'volume' not in df.columns or len(df) < period:
            return 0.5
        
        volumes = df['volume'].tail(period).values
        prices = df['close'].tail(period).values
        
        # Volume trend correlation with price
        if len(volumes) >= 10:
            volume_ma = pd.Series(volumes).rolling(5).mean()
            price_ma = pd.Series(prices).rolling(5).mean()
            
            # Check if volume confirms price trend
            volume_increasing = volume_ma.iloc[-1] > volume_ma.iloc[-10]
            price_increasing = price_ma.iloc[-1] > price_ma.iloc[-10]
            
            if volume_increasing == price_increasing:
                return 0.8  # Good confirmation
            else:
                return 0.3  # Divergence
        
        return 0.5
    
    def _calculate_indicator_alignment(self, indicators: Dict) -> float:
        """Tính độ alignment giữa các indicators"""
        alignment_scores = []
        
        # Check MACD alignment
        macd = indicators.get('macd', {})
        if macd and 'macd_cross' in macd:
            alignment_scores.append(0.5)  # Base score
        
        # Check RSI alignment
        rsi = indicators.get('rsi', {})
        if rsi and 'rsi_14' in rsi:
            rsi_value = rsi['rsi_14'].iloc[-1]
            if 40 < rsi_value < 60:
                alignment_scores.append(0.3)
            else:
                alignment_scores.append(0.1)
        
        # Check Bollinger Bands alignment
        bollinger = indicators.get('bollinger', {})
        if bollinger and 'price_position' in bollinger:
            alignment_scores.append(0.2)
        
        return np.mean(alignment_scores) if alignment_scores else 0.5
    
    def _analyze_volatility(self, df: pd.DataFrame, indicators: Dict) -> Dict:
        """Phân tích volatility chi tiết"""
        volatility_metrics = {}
        
        # Historical volatility (standard deviation of returns)
        returns = df['close'].pct_change().dropna()
        if len(returns) >= 20:
            hist_volatility = returns.std() * np.sqrt(252)  # Annualized
            volatility_metrics['historical'] = hist_volatility
            
            # Volatility regime
            rolling_vol = returns.rolling(20).std() * np.sqrt(252)
            current_vol = rolling_vol.iloc[-1] if not rolling_vol.empty else hist_volatility
            avg_vol = rolling_vol.mean()
            
            volatility_metrics['current'] = current_vol
            volatility_metrics['average'] = avg_vol
            volatility_metrics['ratio'] = current_vol / avg_vol if avg_vol > 0 else 1.0
        
        # ATR-based volatility
        if 'atr' in df.columns:
            atr = df['atr'].iloc[-1]
            current_price = df['close'].iloc[-1]
            volatility_metrics['atr_pct'] = atr / current_price if current_price > 0 else 0.0
        
        # Bollinger Bands width
        bollinger = indicators.get('bollinger', {})
        if bollinger and 'bandwidth' in bollinger:
            volatility_metrics['bb_width'] = bollinger['bandwidth'].iloc[-1]
        
        return volatility_metrics
    
    def _determine_volatility_regime(self, volatility_metrics: Dict) -> str:
        """Xác định volatility regime"""
        if not volatility_metrics:
            return "MEDIUM"
        
        current_vol = volatility_metrics.get('current', 0.2)
        avg_vol = volatility_metrics.get('average', 0.2)
        
        if current_vol > avg_vol * 1.5:
            return "HIGH"
        elif current_vol < avg_vol * 0.7:
            return "LOW"
        else:
            return "MEDIUM"
    
    def _identify_support_resistance(self, df: pd.DataFrame, indicators: Dict) -> Dict:
        """Xác định support và resistance levels"""
        levels = {
            'supports': [],
            'resistances': [],
            'key_levels': [],
            'breakout_levels': []
        }
        
        # Fibonacci levels
        fib_levels = indicators.get('fibonacci', {})
        if fib_levels:
            levels['fibonacci'] = fib_levels
        
        # Pivot Points
        pivot_levels = self._calculate_pivot_points(df)
        levels.update(pivot_levels)
        
        # Recent swing highs and lows
        swing_points = self._identify_swing_points(df, lookback=50)
        levels['swing_highs'] = swing_points['highs']
        levels['swing_lows'] = swing_points['lows']
        
        # Volume Profile levels
        volume_profile = indicators.get('volume_profile', {})
        if volume_profile:
            levels['volume_poc'] = volume_profile.get('poc_price')
            levels['value_area'] = volume_profile.get('value_area', [])
        
        # Ichimoku Cloud levels
        ichimoku = indicators.get('ichimoku', {})
        if ichimoku:
            levels['cloud_top'] = ichimoku['cloud_top'].iloc[-1] if 'cloud_top' in ichimoku else None
            levels['cloud_bottom'] = ichimoku['cloud_bottom'].iloc[-1] if 'cloud_bottom' in ichimoku else None
        
        return levels
    
    def _calculate_pivot_points(self, df: pd.DataFrame, period: str = 'daily') -> Dict:
        """Tính toán pivot points"""
        if len(df) < 2:
            return {}
        
        # Sử dụng daily high/low/close
        if period == 'daily' and len(df) >= 1:
            prev_high = df['high'].iloc[-2] if len(df) >= 2 else df['high'].iloc[-1]
            prev_low = df['low'].iloc[-2] if len(df) >= 2 else df['low'].iloc[-1]
            prev_close = df['close'].iloc[-2] if len(df) >= 2 else df['close'].iloc[-1]
        else:
            # Use recent data
            recent = df.tail(20)
            prev_high = recent['high'].max()
            prev_low = recent['low'].min()
            prev_close = df['close'].iloc[-1]
        
        pivot = (prev_high + prev_low + prev_close) / 3
        
        r1 = (2 * pivot) - prev_low
        s1 = (2 * pivot) - prev_high
        r2 = pivot + (prev_high - prev_low)
        s2 = pivot - (prev_high - prev_low)
        r3 = prev_high + 2 * (pivot - prev_low)
        s3 = prev_low - 2 * (prev_high - pivot)
        
        return {
            'pivot': pivot,
            'r1': r1, 'r2': r2, 'r3': r3,
            's1': s1, 's2': s2, 's3': s3
        }
    
    def _identify_swing_points(self, df: pd.DataFrame, lookback: int = 50) -> Dict:
        """Xác định swing highs và swing lows"""
        if len(df) < lookback:
            return {'highs': [], 'lows': []}
        
        data = df.tail(lookback)
        highs = []
        lows = []
        
        # Simple swing point detection
        for i in range(2, len(data) - 2):
            if (data['high'].iloc[i] > data['high'].iloc[i-1] and 
                data['high'].iloc[i] > data['high'].iloc[i-2] and
                data['high'].iloc[i] > data['high'].iloc[i+1] and
                data['high'].iloc[i] > data['high'].iloc[i+2]):
                highs.append({
                    'price': data['high'].iloc[i],
                    'index': i
                })
            
            if (data['low'].iloc[i] < data['low'].iloc[i-1] and 
                data['low'].iloc[i] < data['low'].iloc[i-2] and
                data['low'].iloc[i] < data['low'].iloc[i+1] and
                data['low'].iloc[i] < data['low'].iloc[i+2]):
                lows.append({
                    'price': data['low'].iloc[i],
                    'index': i
                })
        
        # Sort and get recent ones
        highs.sort(key=lambda x: x['price'], reverse=True)
        lows.sort(key=lambda x: x['price'])
        
        return {
            'highs': highs[:5],  # Top 5 recent highs
            'lows': lows[:5]     # Top 5 recent lows
        }
    
    def _analyze_volume(self, df: pd.DataFrame, indicators: Dict) -> Dict:
        """Phân tích volume chi tiết"""
        if 'volume' not in df.columns:
            return {}
        
        volume_analysis = {}
        
        # Volume statistics
        recent_volume = df['volume'].tail(20)
        volume_analysis['current'] = float(df['volume'].iloc[-1])
        volume_analysis['average_20'] = float(recent_volume.mean())
        volume_analysis['std_20'] = float(recent_volume.std())
        
        # Volume ratio
        volume_analysis['ratio_to_avg'] = (
            volume_analysis['current'] / volume_analysis['average_20'] 
            if volume_analysis['average_20'] > 0 else 1.0
        )
        
        # Volume trend
        volume_ma_short = df['volume'].rolling(5).mean()
        volume_ma_long = df['volume'].rolling(20).mean()
        volume_analysis['trend'] = 'INCREASING' if volume_ma_short.iloc[-1] > volume_ma_long.iloc[-1] else 'DECREASING'
        
        # Volume spikes
        volume_std = recent_volume.std()
        current_volume = df['volume'].iloc[-1]
        volume_analysis['is_spike'] = current_volume > (volume_analysis['average_20'] + 2 * volume_std)
        
        # Volume-Price correlation
        if len(df) >= 20:
            price_changes = df['close'].pct_change().tail(20)
            volume_changes = df['volume'].pct_change().tail(20)
            
            correlation = price_changes.corr(volume_changes)
            volume_analysis['price_correlation'] = correlation if not np.isnan(correlation) else 0.0
        
        return volume_analysis
    
    def _detect_price_patterns(self, df: pd.DataFrame, indicators: Dict) -> List[Dict]:
        """Phát hiện price patterns"""
        patterns = []
        
        if len(df) < 20:
            return patterns
        
        # Check for common patterns
        current_price = df['close'].iloc[-1]
        recent_high = df['high'].tail(10).max()
        recent_low = df['low'].tail(10).min()
        
        # Double Top/Bottom pattern
        double_pattern = self._detect_double_pattern(df)
        if double_pattern:
            patterns.append(double_pattern)
        
        # Head and Shoulders
        hs_pattern = self._detect_head_shoulders(df)
        if hs_pattern:
            patterns.append(hs_pattern)
        
        # Triangle patterns
        triangle_pattern = self._detect_triangle(df)
        if triangle_pattern:
            patterns.append(triangle_pattern)
        
        # Flag/Pennant
        flag_pattern = self._detect_flag_pennant(df)
        if flag_pattern:
            patterns.append(flag_pattern)
        
        return patterns
    
    def _detect_double_pattern(self, df: pd.DataFrame) -> Optional[Dict]:
        """Detect Double Top/Double Bottom pattern"""
        if len(df) < 30:
            return None
        
        # Simplified detection logic
        highs = df['high'].tail(30).values
        lows = df['low'].tail(30).values
        
        # Find local maxima and minima
        from scipy.signal import argrelextrema
        
        maxima_indices = argrelextrema(highs, np.greater, order=3)[0]
        minima_indices = argrelextrema(lows, np.less, order=3)[0]
        
        if len(maxima_indices) >= 2:
            # Check for double top
            last_two_maxima = maxima_indices[-2:]
            if abs(highs[last_two_maxima[0]] - highs[last_two_maxima[1]]) / highs[last_two_maxima[0]] < 0.01:
                return {
                    'pattern': 'DOUBLE_TOP',
                    'confidence': 0.7,
                    'neckline': np.mean(lows[last_two_maxima]),
                    'target': lows[min(minima_indices)] if len(minima_indices) > 0 else None
                }
        
        if len(minima_indices) >= 2:
            # Check for double bottom
            last_two_minima = minima_indices[-2:]
            if abs(lows[last_two_minima[0]] - lows[last_two_minima[1]]) / lows[last_two_minima[0]] < 0.01:
                return {
                    'pattern': 'DOUBLE_BOTTOM',
                    'confidence': 0.7,
                    'neckline': np.mean(highs[last_two_minima]),
                    'target': highs[max(maxima_indices)] if len(maxima_indices) > 0 else None
                }
        
        return None
    
    def _detect_head_shoulders(self, df: pd.DataFrame) -> Optional[Dict]:
        """Detect Head and Shoulders pattern"""
        if len(df) < 40:
            return None
        
        highs = df['high'].tail(40).values
        lows = df['low'].tail(40).values
        
        from scipy.signal import argrelextrema
        
        maxima_indices = argrelextrema(highs, np.greater, order=3)[0]
        
        if len(maxima_indices) >= 3:
            # Check for Head and Shoulders pattern
            left_shoulder = maxima_indices[-3]
            head = maxima_indices[-2]
            right_shoulder = maxima_indices[-1]
            
            # Head should be higher than shoulders
            if (highs[head] > highs[left_shoulder] and 
                highs[head] > highs[right_shoulder] and
                abs(highs[left_shoulder] - highs[right_shoulder]) / highs[head] < 0.02):
                
                # Find neckline (support level between shoulders)
                neckline_start = left_shoulder + 1
                neckline_end = right_shoulder
                neckline_lows = lows[neckline_start:neckline_end]
                
                if len(neckline_lows) > 0:
                    neckline = np.mean(neckline_lows)
                    
                    return {
                        'pattern': 'HEAD_SHOULDERS',
                        'confidence': 0.6,
                        'head_price': highs[head],
                        'neckline': neckline,
                        'target': neckline - (highs[head] - neckline)
                    }
        
        return None
    
    def _detect_triangle(self, df: pd.DataFrame) -> Optional[Dict]:
        """Detect Triangle pattern"""
        if len(df) < 30:
            return None
        
        highs = df['high'].tail(30).values
        lows = df['low'].tail(30).values
        
        # Calculate converging trendlines
        from scipy.stats import linregress
        
        # Upper trendline (resistance)
        x_upper = np.arange(len(highs))
        slope_upper, intercept_upper, _, _, _ = linregress(x_upper, highs)
        
        # Lower trendline (support)
        x_lower = np.arange(len(lows))
        slope_lower, intercept_lower, _, _, _ = linregress(x_lower, lows)
        
        # Check for convergence (slopes moving towards each other)
        if slope_upper < 0 and slope_lower > 0:  # Symmetrical triangle
            pattern_type = 'SYMMETRICAL_TRIANGLE'
            confidence = 0.7
        elif slope_upper < 0 and abs(slope_lower) < 0.001:  # Descending triangle
            pattern_type = 'DESCENDING_TRIANGLE'
            confidence = 0.6
        elif abs(slope_upper) < 0.001 and slope_lower > 0:  # Ascending triangle
            pattern_type = 'ASCENDING_TRIANGLE'
            confidence = 0.6
        else:
            return None
        
        # Calculate breakout point
        current_price = df['close'].iloc[-1]
        upper_line = slope_upper * len(highs) + intercept_upper
        lower_line = slope_lower * len(lows) + intercept_lower
        
        # Determine breakout direction
        if current_price > upper_line:
            breakout_direction = 'BULLISH'
            target = current_price + (upper_line - lower_line)
        elif current_price < lower_line:
            breakout_direction = 'BEARISH'
            target = current_price - (upper_line - lower_line)
        else:
            breakout_direction = 'PENDING'
            target = None
        
        return {
            'pattern': pattern_type,
            'confidence': confidence,
            'breakout_direction': breakout_direction,
            'upper_resistance': upper_line,
            'lower_support': lower_line,
            'target': target
        }
    
    def _detect_flag_pennant(self, df: pd.DataFrame) -> Optional[Dict]:
        """Detect Flag/Pennant pattern"""
        if len(df) < 40:
            return None
        
        prices = df['close'].tail(40).values
        
        # Look for strong move (flagpole)
        first_half = prices[:20]
        second_half = prices[20:]
        
        if len(first_half) < 10 or len(second_half) < 10:
            return None
        
        # Calculate volatility
        vol_first = np.std(first_half) / np.mean(first_half)
        vol_second = np.std(second_half) / np.mean(second_half)
        
        # Check for consolidation after strong move
        if vol_first > vol_second * 2:  # First half more volatile (flagpole)
            # Check for consolidation pattern (flag/pennant)
            from scipy.stats import linregress
            
            x = np.arange(len(second_half))
            slope, _, _, _, _ = linregress(x, second_half)
            
            if abs(slope / np.mean(second_half)) < 0.001:  # Horizontal flag
                pattern_type = 'FLAG'
            else:  # Sloping consolidation
                pattern_type = 'PENNANT'
            
            # Determine direction based on flagpole
            flagpole_direction = 'BULLISH' if first_half[-1] > first_half[0] else 'BEARISH'
            
            # Project target (measure flagpole)
            flagpole_height = abs(first_half[-1] - first_half[0])
            breakout_price = second_half[-1]
            
            if flagpole_direction == 'BULLISH':
                target = breakout_price + flagpole_height
            else:
                target = breakout_price - flagpole_height
            
            return {
                'pattern': pattern_type,
                'confidence': 0.65,
                'direction': flagpole_direction,
                'breakout_price': breakout_price,
                'target': target,
                'flagpole_height': flagpole_height
            }
        
        return None
    
    def _calculate_market_sentiment(self, df: pd.DataFrame, indicators: Dict) -> Dict:
        """Tính toán market sentiment từ các indicators"""
        sentiment_scores = {
            'bullish': 0.0,
            'bearish': 0.0,
            'neutral': 0.0,
            'overall': 'NEUTRAL'
        }
        
        # RSI sentiment
        rsi = indicators.get('rsi', {})
        if rsi and 'rsi_14' in rsi:
            rsi_value = rsi['rsi_14'].iloc[-1]
            if rsi_value > 70:
                sentiment_scores['bearish'] += 0.3
            elif rsi_value < 30:
                sentiment_scores['bullish'] += 0.3
            else:
                sentiment_scores['neutral'] += 0.2
        
        # MACD sentiment
        macd = indicators.get('macd', {})
        if macd and 'macd_cross' in macd:
            if macd['macd_cross'].iloc[-1]:
                sentiment_scores['bullish'] += 0.3
            else:
                sentiment_scores['bearish'] += 0.3
        
        # Supertrend sentiment
        supertrend = indicators.get('supertrend', {})
        if supertrend and 'trend_direction' in supertrend:
            if supertrend['trend_direction'].iloc[-1] == 1:
                sentiment_scores['bullish'] += 0.2
            else:
                sentiment_scores['bearish'] += 0.2
        
        # Ichimoku sentiment
        ichimoku = indicators.get('ichimoku', {})
        if ichimoku:
            tenkan = ichimoku['tenkan_sen'].iloc[-1]
            kijun = ichimoku['kijun_sen'].iloc[-1]
            if tenkan > kijun:
                sentiment_scores['bullish'] += 0.2
            else:
                sentiment_scores['bearish'] += 0.2
        
        # Determine overall sentiment
        if sentiment_scores['bullish'] > sentiment_scores['bearish']:
            sentiment_scores['overall'] = 'BULLISH'
        elif sentiment_scores['bearish'] > sentiment_scores['bullish']:
            sentiment_scores['overall'] = 'BEARISH'
        else:
            sentiment_scores['overall'] = 'NEUTRAL'
        
        return sentiment_scores
    
    def _assess_market_risk(self, df: pd.DataFrame, indicators: Dict) -> Dict:
        """Đánh giá risk level của thị trường"""
        risk_factors = {
            'volatility_risk': 0.0,
            'liquidity_risk': 0.0,
            'trend_risk': 0.0,
            'overall_risk': 'MEDIUM'
        }
        
        # Volatility risk
        volatility = self._calculate_advanced_volatility(df)
        if volatility > 0.03:
            risk_factors['volatility_risk'] = 0.8
        elif volatility > 0.02:
            risk_factors['volatility_risk'] = 0.5
        else:
            risk_factors['volatility_risk'] = 0.2
        
        # Trend risk (strong trends can reverse)
        trend_strength = self._calculate_trend_strength(df, indicators)
        if trend_strength > 0.8:
            risk_factors['trend_risk'] = 0.7  # High risk of reversal
        elif trend_strength < 0.3:
            risk_factors['trend_risk'] = 0.3  # Low risk, no clear trend
        else:
            risk_factors['trend_risk'] = 0.5
        
        # Overall risk assessment
        avg_risk = (risk_factors['volatility_risk'] + risk_factors['trend_risk']) / 2
        
        if avg_risk > 0.6:
            risk_factors['overall_risk'] = 'HIGH'
        elif avg_risk < 0.4:
            risk_factors['overall_risk'] = 'LOW'
        else:
            risk_factors['overall_risk'] = 'MEDIUM'
        
        return risk_factors
    
    def _calculate_trading_zones(self, df: pd.DataFrame, analysis: Dict) -> Dict:
        """Tính toán trading zones cho các market state khác nhau"""
        trading_zones = {}
        
        market_state = analysis['market_state']
        current_price = analysis['price']
        
        if market_state == "SIDEWAYS" or market_state == "IN_CLOUD":
            # Range trading zones
            if len(df) >= IndicatorSettings.RANGE_PERIOD:
                recent_data = df.tail(IndicatorSettings.RANGE_PERIOD)
                range_high = recent_data['high'].max()
                range_low = recent_data['low'].min()
                range_height = range_high - range_low
                
                trading_zones = {
                    'type': 'RANGE_BOUND',
                    'buy_zone_low': range_low + (range_height * 0.1),
                    'buy_zone_high': range_low + (range_height * 0.3),
                    'sell_zone_low': range_high - (range_height * 0.3),
                    'sell_zone_high': range_high - (range_height * 0.1),
                    'stop_buy': range_low - (range_height * 0.1),
                    'stop_sell': range_high + (range_height * 0.1),
                    'range_high': range_high,
                    'range_low': range_low
                }
        elif "BULLISH" in market_state:
            # Trend following zones
            support_levels = analysis['support_resistance'].get('supports', [])
            if support_levels:
                nearest_support = max([s['price'] for s in support_levels if s['price'] < current_price], default=current_price * 0.95)
                
                trading_zones = {
                    'type': 'TREND_FOLLOWING',
                    'entry_zone': current_price * 0.99,  # Slightly below current
                    'add_zone': nearest_support,
                    'stop_loss': nearest_support * 0.98,
                    'take_profit': current_price * 1.03  # 3% target
                }
        elif "BEARISH" in market_state:
            # Trend following for bearish
            resistance_levels = analysis['support_resistance'].get('resistances', [])
            if resistance_levels:
                nearest_resistance = min([r['price'] for r in resistance_levels if r['price'] > current_price], default=current_price * 1.05)
                
                trading_zones = {
                    'type': 'TREND_FOLLOWING',
                    'entry_zone': current_price * 1.01,  # Slightly above current
                    'add_zone': nearest_resistance,
                    'stop_loss': nearest_resistance * 1.02,
                    'take_profit': current_price * 0.97  # 3% target
                }
        
        return trading_zones
    
    def _calculate_advanced_volatility(self, df: pd.DataFrame) -> float:
        """Tính toán volatility nâng cao"""
        if len(df) < 20:
            return 0.02  # Default
        
        # Multiple volatility measures
        measures = []
        
        # 1. Historical volatility (returns std)
        returns = df['close'].pct_change().dropna()
        if len(returns) >= 10:
            hist_vol = returns.std()
            measures.append(hist_vol)
        
        # 2. ATR-based volatility
        if 'atr' in df.columns:
            atr_vol = df['atr'].iloc[-1] / df['close'].iloc[-1]
            measures.append(atr_vol)
        
        # 3. Range-based volatility
        recent_range = df['high'].tail(14).max() - df['low'].tail(14).min()
        range_vol = recent_range / df['close'].iloc[-1]
        measures.append(range_vol)
        
        # Weighted average
        weights = [0.4, 0.3, 0.3]  # Weights for each measure
        weighted_vol = sum(m * w for m, w in zip(measures[:len(weights)], weights))
        
        return weighted_vol
    
    def _synthesize_multi_timeframe_analysis(self, timeframe_analysis: Dict) -> Dict:
        """Tổng hợp analysis từ multiple timeframes"""
        if not timeframe_analysis:
            return self._get_default_analysis()
        
        # Collect states from all timeframes
        states = []
        trend_directions = []
        trend_strengths = []
        
        for tf, analysis in timeframe_analysis.items():
            states.append(analysis.get('market_state', 'NEUTRAL'))
            trend_directions.append(analysis.get('trend_direction', 'NEUTRAL'))
            trend_strengths.append(analysis.get('trend_strength', 0.5))
        
        # Determine consensus
        from collections import Counter
        state_counter = Counter(states)
        direction_counter = Counter(trend_directions)
        
        consensus_state = state_counter.most_common(1)[0][0] if state_counter else 'NEUTRAL'
        consensus_direction = direction_counter.most_common(1)[0][0] if direction_counter else 'NEUTRAL'
        avg_strength = np.mean(trend_strengths) if trend_strengths else 0.5
        
        # Calculate alignment score
        alignment_score = sum(1 for state in states if state == consensus_state) / len(states)
        
        return {
            'consensus_state': consensus_state,
            'consensus_direction': consensus_direction,
            'avg_trend_strength': avg_strength,
            'alignment_score': alignment_score,
            'timeframe_count': len(timeframe_analysis),
            'state_distribution': dict(state_counter),
            'direction_distribution': dict(direction_counter)
        }
    
    def _calculate_timeframe_alignment(self, timeframe_analysis: Dict) -> float:
        """Tính toán độ alignment giữa các timeframes"""
        if len(timeframe_analysis) < 2:
            return 1.0
        
        directions = []
        strengths = []
        
        for tf, analysis in timeframe_analysis.items():
            directions.append(analysis.get('trend_direction', 'NEUTRAL'))
            strengths.append(analysis.get('trend_strength', 0.5))
        
        # Direction alignment
        direction_alignment = sum(1 for d in directions if d == directions[0]) / len(directions)
        
        # Strength consistency
        strength_std = np.std(strengths) if len(strengths) > 1 else 0.0
        strength_alignment = 1.0 - min(1.0, strength_std * 2)
        
        # Combined alignment
        alignment = (direction_alignment * 0.6 + strength_alignment * 0.4)
        
        return alignment
    
    def _update_history(self, analysis: Dict):
        """Cập nhật historical data"""
        self.state_history.append({
            'timestamp': analysis['timestamp'],
            'state': analysis['market_state'],
            'direction': analysis['trend_direction'],
            'strength': analysis['trend_strength']
        })
        
        # Keep only recent history
        if len(self.state_history) > self.memory_length:
            self.state_history = self.state_history[-self.memory_length:]
    
    def _get_default_analysis(self) -> Dict:
        """Trả về default analysis khi không đủ data"""
        return {
            'timestamp': datetime.now(),
            'market_state': 'NEUTRAL',
            'trend_direction': 'NEUTRAL',
            'trend_strength': 0.5,
            'volatility': {'current': 0.02, 'regime': 'MEDIUM'},
            'market_regime': 'NORMAL',
            'support_resistance': {'supports': [], 'resistances': []},
            'risk_level': {'overall_risk': 'MEDIUM'},
            'trading_zones': {}
        }
import pandas as pd
import numpy as np
from config.settings import IndicatorSettings

class TechnicalIndicators:
    """Cung cấp các indicators kỹ thuật"""
    
    @staticmethod
    def add_emas(df):
        """Thêm các đường EMA"""
        df['ema_fast'] = df['close'].ewm(span=IndicatorSettings.EMA_FAST).mean()
        df['ema_slow'] = df['close'].ewm(span=IndicatorSettings.EMA_SLOW).mean()
        df['ema_trend'] = df['close'].ewm(span=IndicatorSettings.EMA_TREND).mean()
        return df
    
    @staticmethod
    def add_rsi(df, period=None):
        """Thêm RSI"""
        period = period or IndicatorSettings.RSI_PERIOD
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / (loss + 1e-10)
        df['rsi'] = 100 - (100 / (1 + rs))
        return df
    
    @staticmethod
    def add_atr(df, period=None):
        """Thêm ATR"""
        period = period or IndicatorSettings.ATR_PERIOD
        hl = df['high'] - df['low']
        hc = abs(df['high'] - df['close'].shift())
        lc = abs(df['low'] - df['close'].shift())
        tr = pd.concat([hl, hc, lc], axis=1).max(axis=1)
        df['atr'] = tr.rolling(period).mean()
        return df
    
    @staticmethod
    def add_bollinger_bands(df, period=20, std=2):
        """Thêm Bollinger Bands"""
        df['bb_middle'] = df['close'].rolling(period).mean()
        bb_std = df['close'].rolling(period).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * std)
        df['bb_lower'] = df['bb_middle'] - (bb_std * std)
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
        return df

strategies\

from .base_strategy import BaseStrategy
from .sideways_strategy import SidewaysStrategy
from .trend_strategy import TrendStrategy
from .strategy_factory import StrategyFactory

__all__ = [
    'BaseStrategy',
    'SidewaysStrategy', 
    'TrendStrategy',
    'StrategyFactory'
]
from abc import ABC, abstractmethod
from config.settings import TradingSettings

class BaseStrategy(ABC):
    """Base class cho tất cả chiến lược"""
    
    def __init__(self, name, min_confidence=None):
        self.name = name
        self.min_confidence = min_confidence or TradingSettings.MIN_CONFIDENCE
        self.performance = {
            'total_trades': 0,
            'winning_trades': 0,
            'total_profit': 0.0
        }
    
    @abstractmethod
    def is_strategy_applicable(self, market_analysis):
        """Kiểm tra chiến lược có áp dụng được không"""
        pass
    
    @abstractmethod
    def generate_signal(self, current_data, market_analysis):
        """Tạo tín hiệu giao dịch"""
        pass
    
    def calculate_confidence(self, signal_data):
        """Tính toán confidence score"""
        base_confidence = 0.5
        
        # Thêm các yếu tố tăng confidence
        if signal_data.get('rsi_extreme', False):
            base_confidence += 0.2
        
        if signal_data.get('trend_alignment', False):
            base_confidence += 0.15
            
        return min(base_confidence, 0.95)
    
    def record_trade(self, profit):
        """Ghi nhận kết quả giao dịch"""
        self.performance['total_trades'] += 1
        self.performance['total_profit'] += profit
        
        if profit > 0:
            self.performance['winning_trades'] += 1
    
    def get_performance(self):
        """Lấy thông tin hiệu suất"""
        win_rate = (self.performance['winning_trades'] / self.performance['total_trades'] * 100 
                   if self.performance['total_trades'] > 0 else 0)
        
        return {
            'name': self.name,
            'total_trades': self.performance['total_trades'],
            'win_rate': win_rate,
            'total_profit': self.performance['total_profit']
        }
"""
Mean Reversion Strategy - Chiến lược giao dịch phục hồi về trung bình
Sử dụng Bollinger Bands, RSI, và các chỉ báo mean reversion khác
"""
from strategies.base_strategy import BaseStrategy
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

try:
    import talib
    HAS_TALIB = True
except ImportError:
    HAS_TALIB = False

class MeanReversionStrategy(BaseStrategy):
    """Chiến lược giao dịch Mean Reversion nâng cao"""
    
    def __init__(self, timeframe_config: Optional[Dict] = None, use_advanced: bool = True):
        super().__init__("MEAN_REVERSION_ADVANCED", min_confidence=0.68)
        
        # Cấu hình multi-timeframe cho mean reversion
        self.timeframe_config = timeframe_config or {
            'primary': ['4h', '1h'],      # Xác định xu hướng chính
            'mean_level': ['30m', '15m'], # Xác định mức trung bình
            'entry': ['5m', '1m']         # Khung vào lệnh
        }
        
        # Trọng số timeframe
        self.timeframe_weights = {
            'primary': 0.3,
            'mean_level': 0.4,
            'entry': 0.3
        }
        
        # Cấu hình indicators
        self.indicators_config = {
            'bollinger_bands': {
                'period': 20,
                'std_dev': 2.0,
                'adaptive': True
            },
            'rsi': {
                'period': 14,
                'overbought': 70,
                'oversold': 30,
                'adaptive_thresholds': True
            },
            'stochastic': {
                'enabled': True,
                'k_period': 14,
                'd_period': 3,
                'slow_period': 3
            },
            'atr': {
                'period': 14,
                'mean_reversion_factor': 1.0
            },
            'z_score': {
                'enabled': True,
                'lookback': 20,
                'threshold': 2.0
            }
        }
        
        # Cấu hình mean reversion
        self.reversion_config = {
            'pullback_requirement': 0.5,      # Yêu cầu pullback từ extreme
            'confirmation_candles': 2,        # Số nến xác nhận
            'volume_confirmation': True,      # Xác nhận volume
            'time_decay_factor': 0.9,         # Giảm confidence theo thời gian
            'max_holding_period': 20          # Số period tối đa giữ lệnh
        }
        
        # Risk Management cho mean reversion
        self.risk_config = {
            'position_sizing': 'volatility',
            'max_position_pct': 0.04,         # 4% tối đa
            'stop_loss_type': 'bollinger',    # bollinger, atr, percentage
            'take_profit_type': 'mean',       # mean, fixed, atr
            'risk_reward_ratio': 2.0,         # Risk/Reward tối thiểu
            'partial_exits': True,            # Thoát lệnh từng phần
            'trailing_stop': False            # Trailing stop cho mean reversion
        }
        
        # Market regime detection
        self._market_regime = None
        self._regime_settings = {
            'high_volatility': {
                'std_dev_multiplier': 2.5,
                'rsi_oversold': 25,
                'rsi_overbought': 75,
                'position_factor': 0.6
            },
            'low_volatility': {
                'std_dev_multiplier': 1.5,
                'rsi_oversold': 35,
                'rsi_overbought': 65,
                'position_factor': 0.8
            },
            'normal': {
                'std_dev_multiplier': 2.0,
                'rsi_oversold': 30,
                'rsi_overbought': 70,
                'position_factor': 1.0
            }
        }
        
        # Performance tracking
        self._reversion_stats = {
            'successful_reversions': 0,
            'failed_reversions': 0,
            'avg_holding_period': 0,
            'avg_profit': 0.0
        }
        
        # Cache system
        self._indicator_cache = {}
        self._cache_timestamp = {}
        
    # ==================== CORE STRATEGY METHODS ====================
    
    def is_strategy_applicable(self, market_analysis: Dict) -> bool:
        """Kiểm tra mean reversion có áp dụng được không"""
        
        # Phát hiện market regime
        self._detect_market_regime(market_analysis)
        
        # Mean reversion hoạt động tốt trong range-bound market
        if market_analysis.get('market_state') == 'RANGING':
            return True
        
        # Cũng có thể áp dụng trong trend với pullback
        if market_analysis.get('market_state') in ['BULLISH', 'BEARISH']:
            # Kiểm tra có pullback đủ mạnh không
            volatility = market_analysis.get('volatility', 0.02)
            if volatility > 0.015:  # Đủ biến động
                return True
        
        # Kiểm tra độ lệch giá
        price_deviation = self._calculate_price_deviation(market_analysis)
        if price_deviation > 1.5:  # Giá lệch > 1.5 std
            return True
        
        return False
    
    def generate_signal(self, current_data: pd.DataFrame, market_analysis: Dict) -> Optional[Dict]:
        """Generate mean reversion signal"""
        
        # 1. Phân tích multi-timeframe
        tf_analysis = self._analyze_multi_timeframe(current_data, market_analysis)
        
        # 2. Tính toán các chỉ báo mean reversion
        mr_indicators = self._calculate_mean_reversion_indicators(current_data)
        
        # 3. Xác định các extreme points
        extremes = self._identify_extreme_points(current_data, mr_indicators)
        
        # 4. Kiểm tra điều kiện mean reversion
        if not self._check_reversion_conditions(extremes, mr_indicators, current_data):
            return None
        
        # 5. Tạo base signal
        signal = self._create_base_signal(current_data, extremes, mr_indicators)
        if signal is None:
            return None
        
        # 6. Xác nhận với multi-timeframe
        tf_confirmation = self._get_timeframe_confirmation(tf_analysis, signal)
        signal['timeframe_confirmation'] = tf_confirmation
        
        # 7. Risk Management
        risk_metrics = self._calculate_reversion_risk(signal, current_data, mr_indicators)
        signal.update(risk_metrics)
        
        # 8. Position Sizing
        position_size = self._calculate_reversion_position(signal, current_data, market_analysis)
        signal['position_size'] = position_size
        
        # 9. Confidence calculation
        signal['confidence'] = self._calculate_reversion_confidence(signal, extremes, mr_indicators)
        
        # 10. Volume confirmation
        volume_signal = self._analyze_volume_pattern(current_data, signal)
        signal['volume_signal'] = volume_signal
        
        # 11. Mean reversion quality score
        quality_score = self._evaluate_reversion_quality(signal, extremes, current_data)
        signal['reversion_quality'] = quality_score
        
        # 12. Validate final signal
        if not self._validate_reversion_signal(signal):
            return None
        
        return signal
    
    # ==================== MULTI-TIMEFRAME ANALYSIS ====================
    
    def _analyze_multi_timeframe(self, data: pd.DataFrame, market_analysis: Dict) -> Dict:
        """Phân tích multi-timeframe cho mean reversion"""
        tf_analysis = {}
        
        if 'multi_timeframe_data' in market_analysis:
            mtf_data = market_analysis['multi_timeframe_data']
            
            for tf_type, tf_list in self.timeframe_config.items():
                tf_analysis[tf_type] = []
                
                for timeframe in tf_list:
                    if timeframe in mtf_data:
                        analysis = self._analyze_timeframe_for_reversion(
                            mtf_data[timeframe], timeframe, tf_type
                        )
                        tf_analysis[tf_type].append(analysis)
        
        return tf_analysis
    
    def _analyze_timeframe_for_reversion(self, data: pd.DataFrame, 
                                       timeframe: str, tf_type: str) -> Dict:
        """Phân tích timeframe cụ thể cho mean reversion"""
        if len(data) < 50:
            return {'timeframe': timeframe, 'error': 'insufficient_data'}
        
        # Tính các chỉ báo mean reversion
        bb = self._calculate_bollinger_bands(data)
        rsi = self._calculate_rsi(data)
        stoch = self._calculate_stochastic(data) if self.indicators_config['stochastic']['enabled'] else None
        zscore = self._calculate_zscore(data) if self.indicators_config['z_score']['enabled'] else None
        
        # Phân tích mean reversion
        current_price = data['close'].iloc[-1]
        bb_position = (current_price - bb['lower'][-1]) / (bb['upper'][-1] - bb['lower'][-1])
        
        # Xác định trạng thái
        is_oversold = (
            (current_price <= bb['lower'][-1] or bb_position < 0.1) and
            rsi[-1] < self.indicators_config['rsi']['oversold']
        )
        
        is_overbought = (
            (current_price >= bb['upper'][-1] or bb_position > 0.9) and
            rsi[-1] > self.indicators_config['rsi']['overbought']
        )
        
        # Tính strength of reversion
        reversion_strength = self._calculate_reversion_strength(
            current_price, bb, rsi, stoch, zscore
        )
        
        return {
            'timeframe': timeframe,
            'tf_type': tf_type,
            'price': float(current_price),
            'bb_upper': float(bb['upper'][-1]),
            'bb_lower': float(bb['lower'][-1]),
            'bb_middle': float(bb['middle'][-1]),
            'bb_position': float(bb_position),
            'rsi': float(rsi[-1]),
            'stochastic': float(stoch[0][-1]) if stoch else None,
            'z_score': float(zscore[-1]) if zscore is not None else None,
            'is_oversold': is_oversold,
            'is_overbought': is_overbought,
            'reversion_strength': reversion_strength,
            'volatility': float(self._calculate_volatility(data)),
            'timestamp': data.index[-1]
        }
    
    # ==================== MEAN REVERSION INDICATORS ====================
    
    def _calculate_mean_reversion_indicators(self, data: pd.DataFrame) -> Dict:
        """Tính toán tất cả indicators cho mean reversion"""
        cache_key = f"indicators_{data.index[-1].timestamp()}"
        
        if cache_key in self._indicator_cache:
            return self._indicator_cache[cache_key]
        
        indicators = {}
        
        # 1. Bollinger Bands
        indicators['bollinger'] = self._calculate_bollinger_bands(data)
        
        # 2. RSI
        indicators['rsi'] = self._calculate_rsi(data)
        
        # 3. Stochastic (nếu enabled)
        if self.indicators_config['stochastic']['enabled']:
            indicators['stochastic'] = self._calculate_stochastic(data)
        
        # 4. Z-Score
        if self.indicators_config['z_score']['enabled']:
            indicators['z_score'] = self._calculate_zscore(data)
        
        # 5. ATR
        indicators['atr'] = self._calculate_atr(data)
        
        # 6. Donchian Channels
        indicators['donchian'] = self._calculate_donchian_channels(data)
        
        # 7. Price position relative to mean
        indicators['price_position'] = self._calculate_price_position(data, indicators['bollinger'])
        
        # 8. Mean reversion probability
        indicators['reversion_prob'] = self._estimate_reversion_probability(data, indicators)
        
        # Cache kết quả
        self._indicator_cache[cache_key] = indicators
        self._cache_timestamp[cache_key] = datetime.now()
        
        return indicators
    
    def _calculate_bollinger_bands(self, data: pd.DataFrame) -> Dict:
        """Tính Bollinger Bands với adaptive std dev"""
        period = self.indicators_config['bollinger_bands']['period']
        
        if self.indicators_config['bollinger_bands']['adaptive']:
            # Adaptive std dev based on volatility
            volatility = data['close'].pct_change().std()
            std_dev = self.indicators_config['bollinger_bands']['std_dev'] * (1 + volatility * 10)
        else:
            std_dev = self.indicators_config['bollinger_bands']['std_dev']
        
        if HAS_TALIB:
            upper, middle, lower = talib.BBANDS(
                data['close'],
                timeperiod=period,
                nbdevup=std_dev,
                nbdevdn=std_dev
            )
        else:
            # Manual calculation
            middle = data['close'].rolling(window=period).mean()
            std = data['close'].rolling(window=period).std()
            upper = middle + (std * std_dev)
            lower = middle - (std * std_dev)
        
        return {
            'upper': upper,
            'middle': middle,
            'lower': lower,
            'band_width': (upper - lower) / middle
        }
    
    def _calculate_rsi(self, data: pd.DataFrame, period: Optional[int] = None) -> np.ndarray:
        """Tính RSI"""
        period = period or self.indicators_config['rsi']['period']
        
        if HAS_TALIB:
            return talib.RSI(data['close'], timeperiod=period)
        else:
            # Manual RSI calculation
            delta = data['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            return 100 - (100 / (1 + rs))
    
    def _calculate_stochastic(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Tính Stochastic Oscillator"""
        k_period = self.indicators_config['stochastic']['k_period']
        d_period = self.indicators_config['stochastic']['d_period']
        slow_period = self.indicators_config['stochastic']['slow_period']
        
        if HAS_TALIB:
            slowk, slowd = talib.STOCH(
                data['high'],
                data['low'],
                data['close'],
                fastk_period=k_period,
                slowk_period=slow_period,
                slowk_matype=0,
                slowd_period=d_period,
                slowd_matype=0
            )
            return slowk, slowd
        else:
            # Manual calculation
            low_min = data['low'].rolling(window=k_period).min()
            high_max = data['high'].rolling(window=k_period).max()
            
            k = 100 * ((data['close'] - low_min) / (high_max - low_min))
            d = k.rolling(window=d_period).mean()
            
            return k, d
    
    def _calculate_zscore(self, data: pd.DataFrame) -> np.ndarray:
        """Tính Z-Score của giá"""
        lookback = self.indicators_config['z_score']['lookback']
        
        rolling_mean = data['close'].rolling(window=lookback).mean()
        rolling_std = data['close'].rolling(window=lookback).std()
        
        # Avoid division by zero
        rolling_std = rolling_std.replace(0, np.nan).ffill().bfill()
        
        zscore = (data['close'] - rolling_mean) / rolling_std
        return zscore
    
    def _calculate_donchian_channels(self, data: pd.DataFrame, period: int = 20) -> Dict:
        """Tính Donchian Channels"""
        upper = data['high'].rolling(window=period).max()
        lower = data['low'].rolling(window=period).min()
        middle = (upper + lower) / 2
        
        return {
            'upper': upper,
            'lower': lower,
            'middle': middle,
            'width': upper - lower
        }
    
    def _calculate_atr(self, data: pd.DataFrame) -> float:
        """Tính Average True Range"""
        period = self.indicators_config['atr']['period']
        
        if HAS_TALIB:
            atr = talib.ATR(data['high'], data['low'], data['close'], timeperiod=period)
            return float(atr.iloc[-1])
        else:
            # Manual ATR calculation
            high_low = data['high'] - data['low']
            high_close = np.abs(data['high'] - data['close'].shift())
            low_close = np.abs(data['low'] - data['close'].shift())
            
            tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
            atr = tr.rolling(window=period).mean()
            
            return float(atr.iloc[-1])
    
    # ==================== EXTREME POINT DETECTION ====================
    
    def _identify_extreme_points(self, data: pd.DataFrame, indicators: Dict) -> Dict:
        """Xác định các extreme points cho mean reversion"""
        extremes = {
            'oversold_points': [],
            'overbought_points': [],
            'current_extreme': None,
            'extreme_type': None
        }
        
        # Lấy chỉ báo
        bb = indicators['bollinger']
        rsi = indicators['rsi']
        
        # Adaptive thresholds based on market regime
        regime_settings = self._regime_settings.get(self._market_regime, self._regime_settings['normal'])
        
        # Tìm các extreme points gần đây (last 50 periods)
        lookback = 50
        if len(data) < lookback:
            lookback = len(data)
        
        recent_data = data.iloc[-lookback:]
        recent_bb_upper = bb['upper'].iloc[-lookback:]
        recent_bb_lower = bb['lower'].iloc[-lookback:]
        recent_rsi = rsi[-lookback:] if len(rsi) >= lookback else rsi
        
        # Tìm oversold points (price at or below lower BB, RSI oversold)
        oversold_mask = (
            (recent_data['close'] <= recent_bb_lower) &
            (recent_rsi <= regime_settings['rsi_oversold'])
        )
        
        if oversold_mask.any():
            oversold_indices = recent_data.index[oversold_mask]
            extremes['oversold_points'] = [
                {
                    'index': idx,
                    'price': float(recent_data.loc[idx, 'close']),
                    'rsi': float(recent_rsi[recent_data.index.get_loc(idx)]),
                    'bb_deviation': float((recent_bb_lower.loc[idx] - recent_data.loc[idx, 'close']) / recent_bb_lower.loc[idx])
                }
                for idx in oversold_indices
            ]
        
        # Tìm overbought points (price at or above upper BB, RSI overbought)
        overbought_mask = (
            (recent_data['close'] >= recent_bb_upper) &
            (recent_rsi >= regime_settings['rsi_overbought'])
        )
        
        if overbought_mask.any():
            overbought_indices = recent_data.index[overbought_mask]
            extremes['overbought_points'] = [
                {
                    'index': idx,
                    'price': float(recent_data.loc[idx, 'close']),
                    'rsi': float(recent_rsi[recent_data.index.get_loc(idx)]),
                    'bb_deviation': float((recent_data.loc[idx, 'close'] - recent_bb_upper.loc[idx]) / recent_bb_upper.loc[idx])
                }
                for idx in overbought_indices
            ]
        
        # Xác định current extreme
        current_price = data['close'].iloc[-1]
        current_rsi = rsi[-1] if len(rsi) > 0 else 50
        
        if (current_price <= bb['lower'].iloc[-1] and 
            current_rsi <= regime_settings['rsi_oversold']):
            extremes['current_extreme'] = {
                'price': float(current_price),
                'rsi': float(current_rsi),
                'type': 'OVERSOLD',
                'strength': self._calculate_extreme_strength(current_price, bb, 'lower')
            }
            extremes['extreme_type'] = 'OVERSOLD'
            
        elif (current_price >= bb['upper'].iloc[-1] and 
              current_rsi >= regime_settings['rsi_overbought']):
            extremes['current_extreme'] = {
                'price': float(current_price),
                'rsi': float(current_rsi),
                'type': 'OVERBOUGHT',
                'strength': self._calculate_extreme_strength(current_price, bb, 'upper')
            }
            extremes['extreme_type'] = 'OVERBOUGHT'
        
        return extremes
    
    def _calculate_extreme_strength(self, price: float, bb: Dict, band_type: str) -> float:
        """Tính strength của extreme point"""
        if band_type == 'lower':
            bb_value = bb['lower'].iloc[-1]
            bb_middle = bb['middle'].iloc[-1]
            deviation = (bb_middle - price) / (bb_middle - bb_value)
        else:  # upper
            bb_value = bb['upper'].iloc[-1]
            bb_middle = bb['middle'].iloc[-1]
            deviation = (price - bb_middle) / (bb_value - bb_middle)
        
        # Strength từ 0.5 đến 1.0
        strength = 0.5 + min(0.5, deviation)
        return float(strength)
    
    # ==================== REVERSION CONDITION CHECKING ====================
    
    def _check_reversion_conditions(self, extremes: Dict, indicators: Dict, 
                                   data: pd.DataFrame) -> bool:
        """Kiểm tra các điều kiện cho mean reversion"""
        
        # Phải có current extreme
        if extremes['current_extreme'] is None:
            return False
        
        extreme_type = extremes['extreme_type']
        extreme_data = extremes['current_extreme']
        
        # 1. Kiểm tra độ mạnh của extreme
        if extreme_data['strength'] < 0.6:
            return False
        
        # 2. Kiểm tra pullback requirement
        if not self._check_pullback_requirement(data, extreme_type):
            return False
        
        # 3. Kiểm tra confirmation candles
        if not self._check_confirmation_candles(data, extreme_type):
            return False
        
        # 4. Kiểm tra volume confirmation (nếu enabled)
        if (self.reversion_config['volume_confirmation'] and 
            not self._check_volume_confirmation(data, extreme_type)):
            return False
        
        # 5. Kiểm tra time decay
        if not self._check_time_decay(data, extremes):
            return False
        
        # 6. Kiểm tra indicator convergence
        if not self._check_indicator_convergence(indicators, extreme_type):
            return False
        
        return True
    
    def _check_pullback_requirement(self, data: pd.DataFrame, extreme_type: str) -> bool:
        """Kiểm tra pullback từ extreme"""
        lookback = min(10, len(data) - 1)
        
        if lookback < 2:
            return False
        
        recent_prices = data['close'].iloc[-lookback:].values
        price_change = (recent_prices[-1] - recent_prices[0]) / recent_prices[0]
        
        if extreme_type == 'OVERSOLD':
            # Cần có pullback lên (tăng giá từ oversold)
            return price_change > 0
        else:  # OVERBOUGHT
            # Cần có pullback xuống (giảm giá từ overbought)
            return price_change < 0
        
        # Kiểm tra độ mạnh pullback
        required_pullback = self.reversion_config['pullback_requirement']
        recent_atr = self._calculate_atr(data.iloc[-lookback:])
        price_movement = abs(price_change * data['close'].iloc[-lookback])
        
        return price_movement >= (recent_atr * required_pullback)
    
    def _check_confirmation_candles(self, data: pd.DataFrame, extreme_type: str) -> bool:
        """Kiểm tra confirmation candles"""
        confirmation_candles = self.reversion_config['confirmation_candles']
        
        if len(data) < confirmation_candles + 1:
            return False
        
        # Lấy recent candles
        recent_data = data.iloc[-(confirmation_candles + 1):]
        
        if extreme_type == 'OVERSOLD':
            # Cần có ít nhất confirmation_candles nến tăng hoặc doji
            bullish_candles = 0
            for i in range(1, len(recent_data)):
                if recent_data['close'].iloc[i] > recent_data['open'].iloc[i]:
                    bullish_candles += 1
                elif abs(recent_data['close'].iloc[i] - recent_data['open'].iloc[i]) / recent_data['open'].iloc[i] < 0.001:
                    bullish_candles += 0.5  # Doji
            
            return bullish_candles >= confirmation_candles * 0.7
            
        else:  # OVERBOUGHT
            # Cần có ít nhất confirmation_candles nến giảm hoặc doji
            bearish_candles = 0
            for i in range(1, len(recent_data)):
                if recent_data['close'].iloc[i] < recent_data['open'].iloc[i]:
                    bearish_candles += 1
                elif abs(recent_data['close'].iloc[i] - recent_data['open'].iloc[i]) / recent_data['open'].iloc[i] < 0.001:
                    bearish_candles += 0.5  # Doji
            
            return bearish_candles >= confirmation_candles * 0.7
    
    def _check_volume_confirmation(self, data: pd.DataFrame, extreme_type: str) -> bool:
        """Kiểm tra volume confirmation"""
        if 'volume' not in data.columns:
            return True  # Không có volume data
        
        lookback = min(20, len(data))
        recent_volume = data['volume'].iloc[-lookback:].values
        recent_close = data['close'].iloc[-lookback:].values
        
        # Tính volume moving average
        volume_ma = np.mean(recent_volume[-5:])
        
        # Current volume
        current_volume = data['volume'].iloc[-1]
        
        if extreme_type == 'OVERSOLD':
            # Volume cao trên oversold là bullish confirmation
            return current_volume > volume_ma * 0.8
        else:  # OVERBOUGHT
            # Volume cao trên overbought là bearish confirmation
            return current_volume > volume_ma * 0.8
    
    def _check_time_decay(self, data: pd.DataFrame, extremes: Dict) -> bool:
        """Kiểm tra time decay factor"""
        decay_factor = self.reversion_config['time_decay_factor']
        
        # Nếu không có previous extremes, return True
        if extremes['extreme_type'] == 'OVERSOLD':
            extreme_points = extremes['oversold_points']
        else:
            extreme_points = extremes['overbought_points']
        
        if len(extreme_points) < 2:
            return True
        
        # Lấy last two extremes
        last_extreme = extreme_points[-1]
        prev_extreme = extreme_points[-2]
        
        # Tính khoảng cách thời gian
        if hasattr(last_extreme['index'], 'timestamp'):
            time_diff = (data.index[-1] - last_extreme['index']).total_seconds() / 3600
        else:
            time_diff = len(data) - 1  # Approximate
            
        # Apply decay factor
        max_age = 24 / (1 - decay_factor)  # Giờ tối đa
        return time_diff <= max_age
    
    def _check_indicator_convergence(self, indicators: Dict, extreme_type: str) -> bool:
        """Kiểm tra sự hội tụ của các indicators"""
        convergence_score = 0
        total_indicators = 0
        
        # RSI convergence
        rsi = indicators['rsi']
        if len(rsi) >= 3:
            rsi_trend = np.polyfit(range(3), rsi[-3:], 1)[0]
            
            if extreme_type == 'OVERSOLD' and rsi_trend > 0:
                convergence_score += 1
            elif extreme_type == 'OVERBOUGHT' and rsi_trend < 0:
                convergence_score += 1
            total_indicators += 1
        
        # Stochastic convergence
        if 'stochastic' in indicators:
            stoch_k, stoch_d = indicators['stochastic']
            if len(stoch_k) >= 2 and len(stoch_d) >= 2:
                k_trend = stoch_k[-1] - stoch_k[-2]
                d_trend = stoch_d[-1] - stoch_d[-2]
                
                if extreme_type == 'OVERSOLD':
                    if k_trend > 0 and d_trend > 0:
                        convergence_score += 1
                else:  # OVERBOUGHT
                    if k_trend < 0 and d_trend < 0:
                        convergence_score += 1
                total_indicators += 1
        
        # Z-Score convergence
        if 'z_score' in indicators:
            zscore = indicators['z_score']
            if len(zscore) >= 2:
                z_trend = zscore[-1] - zscore[-2]
                
                if extreme_type == 'OVERSOLD' and z_trend > -0.1:  # Bắt đầu hồi về mean
                    convergence_score += 1
                elif extreme_type == 'OVERBOUGHT' and z_trend < 0.1:
                    convergence_score += 1
                total_indicators += 1
        
        return convergence_score >= max(1, total_indicators * 0.6)
    
    # ==================== SIGNAL GENERATION ====================
    
    def _create_base_signal(self, data: pd.DataFrame, extremes: Dict, 
                           indicators: Dict) -> Optional[Dict]:
        """Tạo base signal từ mean reversion"""
        extreme_data = extremes['current_extreme']
        
        if extreme_data is None:
            return None
        
        signal = {
            'signal': 'BUY' if extremes['extreme_type'] == 'OVERSOLD' else 'SELL',
            'price': data['close'].iloc[-1],
            'extreme_type': extremes['extreme_type'],
            'extreme_strength': extreme_data['strength'],
            'rsi': float(indicators['rsi'][-1]) if len(indicators['rsi']) > 0 else 50,
            'bb_position': self._calculate_price_position(data, indicators['bollinger']),
            'z_score': float(indicators['z_score'][-1]) if 'z_score' in indicators else 0,
            'timestamp': datetime.now(),
            'strategy': self.name
        }
        
        # Thêm thông tin Bollinger Bands
        bb = indicators['bollinger']
        signal.update({
            'bb_upper': float(bb['upper'].iloc[-1]),
            'bb_lower': float(bb['lower'].iloc[-1]),
            'bb_middle': float(bb['middle'].iloc[-1]),
            'bb_width': float(bb['band_width'].iloc[-1])
        })
        
        # Thêm stochastic nếu có
        if 'stochastic' in indicators:
            stoch_k, stoch_d = indicators['stochastic']
            signal.update({
                'stoch_k': float(stoch_k[-1]),
                'stoch_d': float(stoch_d[-1])
            })
        
        return signal
    
    def _get_timeframe_confirmation(self, tf_analysis: Dict, signal: Dict) -> Dict:
        """Lấy xác nhận từ multi-timeframe analysis"""
        confirmation = {
            'primary_confirm': 0,
            'mean_level_confirm': 0,
            'entry_confirm': 0,
            'overall_score': 0
        }
        
        # Kiểm tra từng nhóm timeframe
        for tf_type in ['primary', 'mean_level', 'entry']:
            if tf_type in tf_analysis and tf_analysis[tf_type]:
                confirm_count = 0
                total_count = len(tf_analysis[tf_type])
                
                for analysis in tf_analysis[tf_type]:
                    if signal['signal'] == 'BUY':
                        if analysis.get('is_oversold', False):
                            confirm_count += 1
                    else:  # SELL
                        if analysis.get('is_overbought', False):
                            confirm_count += 1
                
                confirmation[f'{tf_type}_confirm'] = confirm_count / total_count if total_count > 0 else 0
        
        # Tính overall score có trọng số
        weights = self.timeframe_weights
        confirmation['overall_score'] = (
            confirmation['primary_confirm'] * weights['primary'] +
            confirmation['mean_level_confirm'] * weights['mean_level'] +
            confirmation['entry_confirm'] * weights['entry']
        )
        
        return confirmation
    
    # ==================== RISK MANAGEMENT ====================
    
    def _calculate_reversion_risk(self, signal: Dict, data: pd.DataFrame, 
                                indicators: Dict) -> Dict:
        """Tính toán risk metrics cho mean reversion"""
        current_price = signal['price']
        
        # Xác định stop loss
        if self.risk_config['stop_loss_type'] == 'bollinger':
            if signal['signal'] == 'BUY':
                stop_loss = indicators['bollinger']['lower'].iloc[-1] * 0.99
            else:
                stop_loss = indicators['bollinger']['upper'].iloc[-1] * 1.01
        elif self.risk_config['stop_loss_type'] == 'atr':
            atr = indicators['atr']
            if signal['signal'] == 'BUY':
                stop_loss = current_price - (atr * 1.5)
            else:
                stop_loss = current_price + (atr * 1.5)
        else:  # percentage
            stop_loss_pct = 0.02
            if signal['signal'] == 'BUY':
                stop_loss = current_price * (1 - stop_loss_pct)
            else:
                stop_loss = current_price * (1 + stop_loss_pct)
        
        # Xác định take profit
        if self.risk_config['take_profit_type'] == 'mean':
            take_profit = indicators['bollinger']['middle'].iloc[-1]
        elif self.risk_config['take_profit_type'] == 'atr':
            atr = indicators['atr']
            if signal['signal'] == 'BUY':
                take_profit = current_price + (atr * self.risk_config['risk_reward_ratio'])
            else:
                take_profit = current_price - (atr * self.risk_config['risk_reward_ratio'])
        else:  # fixed
            if signal['signal'] == 'BUY':
                take_profit = current_price * (1 + 0.03)  # 3% fixed
            else:
                take_profit = current_price * (1 - 0.03)
        
        # Tính risk/reward
        risk = abs(current_price - stop_loss)
        reward = abs(take_profit - current_price)
        risk_reward_ratio = reward / risk if risk > 0 else 1.0
        
        # Expected profit based on mean reversion probability
        reversion_prob = indicators.get('reversion_prob', 0.6)
        expected_profit = (reversion_prob * reward - (1 - reversion_prob) * risk) / current_price
        
        # Partial exit levels (nếu enabled)
        partial_exits = {}
        if self.risk_config['partial_exits']:
            if signal['signal'] == 'BUY':
                partial_exits = {
                    'level_1': current_price + (reward * 0.3),
                    'level_2': current_price + (reward * 0.6),
                    'level_3': take_profit
                }
            else:
                partial_exits = {
                    'level_1': current_price - (reward * 0.3),
                    'level_2': current_price - (reward * 0.6),
                    'level_3': take_profit
                }
        
        return {
            'stop_loss': stop_loss,
            'take_profit': take_profit,
            'risk_amount': risk,
            'reward_amount': reward,
            'risk_reward_ratio': risk_reward_ratio,
            'expected_profit': expected_profit,
            'reversion_probability': reversion_prob,
            'partial_exits': partial_exits if self.risk_config['partial_exits'] else None,
            'max_holding_period': self.reversion_config['max_holding_period']
        }
    
    def _calculate_reversion_position(self, signal: Dict, data: pd.DataFrame, 
                                    market_analysis: Dict) -> Dict:
        """Tính toán position size cho mean reversion"""
        account_size = market_analysis.get('account_size', 10000)
        
        position_methods = {}
        
        # 1. Volatility-based position sizing
        volatility = market_analysis.get('volatility', 0.02)
        vol_position = min(self.risk_config['max_position_pct'], 
                          0.02 / max(volatility, 0.001))
        position_methods['volatility'] = vol_position
        
        # 2. Extreme strength based
        extreme_strength = signal.get('extreme_strength', 0.5)
        strength_position = self.risk_config['max_position_pct'] * extreme_strength
        position_methods['strength'] = strength_position
        
        # 3. Z-Score based (nếu có)
        z_score = abs(signal.get('z_score', 1))
        if z_score > 1:
            z_position = min(self.risk_config['max_position_pct'], 
                           0.01 * z_score)
            position_methods['z_score'] = z_position
        
        # Kết hợp các methods
        if position_methods:
            avg_position = np.mean(list(position_methods.values()))
        else:
            avg_position = self.risk_config['max_position_pct'] * 0.5
        
        # Điều chỉnh theo market regime
        regime_factor = self._regime_settings.get(
            self._market_regime, 
            self._regime_settings['normal']
        )['position_factor']
        
        final_position = avg_position * regime_factor
        final_position = min(final_position, self.risk_config['max_position_pct'])
        
        position_value = account_size * final_position
        
        return {
            'percentage': final_position,
            'value': position_value,
            'methods': position_methods,
            'regime_factor': regime_factor
        }
    
    # ==================== CONFIDENCE CALCULATION ====================
    
    def _calculate_reversion_confidence(self, signal: Dict, extremes: Dict, 
                                      indicators: Dict) -> float:
        """Tính toán confidence cho mean reversion signal"""
        base_confidence = 0.5
        
        # 1. Extreme strength (20%)
        extreme_strength = signal.get('extreme_strength', 0.5)
        base_confidence += (extreme_strength - 0.5) * 0.2
        
        # 2. RSI position (20%)
        rsi = signal.get('rsi', 50)
        extreme_type = signal.get('extreme_type')
        
        if extreme_type == 'OVERSOLD':
            rsi_score = max(0, 1 - (rsi / 30))  # Càng thấp càng tốt
        else:  # OVERBOUGHT
            rsi_score = max(0, 1 - ((100 - rsi) / 30))  # Càng cao càng tốt
        
        base_confidence += rsi_score * 0.2
        
        # 3. Bollinger Band position (15%)
        bb_position = signal.get('bb_position', 0.5)
        if extreme_type == 'OVERSOLD':
            bb_score = max(0, 1 - bb_position * 2)  # Càng gần lower band càng tốt
        else:
            bb_score = max(0, (bb_position - 0.5) * 2)  # Càng gần upper band càng tốt
        
        base_confidence += bb_score * 0.15
        
        # 4. Multi-timeframe confirmation (15%)
        tf_score = signal.get('timeframe_confirmation', {}).get('overall_score', 0.5)
        base_confidence += (tf_score - 0.5) * 0.15
        
        # 5. Volume confirmation (10%)
        volume_signal = signal.get('volume_signal', {})
        volume_score = volume_signal.get('score', 0.5)
        base_confidence += (volume_score - 0.5) * 0.1
        
        # 6. Indicator convergence (10%)
        convergence_score = self._calculate_convergence_score(indicators, extreme_type)
        base_confidence += (convergence_score - 0.5) * 0.1
        
        # 7. Market regime adjustment (10%)
        regime = self._market_regime or 'normal'
        if regime == 'high_volatility':
            base_confidence *= 0.9  # Giảm confidence trong high volatility
        elif regime == 'low_volatility':
            base_confidence *= 1.1  # Tăng confidence trong low volatility
        
        # Đảm bảo trong khoảng [min_confidence, 0.95]
        final_confidence = max(self.min_confidence, min(0.95, base_confidence))
        
        return final_confidence
    
    def _calculate_convergence_score(self, indicators: Dict, extreme_type: str) -> float:
        """Tính convergence score của các indicators"""
        score = 0
        count = 0
        
        # RSI trend
        rsi = indicators['rsi']
        if len(rsi) >= 3:
            rsi_trend = np.polyfit(range(3), rsi[-3:], 1)[0]
            if (extreme_type == 'OVERSOLD' and rsi_trend > 0) or \
               (extreme_type == 'OVERBOUGHT' and rsi_trend < 0):
                score += 1
            count += 1
        
        # Stochastic
        if 'stochastic' in indicators:
            stoch_k, stoch_d = indicators['stochastic']
            if len(stoch_k) >= 2:
                k_trend = stoch_k[-1] - stoch_k[-2]
                if (extreme_type == 'OVERSOLD' and k_trend > 0) or \
                   (extreme_type == 'OVERBOUGHT' and k_trend < 0):
                    score += 1
                count += 1
        
        # Z-Score
        if 'z_score' in indicators:
            zscore = indicators['z_score']
            if len(zscore) >= 2:
                z_trend = zscore[-1] - zscore[-2]
                if (extreme_type == 'OVERSOLD' and z_trend > -0.5) or \
                   (extreme_type == 'OVERBOUGHT' and z_trend < 0.5):
                    score += 1
                count += 1
        
        return score / count if count > 0 else 0.5
    
    # ==================== VOLUME ANALYSIS ====================
    
    def _analyze_volume_pattern(self, data: pd.DataFrame, signal: Dict) -> Dict:
        """Phân tích volume pattern cho mean reversion"""
        if 'volume' not in data.columns:
            return {'score': 0.5, 'pattern': 'NO_DATA'}
        
        lookback = min(20, len(data))
        recent_volume = data['volume'].iloc[-lookback:].values
        recent_close = data['close'].iloc[-lookback:].values
        
        # Volume moving averages
        volume_ma5 = np.mean(recent_volume[-5:])
        volume_ma20 = np.mean(recent_volume)
        
        # Current volume
        current_volume = data['volume'].iloc[-1]
        
        # Volume ratio
        volume_ratio_5 = current_volume / volume_ma5 if volume_ma5 > 0 else 1
        volume_ratio_20 = current_volume / volume_ma20 if volume_ma20 > 0 else 1
        
        # Volume pattern
        pattern = 'NEUTRAL'
        score = 0.5
        
        if signal['signal'] == 'BUY':
            # Bullish volume: volume tăng trên oversold
            if volume_ratio_5 > 1.2 and volume_ratio_20 > 1.0:
                pattern = 'BULLISH_VOLUME'
                score = 0.7
            elif volume_ratio_5 > 1.5:
                pattern = 'STRONG_BULLISH_VOLUME'
                score = 0.8
            elif volume_ratio_5 < 0.8:
                pattern = 'LOW_VOLUME'
                score = 0.3
        else:  # SELL
            # Bearish volume: volume tăng trên overbought
            if volume_ratio_5 > 1.2 and volume_ratio_20 > 1.0:
                pattern = 'BEARISH_VOLUME'
                score = 0.7
            elif volume_ratio_5 > 1.5:
                pattern = 'STRONG_BEARISH_VOLUME'
                score = 0.8
            elif volume_ratio_5 < 0.8:
                pattern = 'LOW_VOLUME'
                score = 0.3
        
        return {
            'score': score,
            'pattern': pattern,
            'volume_ratio_5': float(volume_ratio_5),
            'volume_ratio_20': float(volume_ratio_20),
            'current_volume': float(current_volume)
        }
    
    # ==================== REVERSION QUALITY EVALUATION ====================
    
    def _evaluate_reversion_quality(self, signal: Dict, extremes: Dict, 
                                   data: pd.DataFrame) -> Dict:
        """Đánh giá chất lượng của mean reversion setup"""
        quality = {
            'overall_score': 0.0,
            'extreme_quality': 0.0,
            'confirmation_quality': 0.0,
            'risk_quality': 0.0,
            'market_context': 0.0,
            'grade': 'C'  # A, B, C, D
        }
        
        # 1. Extreme quality (30%)
        extreme_strength = signal.get('extreme_strength', 0.5)
        rsi = signal.get('rsi', 50)
        
        extreme_quality = extreme_strength * 0.7
        if signal['extreme_type'] == 'OVERSOLD':
            rsi_quality = max(0, 1 - (rsi / 30))
        else:
            rsi_quality = max(0, 1 - ((100 - rsi) / 30))
        
        extreme_quality += rsi_quality * 0.3
        quality['extreme_quality'] = extreme_quality
        
        # 2. Confirmation quality (30%)
        tf_score = signal.get('timeframe_confirmation', {}).get('overall_score', 0.5)
        volume_score = signal.get('volume_signal', {}).get('score', 0.5)
        
        confirmation_quality = tf_score * 0.6 + volume_score * 0.4
        quality['confirmation_quality'] = confirmation_quality
        
        # 3. Risk quality (25%)
        rr_ratio = signal.get('risk_reward_ratio', 1.0)
        expected_profit = signal.get('expected_profit', 0)
        
        risk_quality = min(1.0, rr_ratio / 2.0) * 0.6  # Tối đa khi RR=2.0
        risk_quality += min(1.0, expected_profit / 0.05) * 0.4  # Tối đa khi profit=5%
        quality['risk_quality'] = risk_quality
        
        # 4. Market context (15%)
        regime_score = 0.5
        if self._market_regime == 'low_volatility':
            regime_score = 0.8
        elif self._market_regime == 'normal':
            regime_score = 0.6
        elif self._market_regime == 'high_volatility':
            regime_score = 0.4
        
        volatility = self._calculate_volatility(data)
        vol_score = max(0, 1 - (volatility / 0.05))  # Tốt nhất khi volatility vừa phải
        
        market_context = regime_score * 0.7 + vol_score * 0.3
        quality['market_context'] = market_context
        
        # Overall score
        overall_score = (
            extreme_quality * 0.3 +
            confirmation_quality * 0.3 +
            risk_quality * 0.25 +
            market_context * 0.15
        )
        quality['overall_score'] = overall_score
        
        # Assign grade
        if overall_score >= 0.8:
            quality['grade'] = 'A'
        elif overall_score >= 0.7:
            quality['grade'] = 'B'
        elif overall_score >= 0.6:
            quality['grade'] = 'C'
        else:
            quality['grade'] = 'D'
        
        return quality
    
    # ==================== SIGNAL VALIDATION ====================
    
    def _validate_reversion_signal(self, signal: Dict) -> bool:
        """Validate mean reversion signal"""
        # Minimum confidence
        if signal.get('confidence', 0) < self.min_confidence:
            return False
        
        # Timeframe confirmation
        tf_score = signal.get('timeframe_confirmation', {}).get('overall_score', 0)
        if tf_score < 0.5:
            return False
        
        # Risk/Reward ratio
        rr_ratio = signal.get('risk_reward_ratio', 1.0)
        if rr_ratio < self.risk_config['risk_reward_ratio']:
            return False
        
        # Expected profit
        expected_profit = signal.get('expected_profit', 0)
        if expected_profit < 0.005:  # Ít nhất 0.5%
            return False
        
        # Reversion quality
        quality = signal.get('reversion_quality', {})
        if quality.get('grade', 'D') == 'D':
            return False
        
        # Market regime check
        if self._market_regime == 'high_volatility':
            # Yêu cầu cao hơn trong high volatility
            if signal.get('confidence', 0) < 0.75:
                return False
        
        return True
    
    # ==================== HELPER METHODS ====================
    
    def _detect_market_regime(self, market_analysis: Dict):
        """Phát hiện market regime"""
        volatility = market_analysis.get('volatility', 0.02)
        
        if volatility > 0.03:
            self._market_regime = 'high_volatility'
        elif volatility < 0.01:
            self._market_regime = 'low_volatility'
        else:
            self._market_regime = 'normal'
    
    def _calculate_price_deviation(self, market_analysis: Dict) -> float:
        """Tính độ lệch giá so với trung bình"""
        # Placeholder - trong thực tế sẽ tính từ price data
        return 1.0
    
    def _calculate_price_position(self, data: pd.DataFrame, bb: Dict) -> float:
        """Tính vị trí giá trong Bollinger Bands"""
        current_price = data['close'].iloc[-1]
        bb_lower = bb['lower'].iloc[-1]
        bb_upper = bb['upper'].iloc[-1]
        
        if bb_upper != bb_lower:
            return (current_price - bb_lower) / (bb_upper - bb_lower)
        else:
            return 0.5
    
    def _calculate_reversion_strength(self, price: float, bb: Dict, rsi: np.ndarray, 
                                    stoch: Optional[Tuple], zscore: Optional[np.ndarray]) -> float:
        """Tính strength của mean reversion setup"""
        strength = 0.0
        
        # Bollinger Band strength (40%)
        bb_position = (price - bb['lower'].iloc[-1]) / (bb['upper'].iloc[-1] - bb['lower'].iloc[-1])
        if bb_position < 0.1 or bb_position > 0.9:
            strength += 0.4
        
        # RSI strength (30%)
        current_rsi = rsi[-1] if len(rsi) > 0 else 50
        if current_rsi < 30 or current_rsi > 70:
            strength += 0.3
        
        # Stochastic strength (20%)
        if stoch is not None:
            stoch_k, stoch_d = stoch
            if len(stoch_k) > 0 and len(stoch_d) > 0:
                if stoch_k[-1] < 20 or stoch_k[-1] > 80:
                    strength += 0.2
        
        # Z-Score strength (10%)
        if zscore is not None and len(zscore) > 0:
            if abs(zscore[-1]) > 2.0:
                strength += 0.1
        
        return min(1.0, strength)
    
    def _calculate_volatility(self, data: pd.DataFrame) -> float:
        """Tính volatility"""
        if len(data) < 20:
            return 0.02
        
        returns = data['close'].pct_change().dropna()
        return float(returns.std())
    
    def _estimate_reversion_probability(self, data: pd.DataFrame, indicators: Dict) -> float:
        """Ước lượng xác suất mean reversion thành công"""
        # Base probability
        prob = 0.6
        
        # Adjust based on indicators
        bb_width = indicators['bollinger']['band_width'].iloc[-1]
        if bb_width > 0.05:  # Wide bands - higher probability
            prob += 0.1
        elif bb_width < 0.02:  # Narrow bands - lower probability
            prob -= 0.1
        
        # RSI extreme
        rsi = indicators['rsi'][-1] if len(indicators['rsi']) > 0 else 50
        if rsi < 25 or rsi > 75:
            prob += 0.1
        elif 40 < rsi < 60:
            prob -= 0.1
        
        # Volume confirmation
        if 'volume' in data.columns:
            recent_volume = data['volume'].iloc[-5:].mean()
            avg_volume = data['volume'].iloc[-20:].mean()
            if recent_volume > avg_volume * 1.2:
                prob += 0.1
        
        return min(0.9, max(0.3, prob))
    
    # ==================== PERFORMANCE TRACKING ====================
    
    def record_reversion_trade(self, profit: float, holding_period: int):
        """Ghi nhận kết quả giao dịch mean reversion"""
        if profit > 0:
            self._reversion_stats['successful_reversions'] += 1
        else:
            self._reversion_stats['failed_reversions'] += 1
        
        # Update average holding period
        total_trades = (self._reversion_stats['successful_reversions'] + 
                       self._reversion_stats['failed_reversions'])
        
        if total_trades > 1:
            current_avg = self._reversion_stats['avg_holding_period']
            new_avg = ((current_avg * (total_trades - 1)) + holding_period) / total_trades
            self._reversion_stats['avg_holding_period'] = new_avg
        else:
            self._reversion_stats['avg_holding_period'] = holding_period
        
        # Update average profit
        if total_trades > 1:
            current_avg_profit = self._reversion_stats['avg_profit']
            new_avg_profit = ((current_avg_profit * (total_trades - 1)) + profit) / total_trades
            self._reversion_stats['avg_profit'] = new_avg_profit
        else:
            self._reversion_stats['avg_profit'] = profit
    
    def get_reversion_stats(self) -> Dict:
        """Lấy thống kê mean reversion"""
        total_trades = (self._reversion_stats['successful_reversions'] + 
                       self._reversion_stats['failed_reversions'])
        
        win_rate = (self._reversion_stats['successful_reversions'] / total_trades * 100 
                   if total_trades > 0 else 0)
        
        return {
            'strategy': self.name,
            'total_reversion_trades': total_trades,
            'successful_reversions': self._reversion_stats['successful_reversions'],
            'failed_reversions': self._reversion_stats['failed_reversions'],
            'win_rate': win_rate,
            'avg_holding_period': self._reversion_stats['avg_holding_period'],
            'avg_profit': self._reversion_stats['avg_profit']
        }
    
    # ==================== UTILITY METHODS ====================
    
    def clear_cache(self):
        """Xóa cache"""
        self._indicator_cache.clear()
        self._cache_timestamp.clear()
    
    def update_config(self, new_config: Dict):
        """Cập nhật configuration"""
        for key, value in new_config.items():
            if hasattr(self, key):
                setattr(self, key, value)
            elif key in self.indicators_config:
                self.indicators_config[key] = value
            elif key in self.reversion_config:
                self.reversion_config[key] = value
            elif key in self.risk_config:
                self.risk_config[key] = value
    
    def get_strategy_stats(self) -> Dict:
        """Lấy thống kê strategy"""
        return {
            'name': self.name,
            'min_confidence': self.min_confidence,
            'market_regime': self._market_regime,
            'cache_size': len(self._indicator_cache),
            'config': {
                'timeframes': self.timeframe_config,
                'indicators': self.indicators_config,
                'reversion': self.reversion_config,
                'risk': self.risk_config
            },
            'reversion_stats': self.get_reversion_stats()
        }
from strategies.base_strategy import BaseStrategy
from indicators.market_analyzer import MarketAnalyzer

class SidewaysStrategy(BaseStrategy):
    """Chiến lược giao dịch trong vùng sideways"""
    
    def __init__(self):
        super().__init__("SIDEWAYS", min_confidence=0.65)
        self.market_analyzer = MarketAnalyzer()
    
    def is_strategy_applicable(self, market_analysis):
        """Áp dụng khi thị trường trong trạng thái sideways"""
        return market_analysis['market_state'] == 'SIDEWAYS'
    
    def generate_signal(self, current_data, market_analysis):
        """Tạo tín hiệu trong vùng sideways"""
        zones = self.market_analyzer.get_trading_zones(current_data)
        if not zones:
            return None
        
        current_price = current_data['close'].iloc[-1]
        rsi = current_data['rsi'].iloc[-1] if 'rsi' in current_data else 50
        
        signal_data = {
            'price': current_price,
            'rsi': rsi,
            'strategy': self.name
        }
        
        # Tín hiệu BUY ở vùng hỗ trợ
        if current_price <= zones['buy_zone'] and rsi < 35:
            signal_data.update({
                'signal': 'BUY',
                'rsi_extreme': True,
                'zone_position': 'support'
            })
        
        # Tín hiệu SELL ở vùng kháng cự
        elif current_price >= zones['sell_zone'] and rsi > 65:
            signal_data.update({
                'signal': 'SELL', 
                'rsi_extreme': True,
                'zone_position': 'resistance'
            })
        
        else:
            return None
        
        # Tính confidence
        signal_data['confidence'] = self.calculate_confidence(signal_data)
        
        return signal_data
from strategies.sideways_strategy import SidewaysStrategy
from strategies.trend_strategy import TrendStrategy

class StrategyFactory:
    """Factory pattern để tạo chiến lược"""
    
    @staticmethod
    def create_strategy(strategy_type):
        """Tạo chiến lược dựa trên type"""
        strategies = {
            'sideways': SidewaysStrategy,
            'trend': TrendStrategy
        }
        
        strategy_class = strategies.get(strategy_type.lower())
        if strategy_class:
            return strategy_class()
        else:
            raise ValueError(f"Unknown strategy type: {strategy_type}")
    
    @staticmethod
    def get_available_strategies():
        """Lấy danh sách chiến lược có sẵn"""
        return ['sideways', 'trend']
from strategies.base_strategy import BaseStrategy
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Import các thư viện nâng cao (optional)
try:
    import torch
    import torch.nn as nn
    from torch.nn import TransformerEncoder, TransformerEncoderLayer
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False
    
try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.preprocessing import StandardScaler
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

class TrendStrategy(BaseStrategy):
    """Chiến lược giao dịch theo xu hướng nâng cao với ML và Multi-Timeframe"""
    
    def __init__(self, timeframe_config: Optional[Dict] = None, use_ml: bool = True):
        super().__init__("TREND_ML_ENHANCED", min_confidence=0.75)
        
        # Cấu hình multi-timeframe chi tiết
        self.timeframe_config = timeframe_config or {
            'trend': ['4h', '1h'],     # Khung xu hướng
            'confirmation': ['30m'],    # Khung xác nhận
            'entry': ['15m', '5m', '1m'] # Khung vào lệnh
        }
        
        # Trọng số theo từng nhóm timeframe
        self.timeframe_weights = {
            'trend': 0.4,
            'confirmation': 0.3,
            'entry': 0.3
        }
        
        # Cấu hình indicator nâng cao
        self.indicators_config = {
            'ema': {'adaptive': True, 'min_period': 9, 'max_period': 200},
            'rsi': {'adaptive': True, 'base_period': 14},
            'ichimoku': {'enabled': True, 'tenkan': 9, 'kijun': 26, 'senkou': 52},
            'atr': {'period': 14, 'multiplier': 2.0},
            'volume_profile': {'enabled': True, 'resolution': 20}
        }
        
        # ML Configuration
        self.ml_config = {
            'enabled': use_ml and HAS_SKLEARN,
            'trend_prediction': True,
            'pattern_recognition': HAS_TORCH,
            'model_retrain_freq': 100,  # Số candle retrain model
            'feature_window': 50        # Số periods cho feature extraction
        }
        
        # Risk Management Configuration
        self.risk_config = {
            'position_sizing': 'kelly',  # kelly, volatility, fixed
            'max_position_pct': 0.05,     # 5% tối đa
            'max_drawdown_pct': 0.10,    # 10% drawdown tối đa
            'monte_carlo_sims': 1000,    # Số simulations
            'stop_loss_type': 'atr',     # atr, percentage, support_resistance
            'take_profit_ratio': 1.5     # Risk/Reward ratio
        }
        
        # Performance Optimization
        self.optimization_config = {
            'vectorized': True,
            'caching': True,
            'parallel_processing': True,
            'gpu_acceleration': HAS_TORCH and torch.cuda.is_available(),
            'cache_ttl': 300  # 5 minutes
        }
        
        # Caching system
        self._cache = {}
        self._cache_timestamps = {}
        
        # ML Models (lazy loading)
        self._trend_model = None
        self._pattern_model = None
        self._scaler = None
        self._feature_columns = None
        
        # Market regime detection
        self._market_regime = None
        self._regime_thresholds = self._initialize_regime_thresholds()
        
        # Liquidity tracking
        self._liquidity_cache = {}
        
        # Transaction cost model
        self.transaction_cost = 0.001  # 0.1% default
        
    def _initialize_regime_thresholds(self) -> Dict:
        """Khởi tạo thresholds cho các market regime"""
        return {
            'high_volatility': {
                'rsi_buy': (30, 60),
                'rsi_sell': (40, 70),
                'ema_distance_factor': 1.5,
                'position_size_factor': 0.5
            },
            'low_volatility': {
                'rsi_buy': (35, 55),
                'rsi_sell': (45, 65),
                'ema_distance_factor': 0.8,
                'position_size_factor': 0.8
            },
            'normal': {
                'rsi_buy': (32, 58),
                'rsi_sell': (42, 68),
                'ema_distance_factor': 1.0,
                'position_size_factor': 1.0
            }
        }
    
    # ==================== CORE STRATEGY METHODS ====================
    
    def is_strategy_applicable(self, market_analysis: Dict) -> bool:
        """Kiểm tra tính khả dụng với ML enhancement"""
        applicable = False
        
        # Detect market regime
        self._detect_market_regime(market_analysis)
        
        # Phân tích multi-timeframe
        if 'timeframe_analysis' in market_analysis:
            tf_analysis = market_analysis['timeframe_analysis']
            
            # Đếm số khung có xu hướng rõ ràng
            trend_count = 0
            total_count = 0
            
            for tf_type, analyses in tf_analysis.items():
                for analysis in analyses:
                    if analysis.get('market_state') in ['BULLISH', 'BEARISH']:
                        trend_count += 1
                    total_count += 1
            
            # Cần ít nhất 60% khung có xu hướng
            if total_count > 0 and (trend_count / total_count) >= 0.6:
                applicable = True
                
                # ML validation nếu enabled
                if self.ml_config['enabled']:
                    ml_confidence = self._ml_validate_trend(market_analysis)
                    if ml_confidence < 0.6:
                        applicable = False
        
        # Kiểm tra liquidity
        liquidity_score = self._calculate_liquidity_score(market_analysis)
        if liquidity_score < 0.3:  # Liquidity quá thấp
            applicable = False
            
        # Kiểm tra volatility extremes
        volatility = market_analysis.get('volatility', 0)
        if volatility > 0.05:  # Quá volatile
            applicable = False
            
        return applicable
    
    def generate_signal(self, current_data: pd.DataFrame, market_analysis: Dict) -> Optional[Dict]:
        """Generate signal với tất cả tính năng nâng cao"""
        
        # 1. Parallel processing cho multi-timeframe analysis
        tf_analysis = self._parallel_timeframe_analysis(market_analysis)
        
        # 2. ML Trend Prediction
        trend_probability = 0.5
        if self.ml_config['enabled'] and self.ml_config['trend_prediction']:
            trend_probability = self._predict_trend_continuation(tf_analysis, current_data)
        
        # 3. Pattern Recognition với Deep Learning
        pattern_signal = None
        if self.ml_config['enabled'] and self.ml_config['pattern_recognition']:
            pattern_signal = self._recognize_trading_patterns(current_data)
        
        # 4. Tạo base signal
        signal = self._generate_enhanced_signal(tf_analysis, current_data, market_analysis)
        
        if signal is None:
            return None
        
        # 5. ML Enhancement
        signal['ml_confidence'] = trend_probability
        if pattern_signal:
            signal['pattern_signal'] = pattern_signal
            signal['confidence'] *= (1 + pattern_signal.get('confidence', 0) * 0.2)
        
        # 6. Risk Management Enhancement
        risk_metrics = self._calculate_risk_metrics(signal, tf_analysis, current_data)
        signal.update(risk_metrics)
        
        # 7. Dynamic Position Sizing
        position_size = self._calculate_position_size(signal, current_data, market_analysis)
        signal['position_size'] = position_size
        
        # 8. Monte Carlo Drawdown Estimation
        drawdown_stats = self._estimate_drawdown_monte_carlo(signal, current_data)
        signal['drawdown_estimation'] = drawdown_stats
        
        # 9. Liquidity Consideration
        liquidity_metrics = self._analyze_liquidity(current_data, market_analysis)
        signal['liquidity_metrics'] = liquidity_metrics
        
        # 10. Adaptive Thresholds based on Market Regime
        self._apply_regime_adaptive_thresholds(signal)
        
        # 11. Ichimoku Cloud Integration
        ichimoku_signals = self._analyze_ichimoku_cloud(current_data)
        signal['ichimoku_signals'] = ichimoku_signals
        
        # 12. Transaction Cost Modeling
        transaction_cost = self._calculate_transaction_cost(signal, current_data)
        signal['transaction_cost'] = transaction_cost
        signal['net_expected_return'] = signal.get('expected_return', 0) - transaction_cost
        
        # Final confidence calculation với tất cả factors
        signal['confidence'] = self._calculate_enhanced_confidence(signal)
        
        # Validate final signal
        if not self._validate_final_signal(signal):
            return None
        
        return signal
    
    # ==================== MULTI-TIMEFRAME PARALLEL PROCESSING ====================
    
    def _parallel_timeframe_analysis(self, market_analysis: Dict) -> Dict:
        """Phân tích multi-timeframe với parallel processing"""
        tf_analysis = {}
        
        if 'multi_timeframe_data' in market_analysis:
            mtf_data = market_analysis['multi_timeframe_data']
            
            # Group data theo priority để parallel processing
            for tf_type, tf_list in self.timeframe_config.items():
                if tf_type not in tf_analysis:
                    tf_analysis[tf_type] = []
                
                for timeframe in tf_list:
                    if timeframe in mtf_data:
                        analysis = self._analyze_timeframe_with_cache(
                            mtf_data[timeframe], 
                            timeframe, 
                            tf_type
                        )
                        tf_analysis[tf_type].append(analysis)
        
        return tf_analysis
    
    def _analyze_timeframe_with_cache(self, data: pd.DataFrame, timeframe: str, tf_type: str) -> Dict:
        """Phân tích timeframe với caching để tối ưu performance"""
        cache_key = f"{timeframe}_{data.index[-1].timestamp()}"
        
        if (self.optimization_config['caching'] and 
            cache_key in self._cache and 
            datetime.now().timestamp() - self._cache_timestamps.get(cache_key, 0) < self.optimization_config['cache_ttl']):
            return self._cache[cache_key]
        
        # Vectorized calculations
        analysis = self._vectorized_timeframe_analysis(data, timeframe, tf_type)
        
        if self.optimization_config['caching']:
            self._cache[cache_key] = analysis
            self._cache_timestamps[cache_key] = datetime.now().timestamp()
        
        return analysis
    
    def _vectorized_timeframe_analysis(self, data: pd.DataFrame, timeframe: str, tf_type: str) -> Dict:
        """Vectorized timeframe analysis cho performance"""
        if len(data) < 50:
            return {'timeframe': timeframe, 'error': 'insufficient_data'}
        
        # Vectorized EMA calculation
        close_prices = data['close'].values
        volume = data['volume'].values if 'volume' in data.columns else np.zeros_like(close_prices)
        
        # Adaptive EMA periods
        fast_period, slow_period = self._calculate_adaptive_ema_periods(close_prices)
        
        # Vectorized EMA calculation
        ema_fast = self._vectorized_ema(close_prices, fast_period)
        ema_slow = self._vectorized_ema(close_prices, slow_period)
        
        # Vectorized RSI
        rsi_period = self._calculate_adaptive_rsi_period_vec(close_prices)
        rsi = self._vectorized_rsi(close_prices, rsi_period)
        
        # Ichimoku Cloud nếu enabled
        ichimoku = {}
        if self.indicators_config['ichimoku']['enabled']:
            ichimoku = self._calculate_ichimoku_vectorized(close_prices)
        
        # Volume Profile
        volume_profile = {}
        if self.indicators_config['volume_profile']['enabled']:
            volume_profile = self._calculate_volume_profile_vectorized(close_prices, volume)
        
        # Trend Analysis
        trend_direction = self._determine_trend_direction_vec(
            close_prices[-1], ema_fast[-1], ema_slow[-1], rsi[-1]
        )
        
        trend_strength = self._calculate_trend_strength_vec(
            ema_fast, ema_slow, close_prices
        )
        
        # Market State
        market_state = self._determine_market_state_vec(
            trend_direction, trend_strength, rsi[-1]
        )
        
        # Support Resistance
        support_resistance = self._identify_support_resistance_vec(close_prices)
        
        return {
            'timeframe': timeframe,
            'tf_type': tf_type,
            'market_state': market_state,
            'trend_direction': trend_direction,
            'trend_strength': trend_strength,
            'price': float(close_prices[-1]),
            'ema_fast': float(ema_fast[-1]),
            'ema_slow': float(ema_slow[-1]),
            'rsi': float(rsi[-1]),
            'ichimoku': ichimoku,
            'volume_profile': volume_profile,
            'support_resistance': support_resistance,
            'timestamp': data.index[-1],
            'volatility': float(self._calculate_volatility_vec(close_prices))
        }
    
    # ==================== MACHINE LEARNING METHODS ====================
    
    def _predict_trend_continuation(self, tf_analysis: Dict, current_data: pd.DataFrame) -> float:
        """Dự đoán xác suất tiếp tục xu hướng bằng ML"""
        if not self.ml_config['enabled']:
            return 0.5
        
        # Extract features
        features = self._extract_ml_features(tf_analysis, current_data)
        
        # Load or train model
        model = self._get_trend_model()
        
        if model is not None and self._scaler is not None:
            try:
                # Scale features
                features_scaled = self._scaler.transform([features])
                
                # Predict probability
                if hasattr(model, 'predict_proba'):
                    proba = model.predict_proba(features_scaled)[0]
                    # Return probability of continuation (class 1)
                    return float(proba[1])
                else:
                    prediction = model.predict(features_scaled)[0]
                    return float(prediction)
            except:
                pass
        
        return 0.5
    
    def _extract_ml_features(self, tf_analysis: Dict, current_data: pd.DataFrame) -> List[float]:
        """Trích xuất features cho ML model"""
        features = []
        
        # Price features
        close_prices = current_data['close'].values
        features.extend([
            close_prices[-1] / close_prices[-2] - 1,  # 1-period return
            close_prices[-1] / close_prices[-6] - 1,  # 5-period return
            close_prices[-1] / close_prices[-21] - 1, # 20-period return
            np.std(close_prices[-20:]) / close_prices[-1],  # Recent volatility
        ])
        
        # Multi-timeframe alignment features
        for tf_type in ['trend', 'confirmation', 'entry']:
            if tf_type in tf_analysis:
                analyses = tf_analysis[tf_type]
                if analyses:
                    # Average trend strength
                    avg_strength = np.mean([a.get('trend_strength', 0.5) for a in analyses])
                    features.append(avg_strength)
                    
                    # Direction consistency
                    directions = [a.get('trend_direction', 'NEUTRAL') for a in analyses]
                    bull_count = sum(1 for d in directions if d == 'BULLISH')
                    bear_count = sum(1 for d in directions if d == 'BEARISH')
                    direction_score = (bull_count - bear_count) / len(directions)
                    features.append(direction_score)
        
        # Technical indicator features
        if len(close_prices) >= 50:
            # RSI features
            rsi = self._vectorized_rsi(close_prices, 14)
            features.append(rsi[-1] / 100)  # Normalized RSI
            
            # EMA features
            ema20 = self._vectorized_ema(close_prices, 20)
            ema50 = self._vectorized_ema(close_prices, 50)
            features.extend([
                close_prices[-1] / ema20[-1] - 1,
                ema20[-1] / ema50[-1] - 1,
            ])
        
        # Volume features (nếu có)
        if 'volume' in current_data.columns:
            volume = current_data['volume'].values
            if len(volume) >= 20:
                features.extend([
                    volume[-1] / np.mean(volume[-20:]),
                    np.std(volume[-20:]) / np.mean(volume[-20:]),
                ])
        
        # Pad features nếu cần
        expected_features = 20  # Số features model mong đợi
        if len(features) < expected_features:
            features.extend([0.0] * (expected_features - len(features)))
        elif len(features) > expected_features:
            features = features[:expected_features]
        
        return features
    
    def _get_trend_model(self):
        """Lazy loading cho trend prediction model"""
        if self._trend_model is None and self.ml_config['enabled'] and HAS_SKLEARN:
            try:
                # Sử dụng Gradient Boosting cho trend prediction
                self._trend_model = GradientBoostingClassifier(
                    n_estimators=100,
                    learning_rate=0.1,
                    max_depth=5,
                    random_state=42
                )
                
                # Initialize scaler
                self._scaler = StandardScaler()
                
                # Model sẽ được trained online với incoming data
                # Hoặc load pre-trained model từ file
                
            except Exception as e:
                print(f"Error loading trend model: {e}")
                self._trend_model = None
        
        return self._trend_model
    
    def _recognize_trading_patterns(self, data: pd.DataFrame):
        """Nhận diện trading patterns sử dụng Deep Learning"""
        if not self.ml_config['pattern_recognition'] or not HAS_TORCH:
            return None
        
        try:
            # Prepare input data cho CNN
            price_sequence = data['close'].values[-100:]  # Last 100 periods
            volume_sequence = data['volume'].values[-100:] if 'volume' in data.columns else np.zeros_like(price_sequence)
            
            # Normalize sequences
            price_norm = (price_sequence - np.mean(price_sequence)) / np.std(price_sequence)
            volume_norm = (volume_sequence - np.mean(volume_sequence)) / np.std(volume_sequence) if np.std(volume_sequence) > 0 else volume_sequence
            
            # Stack features
            features = np.stack([price_norm, volume_norm], axis=1)
            
            # Get model prediction
            model = self._get_pattern_model()
            if model is not None:
                # Convert to tensor
                input_tensor = torch.FloatTensor(features).unsqueeze(0)
                
                # Move to GPU nếu available
                if self.optimization_config['gpu_acceleration']:
                    input_tensor = input_tensor.cuda()
                
                # Predict
                with torch.no_grad():
                    output = model(input_tensor)
                    probabilities = torch.softmax(output, dim=1)
                    
                # Pattern classes: 0=No Pattern, 1=Bullish, 2=Bearish, 3=Reversal
                pattern_idx = torch.argmax(probabilities).item()
                confidence = probabilities[0, pattern_idx].item()
                
                pattern_names = ['NO_PATTERN', 'BULLISH_PATTERN', 'BEARISH_PATTERN', 'REVERSAL_PATTERN']
                
                return {
                    'pattern': pattern_names[pattern_idx],
                    'confidence': confidence,
                    'probabilities': probabilities[0].cpu().numpy().tolist()
                }
                
        except Exception as e:
            print(f"Pattern recognition error: {e}")
        
        return None
    
    def _get_pattern_model(self):
        """Lazy loading cho pattern recognition model"""
        if self._pattern_model is None and HAS_TORCH:
            try:
                # Define simple CNN + Attention model
                class PatternCNN(nn.Module):
                    def __init__(self, input_channels=2, num_classes=4):
                        super(PatternCNN, self).__init__()
                        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)
                        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)
                        self.pool = nn.MaxPool1d(2)
                        self.dropout = nn.Dropout(0.3)
                        
                        # Attention layer
                        self.attention = nn.MultiheadAttention(64, num_heads=4, dropout=0.1)
                        
                        # Fully connected
                        self.fc1 = nn.Linear(64, 32)
                        self.fc2 = nn.Linear(32, num_classes)
                        self.relu = nn.ReLU()
                        
                    def forward(self, x):
                        # x shape: (batch, channels, sequence)
                        x = self.relu(self.conv1(x))
                        x = self.pool(x)
                        x = self.relu(self.conv2(x))
                        x = self.pool(x)
                        
                        # Apply attention
                        x_attn = x.permute(2, 0, 1)  # (seq, batch, features)
                        attn_output, _ = self.attention(x_attn, x_attn, x_attn)
                        x = attn_output.permute(1, 2, 0)
                        
                        # Global average pooling
                        x = torch.mean(x, dim=2)
                        x = self.dropout(x)
                        
                        # FC layers
                        x = self.relu(self.fc1(x))
                        x = self.fc2(x)
                        
                        return x
                
                self._pattern_model = PatternCNN()
                
                # Move to GPU nếu available
                if self.optimization_config['gpu_acceleration']:
                    self._pattern_model = self._pattern_model.cuda()
                
                # Load pre-trained weights nếu có
                # Hoặc sẽ được trained online
                
            except Exception as e:
                print(f"Error creating pattern model: {e}")
        
        return self._pattern_model
    
    # ==================== RISK MANAGEMENT & POSITION SIZING ====================
    
    def _calculate_position_size(self, signal: Dict, data: pd.DataFrame, market_analysis: Dict) -> Dict:
        """Tính toán position size dựa trên multiple methods"""
        account_size = market_analysis.get('account_size', 10000)
        volatility = market_analysis.get('volatility', 0.02)
        
        position_methods = {}
        
        # 1. Kelly Criterion
        if self.risk_config['position_sizing'] == 'kelly':
            win_rate = signal.get('ml_confidence', 0.5)
            avg_win = signal.get('expected_return', 0.02)
            avg_loss = signal.get('max_loss', 0.01)
            
            if avg_loss > 0:
                kelly_f = (win_rate * avg_win - (1 - win_rate) * avg_loss) / (avg_win * avg_loss)
                kelly_f = max(0, min(kelly_f, 0.25))  # Cap at 25%
                position_methods['kelly'] = kelly_f
        
        # 2. Volatility-based
        vol_position = min(0.1, 0.02 / max(volatility, 0.001))
        position_methods['volatility'] = vol_position
        
        # 3. Fixed Fractional
        fixed_fraction = self.risk_config['max_position_pct']
        position_methods['fixed'] = fixed_fraction
        
        # 4. Regime-based adjustment
        regime_factor = self._regime_thresholds.get(self._market_regime, {}).get('position_size_factor', 1.0)
        
        # Kết hợp methods
        final_position = np.mean(list(position_methods.values())) * regime_factor
        
        # Apply max limits
        final_position = min(
            final_position,
            self.risk_config['max_position_pct']
        )
        
        # Calculate position value
        position_value = account_size * final_position
        
        return {
            'percentage': final_position,
            'value': position_value,
            'methods': position_methods,
            'regime_factor': regime_factor
        }
    
    def _estimate_drawdown_monte_carlo(self, signal: Dict, data: pd.DataFrame) -> Dict:
        """Ước lượng drawdown sử dụng Monte Carlo simulation"""
        if self.risk_config['monte_carlo_sims'] == 0:
            return {'estimated_drawdown': 0, 'confidence_interval': (0, 0)}
        
        try:
            # Lấy historical returns
            returns = data['close'].pct_change().dropna().values
            
            if len(returns) < 20:
                return {'estimated_drawdown': 0.05, 'confidence_interval': (0.02, 0.08)}
            
            # Monte Carlo simulation
            num_simulations = min(self.risk_config['monte_carlo_sims'], 1000)
            horizon = 100  # 100 periods forward
            simulated_drawdowns = []
            
            for _ in range(num_simulations):
                # Random walk với historical distribution
                sim_returns = np.random.choice(returns, size=horizon, replace=True)
                sim_prices = np.cumprod(1 + sim_returns)
                
                # Tính max drawdown cho simulation này
                peak = np.maximum.accumulate(sim_prices)
                drawdown = (peak - sim_prices) / peak
                max_drawdown = np.max(drawdown)
                
                simulated_drawdowns.append(max_drawdown)
            
            # Thống kê
            simulated_drawdowns = np.array(simulated_drawdowns)
            
            return {
                'estimated_drawdown': float(np.percentile(simulated_drawdowns, 90)),
                'confidence_interval': (
                    float(np.percentile(simulated_drawdowns, 75)),
                    float(np.percentile(simulated_drawdowns, 95))
                ),
                'mean_drawdown': float(np.mean(simulated_drawdowns)),
                'std_drawdown': float(np.std(simulated_drawdowns))
            }
            
        except Exception as e:
            print(f"Monte Carlo simulation error: {e}")
            return {'estimated_drawdown': 0.05, 'confidence_interval': (0.03, 0.07)}
    
    def _calculate_risk_metrics(self, signal: Dict, tf_analysis: Dict, data: pd.DataFrame) -> Dict:
        """Tính toán risk metrics chi tiết"""
        current_price = signal['price']
        
        # Tính ATR cho stop loss
        atr = self._calculate_atr(data)
        
        # Xác định stop loss level
        if self.risk_config['stop_loss_type'] == 'atr':
            if signal['signal'] == 'BUY':
                stop_loss = current_price - atr * 2
            else:
                stop_loss = current_price + atr * 2
        else:  # percentage
            stop_loss_pct = 0.02  # 2%
            if signal['signal'] == 'BUY':
                stop_loss = current_price * (1 - stop_loss_pct)
            else:
                stop_loss = current_price * (1 + stop_loss_pct)
        
        # Take profit dựa trên risk/reward ratio
        risk = abs(current_price - stop_loss)
        reward = risk * self.risk_config['take_profit_ratio']
        
        if signal['signal'] == 'BUY':
            take_profit = current_price + reward
        else:
            take_profit = current_price - reward
        
        # Risk/Reward Ratio
        risk_reward_ratio = self.risk_config['take_profit_ratio']
        
        # Expected Return (simplified)
        win_probability = signal.get('ml_confidence', 0.5)
        expected_return = (win_probability * reward - (1 - win_probability) * risk) / current_price
        
        return {
            'stop_loss': stop_loss,
            'take_profit': take_profit,
            'risk_amount': risk,
            'reward_amount': reward,
            'risk_reward_ratio': risk_reward_ratio,
            'expected_return': expected_return,
            'atr': atr,
            'max_loss_pct': risk / current_price
        }
    
    # ==================== LIQUIDITY ANALYSIS ====================
    
    def _calculate_liquidity_score(self, market_analysis: Dict) -> float:
        """Tính toán liquidity score"""
        liquidity_data = market_analysis.get('liquidity_data', {})
        
        if not liquidity_data:
            return 0.5  # Default medium liquidity
        
        scores = []
        
        # Volume-based liquidity
        if 'volume_24h' in liquidity_data:
            volume = liquidity_data['volume_24h']
            # Normalize volume score (adjust thresholds based on asset)
            volume_score = min(1.0, volume / 1000000)  # 1M volume = max score
            scores.append(volume_score * 0.4)
        
        # Bid-Ask spread
        if 'bid_ask_spread' in liquidity_data:
            spread = liquidity_data['bid_ask_spread']
            spread_score = max(0, 1 - (spread * 100))  # 1% spread = 0 score
            scores.append(spread_score * 0.3)
        
        # Order book depth
        if 'order_book_depth' in liquidity_data:
            depth = liquidity_data['order_book_depth']
            depth_score = min(1.0, depth / 100000)  # 100k depth = max score
            scores.append(depth_score * 0.3)
        
        return float(np.mean(scores)) if scores else 0.5
    
    def _analyze_liquidity(self, data: pd.DataFrame, market_analysis: Dict) -> Dict:
        """Phân tích liquidity chi tiết"""
        liquidity = {
            'score': self._calculate_liquidity_score(market_analysis),
            'slippage_estimation': 0.0,
            'optimal_order_size': 0.0
        }
        
        # Estimate slippage
        liquidity_score = liquidity['score']
        if liquidity_score > 0.8:
            slippage = 0.0005  # 0.05%
        elif liquidity_score > 0.5:
            slippage = 0.001   # 0.1%
        elif liquidity_score > 0.3:
            slippage = 0.002   # 0.2%
        else:
            slippage = 0.005   # 0.5%
        
        liquidity['slippage_estimation'] = slippage
        
        # Calculate optimal order size (simplified)
        if 'volume' in data.columns:
            avg_volume = data['volume'].mean()
            optimal_size = avg_volume * 0.01  # 1% of average volume
            liquidity['optimal_order_size'] = optimal_size
        
        return liquidity
    
    # ==================== MARKET REGIME DETECTION ====================
    
    def _detect_market_regime(self, market_analysis: Dict):
        """Phát hiện market regime hiện tại"""
        volatility = market_analysis.get('volatility', 0.02)
        trend_strength = market_analysis.get('avg_trend_strength', 0.5)
        
        if volatility > 0.03:
            self._market_regime = 'high_volatility'
        elif volatility < 0.01:
            self._market_regime = 'low_volatility'
        else:
            self._market_regime = 'normal'
        
        return self._market_regime
    
    def _apply_regime_adaptive_thresholds(self, signal: Dict):
        """Áp dụng adaptive thresholds dựa trên market regime"""
        if self._market_regime not in self._regime_thresholds:
            return
        
        thresholds = self._regime_thresholds[self._market_regime]
        
        # Adjust signal thresholds
        if 'rsi' in signal:
            rsi = signal['rsi']
            
            if signal['signal'] == 'BUY':
                rsi_min, rsi_max = thresholds['rsi_buy']
                if not (rsi_min < rsi < rsi_max):
                    signal['confidence'] *= 0.7  # Reduce confidence
            else:  # SELL
                rsi_min, rsi_max = thresholds['rsi_sell']
                if not (rsi_min < rsi < rsi_max):
                    signal['confidence'] *= 0.7
        
        # Adjust position sizing factor
        if 'position_size' in signal:
            position_size = signal['position_size']
            if isinstance(position_size, dict):
                position_size['regime_factor'] = thresholds['position_size_factor']
    
    # ==================== ICHIMOKU CLOUD INTEGRATION ====================
    
    def _analyze_ichimoku_cloud(self, data: pd.DataFrame) -> Dict:
        """Phân tích Ichimoku Cloud signals"""
        if not self.indicators_config['ichimoku']['enabled']:
            return {'enabled': False}
        
        try:
            close_prices = data['close'].values
            
            # Tính Ichimoku components
            ichimoku = self._calculate_ichimoku_vectorized(close_prices)
            
            # Current price position
            current_price = close_prices[-1]
            tenkan = ichimoku['tenkan'][-1]
            kijun = ichimoku['kijun'][-1]
            senkou_a = ichimoku['senkou_a'][-26] if len(ichimoku['senkou_a']) > 26 else ichimoku['senkou_a'][-1]
            senkou_b = ichimoku['senkou_b'][-26] if len(ichimoku['senkou_b']) > 26 else ichimoku['senkou_b'][-1]
            
            # Cloud analysis
            cloud_top = max(senkou_a, senkou_b)
            cloud_bottom = min(senkou_a, senkou_b)
            
            in_cloud = cloud_bottom <= current_price <= cloud_top
            above_cloud = current_price > cloud_top
            below_cloud = current_price < cloud_bottom
            
            # Signal generation
            signals = []
            
            # Tenkan/Kijun crossover
            if tenkan > kijun and ichimoku['tenkan'][-2] <= ichimoku['kijun'][-2]:
                signals.append('TK_CROSSOVER_BULLISH')
            elif tenkan < kijun and ichimoku['tenkan'][-2] >= ichimoku['kijun'][-2]:
                signals.append('TK_CROSSOVER_BEARISH')
            
            # Price relative to cloud
            if above_cloud:
                signals.append('PRICE_ABOVE_CLOUD')
            elif below_cloud:
                signals.append('PRICE_BELOW_CLOUD')
            elif in_cloud:
                signals.append('PRICE_IN_CLOUD')
            
            # Cloud color
            cloud_color = 'GREEN' if senkou_a > senkou_b else 'RED'
            signals.append(f'CLOUD_{cloud_color}')
            
            # Chikou span analysis
            chikou = ichimoku['chikou'][-26] if len(ichimoku['chikou']) > 26 else ichimoku['chikou'][-1]
            if chikou > current_price:
                signals.append('CHIKOU_BULLISH')
            elif chikou < current_price:
                signals.append('CHIKOU_BEARISH')
            
            return {
                'enabled': True,
                'signals': signals,
                'tenkan': float(tenkan),
                'kijun': float(kijun),
                'senkou_a': float(senkou_a),
                'senkou_b': float(senkou_b),
                'cloud_top': float(cloud_top),
                'cloud_bottom': float(cloud_bottom),
                'cloud_color': cloud_color,
                'in_cloud': in_cloud,
                'above_cloud': above_cloud,
                'below_cloud': below_cloud
            }
            
        except Exception as e:
            return {'enabled': False, 'error': str(e)}
    
    # ==================== TRANSACTION COST MODELING ====================
    
    def _calculate_transaction_cost(self, signal: Dict, data: pd.DataFrame) -> float:
        """Tính toán transaction cost chi tiết"""
        base_cost = self.transaction_cost
        
        # Add slippage từ liquidity analysis
        slippage = signal.get('liquidity_metrics', {}).get('slippage_estimation', 0.001)
        
        # Market impact cost (simplified)
        position_size = signal.get('position_size', {}).get('value', 0)
        if position_size > 0:
            # Giả sử 0.01% impact cho mỗi 10k USD
            impact_cost = min(0.001, (position_size / 10000) * 0.0001)
        else:
            impact_cost = 0.0005  # Default
        
        # Total cost
        total_cost = base_cost + slippage + impact_cost
        
        # Round trip cost (entry + exit)
        round_trip_cost = total_cost * 2
        
        return round_trip_cost
    
    # ==================== VECTORIZED CALCULATION METHODS ====================
    
    def _vectorized_ema(self, prices: np.ndarray, period: int) -> np.ndarray:
        """Vectorized EMA calculation"""
        if len(prices) < period:
            return np.full_like(prices, prices[0])
        
        alpha = 2 / (period + 1)
        ema = np.zeros_like(prices)
        ema[0] = prices[0]
        
        for i in range(1, len(prices)):
            ema[i] = alpha * prices[i] + (1 - alpha) * ema[i-1]
        
        return ema
    
    def _vectorized_rsi(self, prices: np.ndarray, period: int = 14) -> np.ndarray:
        """Vectorized RSI calculation"""
        if len(prices) < period + 1:
            return np.full_like(prices, 50.0)
        
        deltas = np.diff(prices)
        seed = deltas[:period + 1]
        
        up = seed[seed >= 0].sum() / period
        down = -seed[seed < 0].sum() / period
        
        rs = up / down if down != 0 else 1.0
        rsi = np.zeros_like(prices)
        rsi[:period] = 50.0
        
        for i in range(period, len(prices)):
            delta = deltas[i - 1]
            
            if delta > 0:
                upval = delta
                downval = 0.0
            else:
                upval = 0.0
                downval = -delta
            
            up = (up * (period - 1) + upval) / period
            down = (down * (period - 1) + downval) / period
            
            rs = up / down if down != 0 else 1.0
            rsi[i] = 100.0 - 100.0 / (1.0 + rs)
        
        return rsi
    
    def _calculate_ichimoku_vectorized(self, prices: np.ndarray) -> Dict:
        """Vectorized Ichimoku Cloud calculation"""
        tenkan_period = self.indicators_config['ichimoku']['tenkan']
        kijun_period = self.indicators_config['ichimoku']['kijun']
        senkou_period = self.indicators_config['ichimoku']['senkou']
        
        # Tenkan-sen (Conversion Line)
        tenkan_high = np.maximum.accumulate(prices[-tenkan_period:])
        tenkan_low = np.minimum.accumulate(prices[-tenkan_period:])
        tenkan = (tenkan_high + tenkan_low) / 2
        
        # Kijun-sen (Base Line)
        kijun_high = np.maximum.accumulate(prices[-kijun_period:])
        kijun_low = np.minimum.accumulate(prices[-kijun_period:])
        kijun = (kijun_high + kijun_low) / 2
        
        # Senkou Span A (Leading Span A)
        senkou_a = ((tenkan + kijun) / 2) if len(tenkan) == len(kijun) else np.zeros_like(tenkan)
        
        # Senkou Span B (Leading Span B)
        senkou_b_high = np.maximum.accumulate(prices[-senkou_period:])
        senkou_b_low = np.minimum.accumulate(prices[-senkou_period:])
        senkou_b = (senkou_b_high + senkou_b_low) / 2
        
        # Chikou Span (Lagging Span)
        chikou = prices
        
        return {
            'tenkan': tenkan,
            'kijun': kijun,
            'senkou_a': senkou_a,
            'senkou_b': senkou_b,
            'chikou': chikou
        }
    
    def _calculate_volume_profile_vectorized(self, prices: np.ndarray, volumes: np.ndarray) -> Dict:
        """Vectorized Volume Profile calculation"""
        if len(prices) == 0 or len(volumes) == 0:
            return {}
        
        # Tìm price range
        price_min = np.min(prices)
        price_max = np.max(prices)
        
        # Tạo bins
        resolution = self.indicators_config['volume_profile']['resolution']
        bins = np.linspace(price_min, price_max, resolution + 1)
        
        # Phân bổ volume vào bins
        digitized = np.digitize(prices, bins)
        volume_profile = np.zeros(resolution)
        
        for i in range(len(prices)):
            if 0 < digitized[i] <= resolution:
                volume_profile[digitized[i] - 1] += volumes[i]
        
        # Tìm Point of Control (POC)
        poc_idx = np.argmax(volume_profile)
        poc_price = (bins[poc_idx] + bins[poc_idx + 1]) / 2
        
        # Value Area (70% volume)
        total_volume = np.sum(volume_profile)
        sorted_indices = np.argsort(volume_profile)[::-1]
        
        cumulative_volume = 0
        value_area_indices = []
        
        for idx in sorted_indices:
            cumulative_volume += volume_profile[idx]
            value_area_indices.append(idx)
            if cumulative_volume >= total_volume * 0.7:
                break
        
        value_area_prices = [(bins[i], bins[i + 1]) for i in value_area_indices]
        
        return {
            'poc_price': float(poc_price),
            'poc_volume': float(volume_profile[poc_idx]),
            'value_area': value_area_prices,
            'total_volume': float(total_volume),
            'profile': volume_profile.tolist(),
            'bins': bins.tolist()
        }
    
    def _calculate_atr(self, data: pd.DataFrame, period: int = 14) -> float:
        """Tính Average True Range"""
        if len(data) < period + 1:
            return 0.0
        
        high = data['high'].values
        low = data['low'].values
        close = data['close'].values
        
        # Vectorized ATR calculation
        tr = np.zeros(len(high) - 1)
        for i in range(1, len(high)):
            hl = high[i] - low[i]
            hc = abs(high[i] - close[i-1])
            lc = abs(low[i] - close[i-1])
            tr[i-1] = max(hl, hc, lc)
        
        atr = np.mean(tr[-period:]) if len(tr) >= period else np.mean(tr)
        
        return float(atr)
    
    # ==================== ENHANCED CONFIDENCE CALCULATION ====================
    
    def _calculate_enhanced_confidence(self, signal: Dict) -> float:
        """Tính confidence với tất cả enhancement factors"""
        base_confidence = signal.get('confidence', 0.5)
        
        # ML confidence boost
        ml_confidence = signal.get('ml_confidence', 0.5)
        base_confidence = base_confidence * 0.7 + ml_confidence * 0.3
        
        # Pattern recognition boost
        if 'pattern_signal' in signal:
            pattern_conf = signal['pattern_signal'].get('confidence', 0.5)
            if signal['pattern_signal'].get('pattern') != 'NO_PATTERN':
                base_confidence = base_confidence * 0.8 + pattern_conf * 0.2
        
        # Multi-timeframe alignment boost
        alignment_score = signal.get('alignment_score', 0.5)
        base_confidence = base_confidence * 0.9 + alignment_score * 0.1
        
        # Risk/Reward ratio adjustment
        rr_ratio = signal.get('risk_reward_ratio', 1.0)
        if rr_ratio >= 2.0:
            base_confidence *= 1.1
        elif rr_ratio <= 0.5:
            base_confidence *= 0.8
        
        # Liquidity adjustment
        liquidity_score = signal.get('liquidity_metrics', {}).get('score', 0.5)
        if liquidity_score < 0.3:
            base_confidence *= 0.7
        elif liquidity_score > 0.7:
            base_confidence *= 1.1
        
        # Ichimoku signals adjustment
        ichimoku_signals = signal.get('ichimoku_signals', {}).get('signals', [])
        bullish_signals = sum(1 for s in ichimoku_signals if 'BULLISH' in s)
        bearish_signals = sum(1 for s in ichimoku_signals if 'BEARISH' in s)
        
        if signal['signal'] == 'BUY' and bullish_signals > bearish_signals:
            base_confidence *= 1.05
        elif signal['signal'] == 'SELL' and bearish_signals > bullish_signals:
            base_confidence *= 1.05
        
        # Ensure within bounds
        final_confidence = max(self.min_confidence, min(0.95, base_confidence))
        
        return final_confidence
    
    def _validate_final_signal(self, signal: Dict) -> bool:
        """Validate final signal với tất cả criteria"""
        # Minimum confidence
        if signal.get('confidence', 0) < self.min_confidence:
            return False
        
        # Risk/Reward ratio
        rr_ratio = signal.get('risk_reward_ratio', 1.0)
        if rr_ratio < 0.8:  # Minimum acceptable ratio
            return False
        
        # Expected return after costs
        net_return = signal.get('net_expected_return', 0)
        if net_return < 0.005:  # Minimum 0.5% expected return
            return False
        
        # Drawdown risk
        drawdown_est = signal.get('drawdown_estimation', {}).get('estimated_drawdown', 0.1)
        if drawdown_est > self.risk_config['max_drawdown_pct']:
            return False
        
        # Liquidity
        liquidity_score = signal.get('liquidity_metrics', {}).get('score', 0.5)
        if liquidity_score < 0.2:
            return False
        
        # Transaction cost impact
        transaction_cost = signal.get('transaction_cost', 0.02)
        if transaction_cost > 0.02:  # 2% maximum
            return False
        
        return True
    
    # ==================== HELPER METHODS ====================
    
    def _calculate_adaptive_ema_periods(self, prices: np.ndarray) -> Tuple[int, int]:
        """Tính toán adaptive EMA periods"""
        volatility = np.std(prices[-20:]) / np.mean(prices[-20:]) if len(prices) >= 20 else 0.02
        
        if volatility > 0.025:  # High volatility
            fast, slow = 13, 34
        elif volatility < 0.01:  # Low volatility
            fast, slow = 21, 55
        else:  # Normal
            fast, slow = 18, 50
        
        # Ensure within config bounds
        fast = max(self.indicators_config['ema']['min_period'], 
                  min(self.indicators_config['ema']['max_period'], fast))
        slow = max(self.indicators_config['ema']['min_period'], 
                  min(self.indicators_config['ema']['max_period'], slow))
        
        return fast, slow
    
    def _calculate_adaptive_rsi_period_vec(self, prices: np.ndarray) -> int:
        """Vectorized adaptive RSI period calculation"""
        if len(prices) < 30:
            return 14
        
        volatility = np.std(prices[-20:]) / np.mean(prices[-20:])
        
        if volatility > 0.02:
            return 10
        elif volatility < 0.005:
            return 20
        else:
            return 14
    
    def _calculate_volatility_vec(self, prices: np.ndarray) -> float:
        """Vectorized volatility calculation"""
        if len(prices) < 20:
            return 0.02
        
        returns = np.diff(prices) / prices[:-1]
        return float(np.std(returns))
    
    def _determine_trend_direction_vec(self, price: float, ema_fast: float, ema_slow: float, rsi: float) -> str:
        """Vectorized trend direction determination"""
        bullish_score = 0
        bearish_score = 0
        
        if price > ema_fast:
            bullish_score += 1
        else:
            bearish_score += 1
        
        if ema_fast > ema_slow:
            bullish_score += 1
        else:
            bearish_score += 1
        
        if 45 < rsi < 70:
            bullish_score += 1
        elif 30 < rsi < 55:
            bearish_score += 1
        
        if bullish_score >= 2:
            return 'BULLISH'
        elif bearish_score >= 2:
            return 'BEARISH'
        else:
            return 'NEUTRAL'
    
    def _calculate_trend_strength_vec(self, ema_fast: np.ndarray, ema_slow: np.ndarray, prices: np.ndarray) -> float:
        """Vectorized trend strength calculation"""
        if len(ema_fast) < 20 or len(ema_slow) < 20:
            return 0.5
        
        # Slope of EMAs
        def calculate_slope(arr):
            x = np.arange(len(arr))
            slope, _ = np.polyfit(x, arr, 1)
            return slope / np.mean(arr) if np.mean(arr) != 0 else 0
        
        fast_slope = abs(calculate_slope(ema_fast[-20:]))
        slow_slope = abs(calculate_slope(ema_slow[-20:]))
        
        # Distance between EMAs
        ema_distance = abs(ema_fast[-1] - ema_slow[-1]) / prices[-1]
        
        # Price consistency
        recent_prices = prices[-5:]
        recent_fast = ema_fast[-5:]
        recent_slow = ema_slow[-5:]
        
        positions = []
        for i in range(len(recent_prices)):
            if recent_prices[i] > recent_fast[i] > recent_slow[i]:
                positions.append(1.0)
            elif recent_prices[i] < recent_fast[i] < recent_slow[i]:
                positions.append(-1.0)
            elif recent_prices[i] > recent_fast[i]:
                positions.append(0.5)
            elif recent_prices[i] < recent_fast[i]:
                positions.append(-0.5)
            else:
                positions.append(0.0)
        
        consistency = abs(np.mean(positions))
        
        # Combined strength
        strength = (fast_slope * 0.4 + 
                   slow_slope * 0.3 + 
                   ema_distance * 100 * 0.2 + 
                   consistency * 0.1)
        
        return min(1.0, max(0.0, strength))
    
    def _determine_market_state_vec(self, trend_direction: str, trend_strength: float, rsi: float) -> str:
        """Vectorized market state determination"""
        if trend_direction == 'BULLISH' and trend_strength > 0.6:
            return 'BULLISH' if rsi < 70 else 'OVERBOUGHT'
        elif trend_direction == 'BEARISH' and trend_strength > 0.6:
            return 'BEARISH' if rsi > 30 else 'OVERSOLD'
        elif trend_strength < 0.3:
            return 'RANGING'
        else:
            return 'NEUTRAL'
    
    def _identify_support_resistance_vec(self, prices: np.ndarray, window: int = 20) -> Dict:
        """Vectorized support/resistance identification"""
        if len(prices) < window * 2:
            return {'support': prices[-1] * 0.95, 'resistance': prices[-1] * 1.05}
        
        # Tìm local minima và maxima
        from scipy.signal import argrelextrema
        
        # Local minima (support)
        minima_indices = argrelextrema(prices[-window*2:], np.less, order=window//4)[0]
        if len(minima_indices) > 0:
            support = np.mean(prices[-window*2:][minima_indices[-min(3, len(minima_indices)):]])
        else:
            support = np.min(prices[-window:])
        
        # Local maxima (resistance)
        maxima_indices = argrelextrema(prices[-window*2:], np.greater, order=window//4)[0]
        if len(maxima_indices) > 0:
            resistance = np.mean(prices[-window*2:][maxima_indices[-min(3, len(maxima_indices)):]])
        else:
            resistance = np.max(prices[-window:])
        
        return {
            'support': float(support),
            'resistance': float(resistance),
            'current_position': float((prices[-1] - support) / (resistance - support)) if resistance != support else 0.5
        }
    
    def _generate_enhanced_signal(self, tf_analysis: Dict, current_data: pd.DataFrame, market_analysis: Dict) -> Optional[Dict]:
        """Generate enhanced signal với tất cả timeframe analysis"""
        # Lấy phân tích từ entry timeframes (M15, M5, M1)
        entry_analyses = tf_analysis.get('entry', [])
        
        if not entry_analyses:
            return None
        
        # Tìm best entry timeframe
        best_entry = self._find_best_entry_timeframe(entry_analyses)
        
        if best_entry is None:
            return None
        
        # Kiểm tra alignment với trend timeframes
        trend_analyses = tf_analysis.get('trend', [])
        confirmation_analyses = tf_analysis.get('confirmation', [])
        
        alignment_score = self._calculate_multi_timeframe_alignment(
            trend_analyses, confirmation_analyses, [best_entry]
        )
        
        # Tạo base signal
        signal = {
            'signal': 'BUY' if best_entry['trend_direction'] == 'BULLISH' else 'SELL',
            'price': best_entry['price'],
            'rsi': best_entry['rsi'],
            'trend_direction': best_entry['trend_direction'],
            'trend_strength': best_entry['trend_strength'],
            'timeframe': best_entry['timeframe'],
            'alignment_score': alignment_score,
            'entry_quality': self._evaluate_entry_quality(best_entry),
            'timestamp': datetime.now(),
            'multi_timeframe_analysis': {
                'trend': trend_analyses,
                'confirmation': confirmation_analyses,
                'entry': entry_analyses
            }
        }
        
        return signal
    
    def _find_best_entry_timeframe(self, entry_analyses: List[Dict]) -> Optional[Dict]:
        """Tìm best entry timeframe trong các khung M15, M5, M1"""
        if not entry_analyses:
            return None
        
        best_score = -1
        best_analysis = None
        
        for analysis in entry_analyses:
            score = self._calculate_entry_score(analysis)
            if score > best_score:
                best_score = score
                best_analysis = analysis
        
        # Minimum score threshold
        if best_score < 0.6:
            return None
        
        return best_analysis
    
    def _calculate_entry_score(self, analysis: Dict) -> float:
        """Tính điểm cho entry timeframe"""
        score = 0.0
        
        # Trend strength contribution
        trend_strength = analysis.get('trend_strength', 0.5)
        score += trend_strength * 0.3
        
        # RSI position
        rsi = analysis.get('rsi', 50)
        trend_direction = analysis.get('trend_direction', 'NEUTRAL')
        
        if trend_direction == 'BULLISH':
            rsi_score = 1.0 - abs(rsi - 45) / 45  # Optimal around 45
        else:  # BEARISH
            rsi_score = 1.0 - abs(rsi - 55) / 45  # Optimal around 55
        score += max(0, rsi_score) * 0.3
        
        # Price position relative to EMAs
        price = analysis.get('price', 0)
        ema_fast = analysis.get('ema_fast', price)
        
        if trend_direction == 'BULLISH':
            ema_score = 1.0 if price > ema_fast else 0.5
        else:
            ema_score = 1.0 if price < ema_fast else 0.5
        score += ema_score * 0.2
        
        # Volatility consideration
        volatility = analysis.get('volatility', 0.02)
        vol_score = 1.0 - min(1.0, volatility / 0.05)  # Lower volatility better
        score += vol_score * 0.2
        
        return min(1.0, max(0.0, score))
    
    def _calculate_multi_timeframe_alignment(self, trend_analyses: List[Dict], 
                                           confirmation_analyses: List[Dict], 
                                           entry_analyses: List[Dict]) -> float:
        """Tính toán alignment score giữa các nhóm timeframe"""
        all_analyses = trend_analyses + confirmation_analyses + entry_analyses
        
        if not all_analyses:
            return 0.5
        
        # Check trend direction consistency
        directions = [a.get('trend_direction', 'NEUTRAL') for a in all_analyses]
        
        if len(set(directions)) == 1:  # All same direction
            direction_score = 1.0
        elif (directions.count('BULLISH') > len(directions) * 0.7 or 
              directions.count('BEARISH') > len(directions) * 0.7):
            direction_score = 0.8
        else:
            direction_score = 0.3
        
        # Check trend strength consistency
        strengths = [a.get('trend_strength', 0.5) for a in all_analyses]
        strength_std = np.std(strengths) if len(strengths) > 1 else 0.5
        strength_score = 1.0 - min(1.0, strength_std * 2)
        
        # Weighted score
        alignment_score = direction_score * 0.6 + strength_score * 0.4
        
        return float(alignment_score)
    
    def _evaluate_entry_quality(self, entry_analysis: Dict) -> Dict:
        """Đánh giá chất lượng entry point"""
        quality = {
            'score': 0.0,
            'momentum': 'NEUTRAL',
            'volatility_state': 'NORMAL',
            'risk_level': 'MEDIUM'
        }
        
        score = 0.0
        
        # RSI momentum
        rsi = entry_analysis.get('rsi', 50)
        if 40 < rsi < 60:
            score += 0.3
            quality['momentum'] = 'BALANCED'
        elif (entry_analysis.get('trend_direction') == 'BULLISH' and rsi < 50) or \
             (entry_analysis.get('trend_direction') == 'BEARISH' and rsi > 50):
            score += 0.4
            quality['momentum'] = 'FAVORABLE'
        else:
            score += 0.2
            quality['momentum'] = 'UNFAVORABLE'
        
        # Volatility state
        volatility = entry_analysis.get('volatility', 0.02)
        if volatility < 0.01:
            score += 0.2
            quality['volatility_state'] = 'LOW'
        elif volatility < 0.03:
            score += 0.3
            quality['volatility_state'] = 'NORMAL'
        else:
            score += 0.1
            quality['volatility_state'] = 'HIGH'
        
        # Price position
        price = entry_analysis.get('price', 0)
        ema_fast = entry_analysis.get('ema_fast', price)
        ema_slow = entry_analysis.get('ema_slow', price)
        
        if entry_analysis.get('trend_direction') == 'BULLISH':
            if price > ema_fast > ema_slow:
                score += 0.3
            elif price > ema_fast:
                score += 0.2
            else:
                score += 0.1
        else:  # BEARISH
            if price < ema_fast < ema_slow:
                score += 0.3
            elif price < ema_fast:
                score += 0.2
            else:
                score += 0.1
        
        # Support/Resistance position
        sr = entry_analysis.get('support_resistance', {})
        if 'current_position' in sr:
            pos = sr['current_position']
            if pos < 0.3 or pos > 0.7:  # Near support or resistance
                score += 0.2
            else:
                score += 0.1
        
        quality['score'] = min(1.0, score)
        
        # Determine risk level
        if quality['score'] > 0.7:
            quality['risk_level'] = 'LOW'
        elif quality['score'] > 0.4:
            quality['risk_level'] = 'MEDIUM'
        else:
            quality['risk_level'] = 'HIGH'
        
        return quality
    
    def _ml_validate_trend(self, market_analysis: Dict) -> float:
        """ML validation cho trend analysis"""
        # Placeholder for ML trend validation
        # Trong thực tế, sẽ sử dụng model để validate trend
        return 0.7  # Default confidence
    
    # ==================== BACKTESTING & OPTIMIZATION SUPPORT ====================
    
    def prepare_for_backtesting(self):
        """Chuẩn bị strategy cho backtesting"""
        # Reset cache
        self._cache.clear()
        self._cache_timestamps.clear()
        
        # Disable ML trong backtest mode nếu cần
        if hasattr(self, 'backtest_mode') and self.backtest_mode:
            self.ml_config['enabled'] = False
        
        # Enable vectorized operations
        self.optimization_config['vectorized'] = True
        self.optimization_config['caching'] = False  # Disable cache trong backtest
        
    def walk_forward_optimization(self, historical_data: Dict, 
                                 optimization_params: Dict) -> Dict:
        """Walk-forward optimization cho strategy parameters"""
        # Phân chia data thành in-sample và out-of-sample
        total_points = len(historical_data)
        in_sample_size = int(total_points * 0.7)
        
        in_sample_data = historical_data[:in_sample_size]
        out_sample_data = historical_data[in_sample_size:]
        
        results = {
            'in_sample': {},
            'out_of_sample': {},
            'best_params': {},
            'stability_score': 0.0
        }
        
        # Placeholder for actual optimization logic
        # Trong thực tế, sẽ thử các parameter combinations
        
        return results
    
    def analyze_parameter_stability(self, backtest_results: List[Dict]) -> Dict:
        """Phân tích parameter stability"""
        if not backtest_results:
            return {'stability_score': 0.0, 'stable_parameters': []}
        
        # Tính toán stability metrics
        parameter_values = {}
        for result in backtest_results:
            params = result.get('parameters', {})
            for key, value in params.items():
                if key not in parameter_values:
                    parameter_values[key] = []
                parameter_values[key].append(value)
        
        # Tính độ ổn định cho từng parameter
        stability_scores = {}
        for param, values in parameter_values.items():
            if len(values) > 1:
                std_dev = np.std(values)
                mean_val = np.mean(values)
                if mean_val != 0:
                    cv = std_dev / mean_val  # Coefficient of variation
                    stability_scores[param] = 1.0 / (1.0 + cv)
                else:
                    stability_scores[param] = 1.0 if std_dev == 0 else 0.0
        
        # Overall stability score
        overall_stability = np.mean(list(stability_scores.values())) if stability_scores else 0.0
        
        return {
            'stability_score': float(overall_stability),
            'parameter_stability': stability_scores,
            'stable_parameters': [p for p, s in stability_scores.items() if s > 0.8]
        }
    
    # ==================== UTILITY METHODS ====================
    
    def clear_cache(self):
        """Xóa cache"""
        self._cache.clear()
        self._cache_timestamps.clear()
    
    def update_config(self, new_config: Dict):
        """Cập nhật configuration"""
        for key, value in new_config.items():
            if hasattr(self, key):
                setattr(self, key, value)
            elif key in self.indicators_config:
                self.indicators_config[key] = value
            elif key in self.ml_config:
                self.ml_config[key] = value
            elif key in self.risk_config:
                self.risk_config[key] = value
            elif key in self.optimization_config:
                self.optimization_config[key] = value
    
    def get_strategy_stats(self) -> Dict:
        """Lấy thống kê về strategy performance"""
        return {
            'name': self.name,
            'min_confidence': self.min_confidence,
            'market_regime': self._market_regime,
            'cache_size': len(self._cache),
            'ml_enabled': self.ml_config['enabled'],
            'gpu_acceleration': self.optimization_config['gpu_acceleration'],
            'config': {
                'timeframes': self.timeframe_config,
                'indicators': self.indicators_config,
                'risk': self.risk_config
            }
        }

utils\


from .logger import TradingLogger
from .helpers import TradingHelpers

__all__ = [
    'TradingLogger',
    'TradingHelpers'
]
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

class TradingHelpers:
    """Các helper functions cho trading"""
    
    @staticmethod
    def calculate_pip_distance(price1, price2, symbol="XAUUSD"):
        """Tính khoảng cách pip"""
        if "XAU" in symbol:
            return abs(price1 - price2) / 0.01
        else:
            return abs(price1 - price2) / 0.0001
    
    @staticmethod
    def format_currency(amount):
        """Định dạng tiền tệ"""
        return f"${amount:,.2f}"
import logging
import sys
from datetime import datetime

class TradingLogger:
    """Logger system cho trading bot"""
    
    def __init__(self, name, level=logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)
        
        if not self.logger.handlers:
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            
            # Console handler
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
    
    def info(self, message):
        self.logger.info(message)
    
    def error(self, message):
        self.logger.error(message)
    
    def warning(self, message):
        self.logger.warning(message)
    
    def debug(self, message):
        self.logger.debug(message)
main\

# main.py
import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import pandas as pd
import numpy as np
from pathlib import Path
import yaml
import schedule
import time
from dataclasses import dataclass, asdict
from enum import Enum

# Import các core modules
from core.data_manager import AdvancedDataManager
from core.market_regime_detector import MarketRegimeDetector, MarketRegime
from core.signal_generator import AdvancedSignalGenerator
from core.risk_manager import AdvancedRiskManager
from core.position_sizer import AdvancedPositionSizer
from core.portfolio_manager import AdvancedPortfolioManager
from core.execution_engine import ExecutionEngine, Order, OrderType, OrderStatus
from core.performance_tracker import PerformanceTracker
from core.ml_pipeline import MLPipeline

# Import strategies
from strategies.trend_strategy import TrendStrategy
from strategies.mean_reversion_strategy import MeanReversionStrategy
from strategies.breakout_strategy import BreakoutStrategy

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('trading_bot.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TradingMode(Enum):
    """Trading mode enumeration"""
    BACKTEST = "BACKTEST"
    PAPER_TRADE = "PAPER_TRADE"
    LIVE_TRADE = "LIVE_TRADE"

@dataclass
class TradingConfig:
    """Trading configuration"""
    # General
    mode: TradingMode = TradingMode.PAPER_TRADE
    symbol: str = "XAUUSD"
    initial_capital: float = 10000.0
    max_drawdown_limit: float = 0.20
    max_position_risk: float = 0.02
    
    # Timeframes
    timeframes: List[str] = None
    
    # Strategies
    enabled_strategies: List[str] = None
    
    # Execution
    max_orders_per_day: int = 10
    order_cooling_period: float = 1.0  # seconds
    
    # Risk Management
    stop_loss_type: str = "atr"  # atr, percentage, support_resistance
    take_profit_ratio: float = 1.5
    
    # ML
    use_ml: bool = True
    ml_retrain_frequency: str = "weekly"  # daily, weekly, monthly
    
    # Monitoring
    health_check_interval: int = 300  # seconds
    performance_report_interval: str = "daily"  # hourly, daily, weekly
    
    def __post_init__(self):
        if self.timeframes is None:
            self.timeframes = ['1h', '30m', '15m', '5m', '1m']
        if self.enabled_strategies is None:
            self.enabled_strategies = ['TREND', 'MEAN_REVERSION', 'BREAKOUT']

class TradingBot:
    """Main trading bot controller - Bộ não chính"""
    
    def __init__(self, config: Optional[TradingConfig] = None):
        self.config = config or TradingConfig()
        self.running = False
        self.start_time = None
        
        # Core modules
        self.data_manager = None
        self.market_regime_detector = None
        self.signal_generator = None
        self.risk_manager = None
        self.position_sizer = None
        self.portfolio_manager = None
        self.execution_engine = None
        self.performance_tracker = None
        self.ml_pipeline = None
        
        # State
        self.current_market_regime = None
        self.last_signal_time = None
        self.daily_order_count = 0
        self.last_health_check = None
        
        # Statistics
        self.stats = {
            'total_signals': 0,
            'executed_trades': 0,
            'profitable_trades': 0,
            'total_pnl': 0.0,
            'start_time': None,
            'uptime': None
        }
        
        # Cache
        self.cache = {}
        self.alert_manager = AlertManager()
        
        logger.info("TradingBot initialized")
    
    async def initialize(self):
        """Khởi tạo tất cả modules"""
        logger.info("Initializing TradingBot...")
        
        try:
            # 1. Initialize Data Manager
            logger.info("Initializing Data Manager...")
            self.data_manager = AdvancedDataManager(
                timeframes=self.config.timeframes,
                cache_size=10000
            )
            
            # Load historical data
            await self._load_historical_data()
            
            # 2. Initialize Market Regime Detector
            logger.info("Initializing Market Regime Detector...")
            self.market_regime_detector = MarketRegimeDetector(
                lookback_period=100,
                volatility_threshold=0.02
            )
            
            # 3. Initialize Risk Manager
            logger.info("Initializing Risk Manager...")
            self.risk_manager = AdvancedRiskManager(
                initial_capital=self.config.initial_capital,
                max_drawdown_limit=self.config.max_drawdown_limit,
                max_position_risk=self.config.max_position_risk
            )
            
            # 4. Initialize Position Sizer
            logger.info("Initializing Position Sizer...")
            self.position_sizer = AdvancedPositionSizer(
                initial_capital=self.config.initial_capital,
                risk_per_trade=self.config.max_position_risk,
                use_kelly=True,
                use_volatility_scaling=True
            )
            
            # 5. Initialize Portfolio Manager
            logger.info("Initializing Portfolio Manager...")
            self.portfolio_manager = AdvancedPortfolioManager(
                initial_capital=self.config.initial_capital,
                max_drawdown=self.config.max_drawdown_limit,
                target_return=0.15
            )
            
            # 6. Initialize Performance Tracker
            logger.info("Initializing Performance Tracker...")
            self.performance_tracker = PerformanceTracker(
                initial_capital=self.config.initial_capital,
                benchmark_symbol='SPY'
            )
            
            # 7. Initialize Signal Generator với strategies
            logger.info("Initializing Signal Generator...")
            self.signal_generator = AdvancedSignalGenerator(
                data_manager=self.data_manager,
                risk_manager=self.risk_manager,
                config={
                    'signal_expiration_minutes': 5,
                    'min_confidence_threshold': 0.65,
                    'max_signals_per_timeframe': 3
                }
            )
            
            # 8. Initialize Execution Engine
            logger.info("Initializing Execution Engine...")
            self.execution_engine = ExecutionEngine(
                max_slippage_pct=0.001,
                max_retries=3,
                use_smart_routing=True
            )
            await self.execution_engine.initialize()
            
            # 9. Initialize ML Pipeline (nếu enabled)
            if self.config.use_ml:
                logger.info("Initializing ML Pipeline...")
                self.ml_pipeline = MLPipeline(
                    feature_config={
                        'feature_window': 50,
                        'target_horizon': 5,
                        'feature_types': ['price', 'volume', 'technical', 'statistical']
                    },
                    use_gpu=False
                )
                
                # Train ML model nếu có đủ data
                await self._train_ml_model()
            
            # 10. Update market regime
            await self._update_market_regime()
            
            logger.info("TradingBot initialized successfully!")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize TradingBot: {str(e)}")
            await self.shutdown()
            return False
    
    async def _load_historical_data(self):
        """Load historical data"""
        try:
            # Tìm file data mới nhất
            data_files = list(Path('.').glob('*gold*.csv')) + \
                        list(Path('.').glob('*xauusd*.csv')) + \
                        list(Path('.').glob('*data*.csv'))
            
            if data_files:
                latest_file = max(data_files, key=lambda x: x.stat().st_mtime)
                logger.info(f"Loading data from {latest_file}")
                
                # Load data (adjust based on your data format)
                data = pd.read_csv(latest_file, parse_dates=True, index_col=0)
                
                # Convert to required format
                historical_data = {
                    '1h': data  # Placeholder - bạn cần convert sang multiple timeframes
                }
                
                self.data_manager.data_store.update(historical_data)
            else:
                logger.warning("No historical data files found. Using sample data.")
                # Tạo sample data cho testing
                await self._create_sample_data()
                
        except Exception as e:
            logger.error(f"Error loading historical data: {str(e)}")
            await self._create_sample_data()
    
    async def _create_sample_data(self):
        """Tạo sample data cho testing"""
        dates = pd.date_range('2024-01-01', '2024-12-31', freq='1h')
        n = len(dates)
        
        for timeframe in self.config.timeframes:
            # Tạo sample data cho mỗi timeframe
            data = pd.DataFrame({
                'open': np.random.normal(1800, 20, n).cumsum(),
                'high': np.random.normal(1810, 25, n).cumsum(),
                'low': np.random.normal(1790, 25, n).cumsum(),
                'close': np.random.normal(1800, 20, n).cumsum(),
                'volume': np.random.normal(1000, 200, n)
            }, index=dates)
            
            self.data_manager.data_store[timeframe] = data
    
    async def _train_ml_model(self):
        """Train ML model"""
        if not self.ml_pipeline or not self.data_manager:
            return
        
        try:
            # Lấy data cho training
            data = self.data_manager.data_store.get('1h')
            if data is None or len(data) < 100:
                logger.warning("Insufficient data for ML training")
                return
            
            # Prepare features
            X, y = self.ml_pipeline.prepare_features(data)
            
            if len(X) > 100:
                # Split data
                self.ml_pipeline.train_test_split(X, y, test_size=0.2)
                
                # Train models
                self.ml_pipeline.train_models()
                
                logger.info("ML model trained successfully")
            else:
                logger.warning("Not enough data for ML training")
                
        except Exception as e:
            logger.error(f"Error training ML model: {str(e)}")
    
    async def _update_market_regime(self):
        """Cập nhật market regime"""
        try:
            data = self.data_manager.data_store.get('1h')
            if data is not None and len(data) > 100:
                regime_result = self.market_regime_detector.detect_regime(data, '1h')
                self.current_market_regime = regime_result['regime']
                
                logger.info(f"Market regime updated: {self.current_market_regime.value}")
                
                # Send alert nếu regime thay đổi
                if hasattr(self, 'last_regime') and self.last_regime != self.current_market_regime:
                    self.alert_manager.send_alert(
                        f"Market regime changed: {self.last_regime.value} → {self.current_market_regime.value}",
                        level="INFO"
                    )
                
                self.last_regime = self.current_market_regime
                
        except Exception as e:
            logger.error(f"Error updating market regime: {str(e)}")
    
    async def run(self):
        """Chạy trading bot chính"""
        if self.running:
            logger.warning("Bot is already running")
            return
        
        logger.info("Starting TradingBot...")
        self.running = True
        self.start_time = datetime.now()
        self.stats['start_time'] = self.start_time
        
        try:
            # Schedule tasks
            self._schedule_tasks()
            
            # Main trading loop
            while self.running:
                try:
                    # 1. Health check
                    await self._health_check()
                    
                    # 2. Update data
                    await self._update_market_data()
                    
                    # 3. Generate signals
                    signals = await self._generate_signals()
                    
                    # 4. Process signals
                    if signals:
                        await self._process_signals(signals)
                    
                    # 5. Monitor positions
                    await self._monitor_positions()
                    
                    # 6. Update statistics
                    self._update_statistics()
                    
                    # Sleep based on mode
                    if self.config.mode == TradingMode.LIVE_TRADE:
                        await asyncio.sleep(1)  # 1 second for live trading
                    else:
                        await asyncio.sleep(5)  # 5 seconds for paper/backtest
                        
                except KeyboardInterrupt:
                    logger.info("Keyboard interrupt received. Shutting down...")
                    break
                except Exception as e:
                    logger.error(f"Error in main loop: {str(e)}")
                    await asyncio.sleep(10)  # Wait before retry
        
        finally:
            await self.shutdown()
    
    def _schedule_tasks(self):
        """Lập lịch các periodic tasks"""
        
        # Health check mỗi 5 phút
        schedule.every(self.config.health_check_interval).seconds.do(
            lambda: asyncio.create_task(self._health_check())
        )
        
        # Update market regime mỗi giờ
        schedule.every(1).hours.do(
            lambda: asyncio.create_task(self._update_market_regime())
        )
        
        # Performance report hàng ngày
        if self.config.performance_report_interval == "daily":
            schedule.every().day.at("23:59").do(
                lambda: asyncio.create_task(self._generate_performance_report())
            )
        
        # Retrain ML model hàng tuần
        if self.config.use_ml and self.config.ml_retrain_frequency == "weekly":
            schedule.every().monday.at("02:00").do(
                lambda: asyncio.create_task(self._train_ml_model())
            )
        
        logger.info("Scheduled tasks initialized")
    
    async def _health_check(self):
        """Health check cho system"""
        current_time = datetime.now()
        
        # Kiểm tra mỗi 5 phút
        if self.last_health_check and \
           (current_time - self.last_health_check).seconds < 300:
            return
        
        logger.info("Running health check...")
        
        health_status = {
            'timestamp': current_time,
            'status': 'HEALTHY',
            'issues': []
        }
        
        # Kiểm tra các modules
        modules = [
            ('Data Manager', self.data_manager),
            ('Risk Manager', self.risk_manager),
            ('Portfolio Manager', self.portfolio_manager),
            ('Execution Engine', self.execution_engine),
            ('Performance Tracker', self.performance_tracker)
        ]
        
        for name, module in modules:
            if module is None:
                health_status['issues'].append(f"{name} not initialized")
        
        # Kiểm tra data freshness
        if self.data_manager and hasattr(self.data_manager, 'data_store'):
            for timeframe, data in self.data_manager.data_store.items():
                if not data.empty:
                    last_data_time = data.index[-1]
                    data_age = (current_time - last_data_time).total_seconds() / 3600
                    
                    if data_age > 24:  # Data quá 24 giờ
                        health_status['issues'].append(
                            f"{timeframe} data is {data_age:.1f} hours old"
                        )
        
        # Kiểm tra memory usage
        try:
            import psutil
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > 90:
                health_status['issues'].append(f"High memory usage: {memory_percent}%")
        except ImportError:
            pass
        
        # Update status
        if health_status['issues']:
            health_status['status'] = 'WARNING'
            logger.warning(f"Health check issues: {health_status['issues']}")
            
            # Send alert cho critical issues
            critical_issues = [issue for issue in health_status['issues'] 
                             if 'not initialized' in issue or 'hours old' in issue]
            if critical_issues:
                self.alert_manager.send_alert(
                    f"Health check warnings: {', '.join(critical_issues)}",
                    level="WARNING"
                )
        else:
            logger.info("Health check passed")
        
        self.last_health_check = current_time
        return health_status
    
    async def _update_market_data(self):
        """Cập nhật market data"""
        try:
            # Trong real trading, đây sẽ là nơi lấy real-time data
            # Hiện tại chỉ log để demo
            if self.config.mode == TradingMode.LIVE_TRADE:
                logger.debug("Updating real-time market data...")
                # TODO: Implement real-time data feed
            elif self.config.mode == TradingMode.PAPER_TRADE:
                # Simulate data update
                pass
                
        except Exception as e:
            logger.error(f"Error updating market data: {str(e)}")
    
    async def _generate_signals(self) -> List[Dict]:
        """Generate trading signals"""
        try:
            signals = self.signal_generator.generate_signals(
                symbol=self.config.symbol,
                timeframes=self.config.timeframes
            )
            
            if signals:
                logger.info(f"Generated {len(signals)} signal(s)")
                self.stats['total_signals'] += len(signals)
                
                # Filter signals based on market regime
                filtered_signals = self._filter_signals_by_regime(signals)
                
                return filtered_signals
            else:
                return []
                
        except Exception as e:
            logger.error(f"Error generating signals: {str(e)}")
            return []
    
    def _filter_signals_by_regime(self, signals: List[Dict]) -> List[Dict]:
        """Filter signals based on current market regime"""
        if not self.current_market_regime:
            return signals
        
        filtered_signals = []
        
        for signal in signals:
            signal_type = signal.get('signal', '')
            regime = self.current_market_regime
            
            # Check if signal aligns with regime
            if regime in [MarketRegime.STRONG_BULL, MarketRegime.BULL]:
                if signal_type == 'BUY':
                    filtered_signals.append(signal)
            elif regime in [MarketRegime.STRONG_BEAR, MarketRegime.BEAR]:
                if signal_type == 'SELL':
                    filtered_signals.append(signal)
            else:
                # Neutral or ranging regime - accept all signals
                filtered_signals.append(signal)
        
        return filtered_signals
    
    async def _process_signals(self, signals: List[Dict]):
        """Process và execute signals"""
        for signal in signals:
            try:
                # Check if signal is still valid
                if not self._is_signal_valid(signal):
                    logger.warning(f"Signal expired: {signal.get('signal_id')}")
                    continue
                
                # Get current market data
                market_data = await self._get_current_market_data(signal)
                
                # Risk check
                risk_check = self.risk_manager.check_risk_limits(
                    signal,
                    self.portfolio_manager.positions
                )
                
                if not risk_check[0]:
                    logger.warning(f"Risk check failed: {risk_check[1]}")
                    continue
                
                # Calculate position size
                account_info = {
                    'capital': self.portfolio_manager.current_capital,
                    'positions': self.portfolio_manager.positions
                }
                
                position_data = self.position_sizer.calculate_position_size(
                    signal, market_data, account_info
                )
                
                if position_data.get('position_size_pct', 0) <= 0:
                    logger.warning("Position size too small, skipping")
                    continue
                
                # Execute order
                execution_result = await self._execute_order(
                    signal, position_data, market_data
                )
                
                if execution_result['success']:
                    # Update portfolio
                    self._update_portfolio_after_execution(
                        signal, position_data, execution_result
                    )
                    
                    logger.info(f"Order executed: {execution_result['order_id']}")
                    
                    # Send alert
                    self.alert_manager.send_alert(
                        f"Order executed: {signal.get('signal')} {self.config.symbol} "
                        f"at {execution_result.get('fill_price', 0):.2f}",
                        level="INFO"
                    )
                
            except Exception as e:
                logger.error(f"Error processing signal: {str(e)}")
    
    def _is_signal_valid(self, signal: Dict) -> bool:
        """Kiểm tra xem signal còn valid không"""
        expiration_time = signal.get('expiration_time')
        if expiration_time and datetime.now() > expiration_time:
            return False
        
        # Check confidence threshold
        confidence = signal.get('confidence', 0)
        if confidence < 0.6:  # Minimum confidence
            return False
        
        return True
    
    async def _get_current_market_data(self, signal: Dict) -> Dict:
        """Lấy current market data"""
        timeframe = signal.get('timeframe', '1h')
        
        # Get data from data manager
        data = self.data_manager.data_store.get(timeframe)
        
        if data is None or data.empty:
            return {}
        
        # Get current price
        current_price = data['close'].iloc[-1] if len(data) > 0 else 0
        
        # Get volatility
        volatility = 0.02  # Placeholder
        
        # Get support/resistance
        support_resistance = self.data_manager.get_support_resistance_levels(timeframe)
        
        # Get liquidity data
        liquidity_metrics = {
            'score': 0.7,  # Placeholder
            'slippage_estimation': 0.001
        }
        
        return {
            'price': current_price,
            'volatility': volatility,
            'support_resistance': support_resistance,
            'liquidity_metrics': liquidity_metrics,
            'market_regime': self.current_market_regime.value if self.current_market_regime else 'NEUTRAL'
        }
    
    async def _execute_order(self, 
                           signal: Dict,
                           position_data: Dict,
                           market_data: Dict) -> Dict:
        """Execute order"""
        
        if self.config.mode == TradingMode.BACKTEST:
            # Simulate execution cho backtest
            return self._simulate_execution(signal, position_data, market_data)
        
        elif self.config.mode == TradingMode.PAPER_TRADE:
            # Paper trading - log but don't execute
            logger.info(f"[PAPER] Would execute: {signal.get('signal')} "
                       f"{position_data.get('units', 0):.2f} units "
                       f"at ~{market_data.get('price', 0):.2f}")
            
            return {
                'success': True,
                'order_id': f"paper_{datetime.now().timestamp()}",
                'status': 'FILLED',
                'fill_price': market_data.get('price', 0),
                'fees': 0.0,
                'slippage': 0.0,
                'timestamp': datetime.now()
            }
        
        elif self.config.mode == TradingMode.LIVE_TRADE:
            # Real execution
            return await self.execution_engine.execute_order(
                signal, position_data, market_data
            )
    
    def _simulate_execution(self, 
                          signal: Dict,
                          position_data: Dict,
                          market_data: Dict) -> Dict:
        """Simulate execution cho backtest"""
        
        current_price = market_data.get('price', 0)
        
        # Add random slippage
        slippage = np.random.normal(0.0005, 0.0002)  # 0.05% mean slippage
        fill_price = current_price * (1 + slippage) if signal.get('signal') == 'BUY' \
                    else current_price * (1 - slippage)
        
        # Simulate fees
        fees = abs(fill_price * position_data.get('units', 0) * 0.001)  # 0.1% fee
        
        return {
            'success': True,
            'order_id': f"sim_{datetime.now().timestamp()}",
            'status': 'FILLED',
            'fill_price': fill_price,
            'fees': fees,
            'slippage': abs(slippage),
            'timestamp': datetime.now()
        }
    
    def _update_portfolio_after_execution(self,
                                        signal: Dict,
                                        position_data: Dict,
                                        execution_result: Dict):
        """Cập nhật portfolio sau khi execute order"""
        
        # Create position record
        position_record = {
            **position_data,
            'signal': signal.get('signal'),
            'strategy': signal.get('strategy'),
            'timeframe': signal.get('timeframe'),
            'confidence': signal.get('confidence', 0.5),
            'entry_price': execution_result.get('fill_price'),
            'entry_time': datetime.now(),
            'status': 'OPEN',
            'order_id': execution_result.get('order_id'),
            'fees': execution_result.get('fees', 0),
            'slippage': execution_result.get('slippage', 0)
        }
        
        # Add to portfolio
        symbol = f"{self.config.symbol}_{signal.get('timeframe', '1h')}"
        self.portfolio_manager.add_position(
            symbol=symbol,
            position_data=position_record,
            signal_data=signal
        )
        
        # Update performance tracker
        trade_record = {
            'symbol': self.config.symbol,
            'signal': signal.get('signal'),
            'entry_price': execution_result.get('fill_price'),
            'exit_price': None,
            'units': position_data.get('units', 0),
            'pnl': 0.0,  # Will be updated when closed
            'entry_time': datetime.now(),
            'exit_time': None,
            'strategy': signal.get('strategy'),
            'confidence': signal.get('confidence', 0.5)
        }
        
        # Note: P&L will be calculated when position is closed
        
        self.stats['executed_trades'] += 1
    
    async def _monitor_positions(self):
        """Monitor và manage open positions"""
        try:
            for symbol, position in list(self.portfolio_manager.positions.items()):
                if position.get('status') == 'OPEN':
                    # Check stop loss và take profit
                    should_close = await self._check_position_exit(position)
                    
                    if should_close:
                        await self._close_position(symbol, position, should_close['reason'])
        
        except Exception as e:
            logger.error(f"Error monitoring positions: {str(e)}")
    
    async def _check_position_exit(self, position: Dict) -> Optional[Dict]:
        """Kiểm tra xem có nên close position không"""
        
        # Get current market price
        current_price = await self._get_current_price()
        
        if current_price <= 0:
            return None
        
        entry_price = position.get('entry_price', 0)
        stop_loss = position.get('stop_loss', 0)
        take_profit = position.get('take_profit', 0)
        signal_type = position.get('signal', 'BUY')
        
        if signal_type == 'BUY':
            # Check stop loss
            if current_price <= stop_loss:
                return {'reason': 'STOP_LOSS', 'price': current_price}
            
            # Check take profit
            if current_price >= take_profit:
                return {'reason': 'TAKE_PROFIT', 'price': current_price}
        
        else:  # SELL
            # Check stop loss
            if current_price >= stop_loss:
                return {'reason': 'STOP_LOSS', 'price': current_price}
            
            # Check take profit
            if current_price <= take_profit:
                return {'reason': 'TAKE_PROFIT', 'price': current_price}
        
        # Check time-based exit (max holding period)
        entry_time = position.get('entry_time')
        if entry_time:
            holding_hours = (datetime.now() - entry_time).total_seconds() / 3600
            max_holding_hours = 24  # Max 24 hours
            
            if holding_hours > max_holding_hours:
                return {'reason': 'MAX_HOLDING_TIME', 'price': current_price}
        
        return None
    
    async def _get_current_price(self) -> float:
        """Lấy current market price"""
        try:
            data = self.data_manager.data_store.get('1h')
            if data is not None and len(data) > 0:
                return float(data['close'].iloc[-1])
        except:
            pass
        
        return 0.0
    
    async def _close_position(self, 
                            symbol: str,
                            position: Dict,
                            exit_reason: str):
        """Close position"""
        
        current_price = await self._get_current_price()
        
        if current_price <= 0:
            logger.error(f"Cannot close position {symbol}: Invalid current price")
            return
        
        # Close position in portfolio manager
        close_result = self.portfolio_manager.close_position(
            symbol=symbol,
            exit_price=current_price,
            exit_reason=exit_reason
        )
        
        if close_result['success']:
            # Update performance tracker
            pnl = close_result['position'].get('pnl', 0)
            self.performance_tracker.update_trade({
                'symbol': self.config.symbol,
                'entry_price': position.get('entry_price'),
                'exit_price': current_price,
                'pnl': pnl,
                'entry_time': position.get('entry_time'),
                'exit_time': datetime.now(),
                'strategy': position.get('strategy'),
                'exit_reason': exit_reason
            })
            
            # Update statistics
            if pnl > 0:
                self.stats['profitable_trades'] += 1
            self.stats['total_pnl'] += pnl
            
            logger.info(f"Position closed: {symbol}, P&L: {pnl:.2f}, Reason: {exit_reason}")
            
            # Send alert
            self.alert_manager.send_alert(
                f"Position closed: {symbol}, P&L: {pnl:.2f}, Reason: {exit_reason}",
                level="INFO" if pnl >= 0 else "WARNING"
            )
    
    def _update_statistics(self):
        """Cập nhật statistics"""
        if self.start_time:
            self.stats['uptime'] = str(datetime.now() - self.start_time)
    
    async def _generate_performance_report(self):
        """Generate performance report"""
        try:
            report = self.performance_tracker.get_performance_report()
            
            # Save report to file
            report_date = datetime.now().strftime("%Y%m%d")
            report_file = f"reports/performance_{report_date}.json"
            
            Path('reports').mkdir(exist_ok=True)
            
            with open(report_file, 'w') as f:
                json.dump(report, f, indent=2, default=str)
            
            logger.info(f"Performance report saved to {report_file}")
            
            # Log summary
            summary = report.get('summary', {})
            logger.info(
                f"Performance Summary - "
                f"Total Return: {summary.get('total_return', 0):.2%}, "
                f"Win Rate: {summary.get('win_rate', 0):.1%}, "
                f"Total P&L: {summary.get('total_pnl', 0):.2f}"
            )
            
        except Exception as e:
            logger.error(f"Error generating performance report: {str(e)}")
    
    async def shutdown(self):
        """Shutdown trading bot"""
        logger.info("Shutting down TradingBot...")
        self.running = False
        
        try:
            # Close all open positions
            for symbol, position in list(self.portfolio_manager.positions.items()):
                if position.get('status') == 'OPEN':
                    await self._close_position(symbol, position, 'SHUTDOWN')
            
            # Shutdown execution engine
            if self.execution_engine:
                await self.execution_engine.close()
            
            # Generate final report
            await self._generate_performance_report()
            
            # Save state
            self._save_state()
            
            logger.info("TradingBot shutdown complete")
            
        except Exception as e:
            logger.error(f"Error during shutdown: {str(e)}")
    
    def _save_state(self):
        """Save bot state"""
        try:
            state = {
                'config': asdict(self.config),
                'stats': self.stats,
                'portfolio_state': self.portfolio_manager.get_portfolio_report(),
                'performance_state': self.performance_tracker.get_performance_report(),
                'timestamp': datetime.now().isoformat()
            }
            
            Path('state').mkdir(exist_ok=True)
            state_file = f"state/bot_state_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(state_file, 'w') as f:
                json.dump(state, f, indent=2, default=str)
            
            logger.info(f"Bot state saved to {state_file}")
            
        except Exception as e:
            logger.error(f"Error saving state: {str(e)}")
    
    def get_status(self) -> Dict:
        """Get current bot status"""
        return {
            'running': self.running,
            'mode': self.config.mode.value,
            'start_time': self.start_time,
            'uptime': self.stats.get('uptime'),
            'market_regime': self.current_market_regime.value if self.current_market_regime else None,
            'statistics': self.stats,
            'portfolio_summary': {
                'open_positions': len([p for p in self.portfolio_manager.positions.values() 
                                      if p.get('status') == 'OPEN']),
                'total_value': self.portfolio_manager.current_capital,
                'total_pnl': self.stats['total_pnl']
            }
        }


class AlertManager:
    """Quản lý alerts và notifications"""
    
    def __init__(self):
        self.alerts = []
        self.max_alerts = 1000
        
        # Notification channels
        self.channels = {
            'console': True,
            'log_file': True,
            # 'telegram': False,  # Optional
            # 'email': False,     # Optional
            # 'discord': False,   # Optional
        }
    
    def send_alert(self, message: str, level: str = "INFO"):
        """Gửi alert"""
        alert = {
            'timestamp': datetime.now(),
            'level': level,
            'message': message
        }
        
        self.alerts.append(alert)
        
        # Keep only recent alerts
        if len(self.alerts) > self.max_alerts:
            self.alerts = self.alerts[-self.max_alerts:]
        
        # Send to channels
        if self.channels['console']:
            print(f"[{level}] {message}")
        
        if self.channels['log_file']:
            logger.log(
                getattr(logging, level),
                message
            )
        
        # TODO: Implement other channels
    
    def get_recent_alerts(self, count: int = 10) -> List[Dict]:
        """Lấy recent alerts"""
        return self.alerts[-count:] if self.alerts else []


def load_config(config_file: str = "config.yaml") -> TradingConfig:
    """Load configuration từ file"""
    try:
        with open(config_file, 'r') as f:
            config_data = yaml.safe_load(f)
        
        # Convert string mode to enum
        if 'mode' in config_data:
            config_data['mode'] = TradingMode(config_data['mode'])
        
        return TradingConfig(**config_data)
        
    except FileNotFoundError:
        logger.warning(f"Config file {config_file} not found, using defaults")
        return TradingConfig()
    except Exception as e:
        logger.error(f"Error loading config: {str(e)}")
        return TradingConfig()


async def main():
    """Main entry point"""
    print("\n" + "="*60)
    print("TRADING BOT v2.0 - ADVANCED ML TRADING SYSTEM")
    print("="*60 + "\n")
    
    # Load configuration
    config = load_config()
    
    print("Configuration:")
    print(f"  Mode: {config.mode.value}")
    print(f"  Symbol: {config.symbol}")
    print(f"  Capital: ${config.initial_capital:,.2f}")
    print(f"  Strategies: {', '.join(config.enabled_strategies)}")
    print(f"  Timeframes: {', '.join(config.timeframes)}")
    print(f"  ML Enabled: {config.use_ml}")
    print()
    
    # Create và initialize bot
    bot = TradingBot(config)
    
    if await bot.initialize():
        try:
            # Run bot
            print("Bot initialized successfully. Starting...")
            print("Press Ctrl+C to stop\n")
            
            await bot.run()
            
        except KeyboardInterrupt:
            print("\nShutdown requested by user")
        except Exception as e:
            logger.error(f"Fatal error: {str(e)}")
        finally:
            await bot.shutdown()
    else:
        print("Failed to initialize bot. Exiting...")
    
    print("\nTradingBot stopped.")


if __name__ == "__main__":
    # Run the bot
    asyncio.run(main())
